<!DOCTYPE html>
<html lang="en-us">
  <head>
  <link href="https://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
  <title>
    
      Asymptotic Notation &middot; Ozaner
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-144-precomposed.png">
                                 <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- Load JQuery (JavaScript)-->
  <script src="https://code.jquery.com/jquery-2.2.4.min.js"></script>

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      positionToHash: true,
      jax: ["input/TeX", "output/HTML-CSS"],
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$', '$$']],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      },
      messageStyle: "none",
      "HTML-CSS": {
        preferredFont: "TeX",
        availableFonts: ["STIX","TeX"],
        styles: {".MathJax": {color: "#CCCCCC"}}
      },
      displayAlign: "center!important",
      displayIndent: "0em"
    });
    MathJax.Hub.Queue(function() {
      // Fix <code> tags after MathJax finishes running. This is a
      // hack to overcome a shortcoming of Markdown. Discussion at
      // https://github.com/mojombo/jekyll/issues/199
      var all = MathJax.Hub.getAllJax(), i;
      for(i = 0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
      }
    });
  </script>

  <!-- Adds MathJax -->
  <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>

  <!-- Adds Awesome Icons-->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">

  <!-- Collects tags of all posts-->
  
    






  

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-111962015-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-111962015-1');
  </script>
</head>

  <body>

    <div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/">
          $$\large{\text {Oz}}\tiny{\underbrace{\large {\text {∀n∈ℝ}}}_{\tiny {\text {For all real n...}}}}$$
        </a>
      </h1>
      <p class="lead">A compilation of my notes, projects, and articles.</p>
    </div>

    <nav class="sidebar-nav">
      <a href="https://medium.com/@ozanerhansha" target="_blank">Articles</a><br/>

      

      
      
        
          
        
      
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/notes/">Notes</a>
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/projects/">Projects</a>
          
        
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/references/">Interesting References</a>
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      

      <style>
        p.small {
            line-height: 0.7;
        }
      </style>

      <br>

      <p class="small">
        <!--- Icon Links -->
        <b>Links: </b>
        <a href="https://github.com/ozanerhansha"><i class='fa fa-github'></i></a>
        <!-- <a href="https://www.quora.com/profile/Ozaner-Hansha/questions"><i class='fa fa-quora'></i></a> -->
        <a href="https://medium.com/@ozanerhansha"<i class='fa fa-medium'></i></a>
        <!-- <a href="https://math.stackexchange.com/users/490433/ozaner-hansha?tab=questions"><i class='fa fa-stack-exchange'></i></a> -->

        <br>
        <font size="2"><a href="https://github.com/ozanerhansha/ozanerhansha.github.io">This Website's Code</a></font>
      </p>
    </nav>
    <p>2018 Ozaner Hansha</p>
  </div>
</div>


    <div class="content container">
      <div class="post">
  <h1 class="post-title">Asymptotic Notation</h1>
  <span class="post-date">October 5, 2018</span>
  <span>[
  
    
    <a href="/tag/math"><code class="highligher-rouge"><nobr>math</nobr></code>&nbsp;</a>
  
    
    <a href="/tag/computer-science"><code class="highligher-rouge"><nobr>computer-science</nobr></code>&nbsp;</a>
  
]</span>
  <p>The notation used to describe the <strong>asymptotics</strong>, or limiting behavior, of functions consists of a set of 6 relations. These different relations allow us to compare the growth of different functions as they approach some constant or, in most cases, infinity.</p>

<p>In the infinite case, these relations usually boil down to the functions’ <em>most significant term</em>. That is, the term that grows the quickest as a function of their input. For example, in the function $x^3+5x^2+x$ this role falls on the $x^3$ term as the other terms become insignificant (i.e tend to 0) as $x\to\infty$.</p>

<!--more-->

<p>These notations, particularly big $O$, are most commonly used to classify and compare the computational complexity (both temporal and spatial) of different algorithms as a function of their input size. Ideally, an algorithm will be on the order of a relatively slowly growing function (e.g $\log n$ grows slower than $n^2$) as they take less resources and thus are more feasible to compute.</p>

<p>They are also used in the approximation of functions that would be infeasible, or even impossible, to calculate otherwise.</p>

<table>
  <thead>
    <tr>
      <th>Name</th>
      <th style="text-align: center">Set Membership<br />Notation</th>
      <th style="text-align: center">Comparitive<br />Notation</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Little-$o$</td>
      <td style="text-align: center">$f(n)\in o(g(n))$</td>
      <td style="text-align: center">$f\prec g$</td>
    </tr>
    <tr>
      <td>Big-$O$</td>
      <td style="text-align: center">$f(n)\in O(g(n))$</td>
      <td style="text-align: center">$f\preceq g$</td>
    </tr>
    <tr>
      <td>Big-$\Theta$</td>
      <td style="text-align: center">$f(n)\in \Theta(g(n))$</td>
      <td style="text-align: center">$f\asymp g$</td>
    </tr>
    <tr>
      <td>Big-$\Omega$</td>
      <td style="text-align: center">$f(n)\in \Omega(g(n))$</td>
      <td style="text-align: center">$f\succeq g$</td>
    </tr>
    <tr>
      <td>Little-$\omega$</td>
      <td style="text-align: center">$f(n)\in \omega(g(n))$</td>
      <td style="text-align: center">$f\succ g$</td>
    </tr>
    <tr>
      <td>Asymptotic<br />Equivalence</td>
      <td style="text-align: center">N/A</td>
      <td style="text-align: center">$f\sim g$</td>
    </tr>
  </tbody>
</table>

<h2 id="formal-definitions">Formal Definitions</h2>
<p>We can formalize this notation by considering either how the values of the function grow after some sufficiently large value $x_0$ or by the limit of their ratio as $x$ approaches infinity. Both notions are given below:</p>

<p><em>Note that these two different definitions are similar but not equivalent. Also note that asymptotic equivalence only has a limit definitoin.</em></p>

<details>
<summary><h3 class="inline">Growth Definitions</h3></summary>
A function $f(x)$ is big $O$ of $g(x)$, i.e $f(x)\in O(g(x))$, if the following holds:

$$f(x)\in O(g(x))\equiv (\exists k,x_0)\ x\ge x_0\rightarrow f(x)\le kg(x)$$

This means that there is some point such that for all $x$ after it, $f(x)$ is less than or equal to $g(x)$ up to some constant.

The definitions for the other notations are similar:

$$\begin{align}
f(x)\in o(g(x))&amp;\equiv \left(\forall k,\exists x_0\right) x\ge x_0&amp;&amp;\rightarrow f(x)\lt kg(x)\\
f(x)\in O(g(x))&amp;\equiv \left(\exists k,x_0\right) x\ge x_0&amp;&amp;\rightarrow f(x)\le kg(x)\\
f(x)\in \Theta(g(x))&amp;\equiv \left(\exists k_1,k_2,x_0\right) x\ge x_0&amp;&amp;\rightarrow k_1g(x)\le f(x)\le k_2g(x)\\
f(x)\in \Omega(g(x))&amp;\equiv \left(\exists k,x_0\right) x\ge x_0&amp;&amp;\rightarrow f(x)\ge kg(x)\\
f(x)\in \omega(g(x))&amp;\equiv \left(\forall k,\exists x_0\right) x\ge x_0&amp;&amp;\rightarrow f(x)\gt kg(x)
\end{align}$$

<i>Where $f,g:\mathbb{R}\to\mathbb{R}$ and $x,x_0,k,k_1,k_2\in\mathbb{R}.$</i>
<p></p>
</details>

<details>
<summary><h3 class="inline">Limit Definitions</h3></summary>
Defining asymptotic notation via limits is cleaner, both visually and possibly conceptually. However, doing so comes with added stipulations about limit existence and so on but if those conditions are met, we can define them as so:

$$\begin{align}
f(x)\prec g(x)&amp;\equiv \lim_{x\to\infty}\frac{f(x)}{g(x)}=0\\
f(x)\preceq g(x)&amp;\equiv \limsup_{x\to\infty}\left|\frac{f(x)}{g(x)}\right|\lt\infty\\
f(x)\asymp g(x)&amp;\equiv (\exists k)\lim_{x\to\infty}\frac{f(x)}{g(x)}=k\\
f(x)\sim g(x)&amp;\equiv \lim_{x\to\infty}\frac{f(x)}{g(x)}=1\\
f(x)\succeq g(x)&amp;\equiv \liminf_{x\to\infty}\left|\frac{f(x)}{g(x)}\right|\gt 0\\
f(x)\succ g(x)&amp;\equiv \lim_{x\to\infty}\left|\frac{f(x)}{g(x)}\right|=\infty
\end{align}$$
<p></p>
</details>

<h4 id="asymptotic-with-respect-to-constants">Asymptotic with respect to Constants</h4>
<p>Notice that while the limit definitions above use limits to infinity and the growth definitions used a similar notion, asymptotic notation can be generalized to limits to any constant and not just infinity. For example, with big-$O$ this looks like:</p>

<script type="math/tex; mode=display">f(x)\in O(g(x)) \ \ (\text{as }x\to c)</script>

<p>In this more general case, the asymptotic notations are actually families of notations indexed by some constant $c$. We can modify the limit definitions given above by simply replacing $x\to\infty$ with $x\to c$.</p>

<p>That said, the $x\to\infty$ case is so much more common that when no approached value is specified, we will assume this to be the case. In other words:</p>

<script type="math/tex; mode=display">f(x)\in O(g(x)) \ \ (\text{as }x\to\infty)\iff f(x)\in O(g(x))</script>

<h2 id="big-o-vs-omega-vs-theta">Big $O$ vs. $\Omega$ vs. $\Theta$</h2>
<h3 id="big-o">Big $O$</h3>
<p>Big $O$, the most used notation, describes what is called the <strong>upper-bound complexity</strong> of an algorithm. Thus a function $f$ that is big $O$ of a function $g$ grows as fast or slower than $g$ (up to a constant). When a function $f$ is <strong>‘of the order’</strong> $g$ this means that it is big $O$ of $g$ and not any other notation.</p>

<h4 id="popularity">Popularity</h4>
<p>While big $\Theta$ is the most descriptive of the 3 notations, it is not as popular as big $O$. Why is this? Well, it is in part due to laziness and convention.</p>

<p>However there is a more practical reason for this as well. Proving something is big $\Theta$ means proving it is both big $O$ and big $\Omega$ which may be much more difficult or even impossible compared to just proving one or the other.</p>

<p>As a result of this, it is generally expected that when someone states an algorithm is $O(g(x))$ that $g(x)$ is as small as they could make it. This makes it a tighter bound when possible but doesn’t enforce it incase such a bound is not possible to prove.</p>

<h4 id="common-orders">Common Orders</h4>
<p>Below is a table of common terminology for certain lower-bound complexity classes (from smallest to largest):</p>

<table>
  <thead>
    <tr>
      <th>Complexity Class</th>
      <th>Notation</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Constant</td>
      <td>$O(1)$</td>
    </tr>
    <tr>
      <td>Logarithmic</td>
      <td>$O(\log n)$</td>
    </tr>
    <tr>
      <td>Polylogarithmic</td>
      <td>$O((\log n)^k)$</td>
    </tr>
    <tr>
      <td>Fractional Power</td>
      <td>$O(n^k)_{\ 0&lt;k&lt;1}$</td>
    </tr>
    <tr>
      <td>Linear</td>
      <td>$O(n)$</td>
    </tr>
    <tr>
      <td>Linearithmic</td>
      <td>$O(n\log n)$</td>
    </tr>
    <tr>
      <td>Quadratic</td>
      <td>$O(n^2)$</td>
    </tr>
    <tr>
      <td>Polynomial</td>
      <td>$O(n^k)_{\ k&gt;1}$</td>
    </tr>
    <tr>
      <td>Exponential</td>
      <td>$O(k^n)_{\ k&gt;1}$</td>
    </tr>
    <tr>
      <td>Factorial</td>
      <td>$O(n!)$</td>
    </tr>
  </tbody>
</table>

<details>
<summary><h3 class="inline">Big $\Omega$</h3></summary>
This contrasts with big $\Omega$ which describes the <b>lower-bound complexity</b> of an algorithm. So a function $f$ that is big $\Omega$ of a function $g$ grows as fast or faster than $g$ (up to a constant).
<p></p>
</details>

<details>
<summary><h3 class="inline">Big $\Theta$</h3></summary>
Big $\Theta$, on the other hand, is known as a <bf>tight bound</bf> on the complexity of an algorithm and can be equivalently stated as:

$$f(x)\in\Theta(g(x))\equiv f(x)\in O(g(x))\wedge f(x)\in\Omega(g(x))$$

And so, following the pattern, a function $f$ that is big $\Theta$ of a function $g$ grows as fast as $g$ (up to a constant).

<h4>Equivalence Relation</h4>
Note that, unlike big $O$ and $\Omega$, the relation $f(x)\in\Theta(g(x))$ forms an equivalence relation given some function $g$.

<details>
<summary><strong>Reflexivity</strong></summary>
I'll do this later.
</details>

<details>
<summary><strong>Symmetry</strong></summary>
I'll do this later.
</details>

<details>
<summary><strong>Transitivity</strong></summary>
I'll do this later.
</details>

<h4>Asymptotic Equivalence</h4>
Another interesting point is that $f(x)\in\Theta(g(x))$ is actually a more general case of $f(x)\sim g(x)$ where $\sim$ is <a href="\asymptotic-equivalence#relation-to-algorithmic-complexity">asymptotic equivalence</a>. This should come as no surprise. After all, $f$ and $g$ are asymptotically equal up to a constant, and so if that constant happened to be $1$ we would be left with asymptotic equivalence.
</details>

<h2 id="regarding-notation">Regarding Notation</h2>
<p>The use of the Bachmann–Landau notations is, unfortunately, quite varied and oftentimes informal. As such, I will try to clear up some of the nuance packed into these notations.</p>

<h4 id="terminology">Terminology</h4>
<p>When someone refers to the complexity of an algorithm without context they are assumed to be referring to its temporal, rather than its spatial, complexity. Similarly, when the order of an algorithm (its big $O$) is given without context it is assumed to be the algorithm’s <strong>worst-case</strong> complexity (as opposed to the best/average case).</p>

<h4 id="equality-vs-membership">Equality vs Membership</h4>
<p>While I’ve used the notation $f(x)\in O(g(x))$ throughout this post as well as in my other writing, it is more common to write:</p>

<script type="math/tex; mode=display">f(x)=O(g(x))\equiv f(x)\in O(g(x))</script>

<p>This is an <strong>abuse of notation</strong> and does not <em>really</em> denote equality between the function $f$ and the class of functions on the order of $g$. It is merely a shorthand and, as we’ll see below, this abuse can be taken to a much farther extreme…</p>

<details>
<summary><h4 class="inline">Equational Notation</h4></summary>
It is possible to use, for example, big $O$ notation in equations, extending the equality <i>analogy</i> that is common practice. For instance:

$$n^{O(1)}=O(e^n)$$

Is equivalent to the following, more formal, proposition:

$$(\exists f,g)\left(f\in O(1)\wedge g\in O(e^n)\right)n^{f(n)}=g(n)$$

And in general, the equation means that if all the big $O$'s on both sides were replaced with some function in that class, the equation is true. There just has to exist one set of functions that make the equation true.
<p></p>
</details>

<details>
<summary><h4 class="inline">$n$ vs. $x$</h4></summary>
Although I used the variable $x$ in the definitions given above, the variable $n$ is more common in Bachmann-Landau notation. This is because $x$ is conventionally used to denote a continuous valued, real variable, while $n$ is conventionally used to denote a discrete, integer valued variable. This jives with the fact that these notations are mostly used in describing the computational complexity of algorithms expressed as functions of their, discrete sized, inputs (e.g there are no lists of size $2.5$ and even further to the point, we cannot subdivide the bit).
<p></p>
</details>

<h2 id="properties">Properties</h2>
<p>I am omitting the $(x)$ after the functions (which, strictly speaking, shouldn’t have been present in the first place) for readability:</p>

<ul>
  <li>$f_1\in O(g_1)\wedge f_2\in O(g_2)\rightarrow f_1+f_2\in O(g_1+g_2)$</li>
  <li>$f_1\in O(g_1)\wedge f_2\in O(g_2)\rightarrow f_1f_2\in O(g_1g_2)$</li>
  <li>$f\cdot O(g)\in O(fg)$</li>
  <li>$kf\in O(g)\rightarrow f\in O(g)$</li>
  <li>$O(kg)=O(g)$</li>
</ul>

<p><em>Where $k$ is some constant</em></p>

<h2 id="examples">Examples</h2>
<ul>
  <li>$5x^3\in O(x^3)$ Same most significant term (times a constant).</li>
  <li>$x^2\in O(x^3)$ Lower than most significant term.</li>
  <li>$2n^2\in O(n!+n)$ Lower than most significant term.</li>
  <li>
    <p>$x^3\not\in O(x^2)$ Can’t grow faster than function.</p>
  </li>
  <li>$2x^2\in\Omega(x^2)$ Same most significant term (times a constant).</li>
  <li>$x^3\in\Omega(x)$ Greater than most significant term.</li>
  <li>$n!\in\Omega(3n^2+2n)$ Greater than most significant term.</li>
  <li>
    <p>$x^3\not\in\Omega(x^4)$ Can’t grow slower than function.</p>
  </li>
  <li>$2x^2\in\Theta(x^2)$ Same most significant term (times a constant).</li>
  <li>$x^3\in\Theta(15x^3+4x^2)$ Same most significant term (times a constant).</li>
  <li>$n!\not\in\Theta(3n^2+2n)$ Most significant terms don’t match.</li>
</ul>

</div>

<div class="related">
  <h2>Related Posts</h2>
  <ul class="related-posts">
    
      <li>
        <h3>
          <a href="/bachmann-landau-notation/">
            Bachmann–Landau Notation
            <small>28 Sep 2018</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/iverson-bracket/">
            Iverson Bracket
            <small>27 Aug 2018</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/asymptotic-equivalence/">
            Asymptotic Equivalence
            <small>26 Aug 2018</small>
          </a>
        </h3>
      </li>
    
  </ul>
</div>

    </div>
  </body>
</html>
