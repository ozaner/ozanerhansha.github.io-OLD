<!DOCTYPE html>
<html lang="en-us">
  <head>
  <link href="https://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
  <title>
    
      Playing Atari Pong with Reinforcement Learning &middot; Ozaner
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-144-precomposed.png">
                                 <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- Load JQuery (JavaScript)-->
  <script src="https://code.jquery.com/jquery-2.2.4.min.js"></script>

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      positionToHash: true,
      jax: ["input/TeX", "output/HTML-CSS"],
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$', '$$']],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      },
      messageStyle: "none",
      "HTML-CSS": {
        preferredFont: "TeX",
        availableFonts: ["STIX","TeX"],
        styles: {".MathJax": {color: "#CCCCCC"}}
      },
      displayAlign: "center!important",
      displayIndent: "0em"
    });
    MathJax.Hub.Queue(function() {
      // Fix <code> tags after MathJax finishes running. This is a
      // hack to overcome a shortcoming of Markdown. Discussion at
      // https://github.com/mojombo/jekyll/issues/199
      var all = MathJax.Hub.getAllJax(), i;
      for(i = 0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
      }
    });
  </script>

  <!-- Adds MathJax -->
  <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>

  <!-- Adds Awesome Icons-->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">

  <!-- Collects tags of all posts-->
  
    






  

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-111962015-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-111962015-1');
  </script>
</head>

  <body>

    <div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/">
          $$\large{\text {Oz}}\tiny{\underbrace{\large {\text {∀n∈ℝ}}}_{\tiny {\text {For all real n...}}}}$$
        </a>
      </h1>
      <p class="lead">A compilation of my notes, projects, and articles.</p>
    </div>

    <nav class="sidebar-nav">
      <a href="https://medium.com/@ozanerhansha" target="_blank">Articles</a><br/>

      

      
      
        
          
        
      
        
          
        
      
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/notes/">Notes</a>
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/projects/">Projects</a>
          
        
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/references/">Interesting References</a>
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      

      <style>
        p.small {
            line-height: 0.7;
        }
      </style>

      <br>

      <p class="small">
        <!--- Icon Links -->
        <b>Links: </b>
        <a href="https://github.com/ozanerhansha"><i class='fa fa-github'></i></a>
        <!-- <a href="https://www.quora.com/profile/Ozaner-Hansha/questions"><i class='fa fa-quora'></i></a> -->
        <a href="https://medium.com/@ozanerhansha"<i class='fa fa-medium'></i></a>
        <!-- <a href="https://math.stackexchange.com/users/490433/ozaner-hansha?tab=questions"><i class='fa fa-stack-exchange'></i></a> -->

        <br>
        <font size="2"><a href="https://github.com/ozanerhansha/ozanerhansha.github.io">This Website's Code</a></font>
      </p>
    </nav>
    <p>2018 Ozaner Hansha</p>
  </div>
</div>


    <div class="content container">
      <div class="post">
  <h1 class="post-title">Playing Atari Pong with Reinforcement Learning</h1>
  <span class="post-date">March 18, 2018</span>
  <span>[
  
    
    <a href="/tag/computer-science"><code class="highligher-rouge"><nobr>computer-science</nobr></code>&nbsp;</a>
  
    
    <a href="/tag/machine-learning"><code class="highligher-rouge"><nobr>machine-learning</nobr></code>&nbsp;</a>
  
    
    <a href="/tag/projects"><code class="highligher-rouge"><nobr>projects</nobr></code>&nbsp;</a>
  
]</span>
  <h2 id="background">Background</h2>
<p>In 2013 the relatively new AI startup DeepMind released their paper <a href="https://arxiv.org/pdf/1312.5602.pdf"><em>Playing Atari with Deep Reinforcement Learning</em></a> detailing an artificial neural network that was able to play, not 1, but 7 Atari games with human and even super-human level proficiency. What made this paper so astounding was the fact that it was a single, general purpose neural network (a <strong>general artificial intelligence</strong> if you will) that could be trained to play all these games rather than 7 separate ones.</p>

<p>If this wasn’t enough, in 2015 they blew the machine learning community, and everyone else considering the news coverage, away with their paper <a href="https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf"><em>Human-level control through deep reinforcement learning</em></a> in which they construct what they call a <strong>Deep Q Network</strong> (DQN) to play <em>42</em> different Atari games, all of varying complexity, with performance that exceeded a professional human player.<a href="https://deepmind.com/research/dqn/"><sup>1</sup></a></p>

<!--more-->

<!-- ![xkcd](https://imgs.xkcd.com/comics/pong.png?style=centerme)
<center><i>Relevant xckd</i></center> -->

<h2 id="q-learning">Q-Learning</h2>
<p>The researchers at Google’s DeepMind achieved this stunning success with a type of machine learning called <strong>reinforcement learning</strong> and more specifically <strong>Q-learning</strong>. In essence, the goal of Q-learning is to approximate some ideal function $Q(s,a)$ that outputs a reward (how good we are doing at the task), where $s$ is a possible state of the environment/game/etc. and $a$ is a possible action to take in that state. If we had such a function, or even a good approximation, we could simply plug in our current state and choose whatever action will maximize $Q$ which would then maximize how well we perform the task. To approximate this function, the researches used a convolutional neural network (CNN) and trained it using Q-learning, thus creating a Deep Q Network. You can read more about Q-learning and DQNs <a href="https://ai.intel.com/demystifying-deep-reinforcement-learning/">here</a>.</p>

<p>By implementing Q-learning in a convolutional neural network (CNN) they create a DQN capable of predicting what actions to take based on the current state of the game.</p>

<h2 id="policy-gradients">Policy Gradients</h2>
<p>That said, Q-learning isn’t the only way to achieve these results. Another popular type of reinforcement learning is what known as <strong>policy gradients</strong>. This method is more direct and conceptually simpler than Q-learning. Essentially, you input the current state, action taken, and reward given at every step and optimize the network accordingly.</p>

<p>And make no mistake, while simpler, policy networks can be just as good as DQNs. In fact, when tuned correctly, they perform even better than DQNs. Don’t believe me? Just ask the authors of the original papers themselves:
<a href="https://arxiv.org/pdf/1602.01783.pdf">Asynchronous Methods for Deep Reinforcement Learning</a>.</p>

<h2 id="my-attempt">My Attempt</h2>
<!-- Encouraged by the aforementioned research, I thought I would attempt to create an ANN capable of playing pong using reinforcement learning. Using OpenAI's [Gym](https://gym.openai.com) package to model [Pong](https://gym.openai.com/envs/Pong-v0/), and Google's [*TensorFlow*](https://www.tensorflow.org/) library to construct the network, I'll attempt to explain the code, its results, and its accuracy. -->
<p>Encouraged by the aforementioned research, I thought I would attempt to create an ANN capable of playing pong using reinforcement learning. To do this I used OpenAI’s <a href="https://gym.openai.com">Gym</a> package to model <a href="https://gym.openai.com/envs/Pong-v0/">Pong</a>, and Google’s <a href="https://www.tensorflow.org/"><em>TensorFlow</em></a> library to construct the network.</p>

<p>The <a href="https://github.com/ozanerhansha/NeuralNetworks/tree/master/src/pongRL">code</a> for this network, dubbed PongNet, is in my <a href="https://github.com/ozanerhansha/NeuralNetworks">NeuralNetwork</a> repository on GitHub.</p>

<h2 id="results">Results</h2>
<p>Learning starts to appear after 1500 games (a game goes on until one player reaches 20 points) and it reaches a 50% win-rate at around 8000 games. More testing needs to be done to see the maximum accuracy of this particular network.</p>

<p><img src="/assets/projects/pongAI/pongdata.png?style=centerme" alt="pongdata" /></p>

<p>Below is the network (on a good day) playing against the same bot it trained with for 10,000 games.</p>

<p><img src="/assets/projects/pongAI/pongai.gif?style=centerme" alt="bc" /></p>

</div>

<div class="related">
  <h2>Related Posts</h2>
  <ul class="related-posts">
    
      <li>
        <h3>
          <a href="/sequential-search/">
            Sequential Search
            <small>08 Oct 2018</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/asymptotic-notation/">
            Asymptotic Notation
            <small>07 Oct 2018</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/iverson-bracket/">
            Iverson Bracket
            <small>27 Aug 2018</small>
          </a>
        </h3>
      </li>
    
  </ul>
</div>

    </div>
  </body>
</html>
