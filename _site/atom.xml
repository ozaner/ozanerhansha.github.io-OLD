<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="https://www.w3.org/2005/Atom">

 <title>Ozaner</title>
 <link href="http://localhost:4000/atom.xml" rel="self"/>
 <link href="http://localhost:4000/"/>
 <updated>2018-05-10T19:39:15-04:00</updated>
 <id>http://localhost:4000</id>
 <author>
   <name>Ozaner Hansha</name>
   <email></email>
 </author>

 
 <entry>
   <title>Parity Sequences</title>
   <link href="http://localhost:4000/parity-sequences/"/>
   <updated>2018-05-10T00:00:00-04:00</updated>
   <id>http://localhost:4000/parity-sequences</id>
   <content type="html">&lt;!--
New Date is formal reformulation of parity sequences

Thought about this when we had to find Taylor series' for certain sinusoidal functions and the terms had hard to capture patterns of negative/even terms. It is possible to capture this via more sinusoidal functions but I wanted a polynomial answer. I don't think there is a polynomial answer for the general case, and the 2 examples below are the extent of my findings.

first publish date: 2018-03-03--&gt;

&lt;p&gt;While toying around with infinite alternating series, I become interested in the idea of alternation beyond the simple $+,-,+,-$ like that of the power series of $\sin x$. This is where &lt;strong&gt;parity sequences&lt;/strong&gt; come in.&lt;/p&gt;

&lt;p&gt;A parity sequence is a pattern of alternating even and odd numbers. Only the parity of the numbers is important, not the actual values themselves. As such, we can represent the parity sequence of a sequence as a list of “$+$” for even and “$-$” for odd numbers. Below I formally define what the notion of a parity sequence, and some of its properties.&lt;/p&gt;

&lt;h2 id=&quot;definition-and-equivalence&quot;&gt;Definition and Equivalence&lt;/h2&gt;
&lt;p&gt;Consider the set of all integer sequences (that is, all functions from the natural numbers to the integers):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbb{Z}^\mathbb{N}\equiv\{f\mid f:\mathbb{N}\to\mathbb{Z}\}&lt;/script&gt;

&lt;!--more--&gt;

&lt;p&gt;We can define an equivalence relation $\sim$ on $\mathbb{Z}^\mathbb{N}$ that relates sequences that have the same parity for each element:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sim\equiv\{\left(a_n,b_n\right)\in (\mathbb{Z}^\mathbb{N})^2\mid\forall n\in\mathbb{N},\ (-1)^{a_n}=(-1)^{b_n}\}&lt;/script&gt;

&lt;p&gt;Or in English: if $-1$ to the $n$th element of sequence $a_n$ equals $-1$ to the $n$th element of sequence $b_n$, then the sequences are &lt;strong&gt;parity equivalent&lt;/strong&gt;. We can write this more succinctly as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
a_n\sim b_n&amp;\equiv (a_n,b_n)\in \sim\\
&amp;\equiv (-1)^{a_n}=(-1)^{b_n}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Using this notion of parity equivalence, we can say that a parity sequence of a given sequence $S_n$ is simply its equivalence class in $\sim$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;[S_n]=\{x\in S_\mathbb{Z}\mid S_n\sim x\}&lt;/script&gt;

&lt;h2 id=&quot;-and---notation&quot;&gt;$+$ and $-$ Notation&lt;/h2&gt;
&lt;p&gt;Because a parity sequence is uniquely defined by the parity of the elements of any one of its members, we can represent the parity sequence of a sequence by replacing all its even terms with a $+$ and its odd terms with a $-$. For example:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;n^2=\{0,1,4,9,16,25,36,49,\cdots\}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;[n^2]=\{+,-,+,-,+,-,+,-,\cdots\}&lt;/script&gt;

&lt;h2 id=&quot;parity-swap&quot;&gt;Parity Swap&lt;/h2&gt;
&lt;p&gt;A useful property to take note of when constructing these sequences is that adding $1$ to a sequence $P$ flips the parity of each element in the sequence. After performing this &lt;strong&gt;parity swap&lt;/strong&gt;, we call the resulting parity sequence $\bar{P}$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P_n=[n^2]=\{+,-,+,-,+,-,+,\cdots\}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\bar{P_n}=[n^2+1]=\{-,+,-,+,-,+,-,\cdots\}&lt;/script&gt;

&lt;p&gt;This is an obvious consequence of the fact that an even number plus $1$ is an odd number and vice versa for odd numbers.&lt;/p&gt;

&lt;h2 id=&quot;eveningodding-out&quot;&gt;Evening/Odding Out&lt;/h2&gt;
&lt;p&gt;When any parity sequence is multiplied by $2$ it becomes parity equivalent to $2n$. This is called &lt;strong&gt;evening out&lt;/strong&gt; a sequence:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;[2P_n]=\{+,+,+,+,+,\cdots\}&lt;/script&gt;

&lt;p&gt;Similarly, when any parity sequence is multiplied by $2$ then increased by $1$, it becomes parity equivalent to $2n+1$. This is called &lt;strong&gt;odding out&lt;/strong&gt; a sequence:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;[2P_n+1]=\{-,-,-,-,-,\cdots\}&lt;/script&gt;

&lt;h2 id=&quot;uses&quot;&gt;Uses&lt;/h2&gt;
&lt;p&gt;Notice that when we raise $-1$ to the power of one of these sequences, it will evaluate to $+1$ for even values and $-1$ for odd values.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;This property is where the $+$ $-$ notation comes from.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;When constructing things like Taylor Series’, this is a useful property to have. Terms that alternate from negative to positives can be dealt with by appending a factor of $(-1)^{P_n}$ to the series.&lt;/p&gt;

&lt;h2 id=&quot;examples&quot;&gt;Examples&lt;/h2&gt;
&lt;p&gt;All examples start indexing at 0 (the most rational way to index lists).&lt;/p&gt;
&lt;h4 id=&quot;base-case-parity-sequences&quot;&gt;Base Case Parity Sequences&lt;/h4&gt;
&lt;p&gt;The two most simple parity sequences are simply the natural numbers:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;n=\{0,1,2,3,4,5,6,7,\cdots\}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;[n]=\{+,-,+,-,+,-,+,-,\cdots\}&lt;/script&gt;

&lt;p&gt;and the even numbers:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;2n=\{0,2,4,6,8,10,12,14,\cdots\}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;[2n]=\{+,+,+,+,+,+,+,+,\cdots\}&lt;/script&gt;

&lt;p&gt;These two sequences can be parity swapped to form the parity sequences ${-,+,-,+,-,+,\cdots}$ and ${-,-,-,-,-,-,\cdots}$ respectively.&lt;/p&gt;

&lt;h4 id=&quot;the-fibonacci-sequence&quot;&gt;The Fibonacci Sequence&lt;/h4&gt;
&lt;p&gt;The Fibonacci sequence can be considered a parity sequence with the pattern:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
F_n&amp;=F_{n-2}+F_{n-1}\\
&amp;=\{0,1,1,2,3,5,8,13,\cdots\}\\
\end{align} %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{where } F_0=0 \land F_1 = 1&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;[F_n]=\{+,-,-,+,-,-,+,-,\cdots\}&lt;/script&gt;

&lt;details&gt;
  &lt;summary&gt;Closed Form&lt;/summary&gt;
  &lt;p&gt;

  $$\begin{align}
  F_n=\frac{\phi^n-\psi^n}{\sqrt 5}&amp;amp;=\{0,1,1,2,3,5,8,13,\cdots\}
  \end{align}$$

  $$[F_n]=\{+,-,-,+,-,-,+,-,\cdots\}$$

  $$\begin{align*}
  \text{where } &amp;amp;\phi=\frac{1+\sqrt 5}{2} \text{ (the golden ratio)}\\
  &amp;amp;\psi=\frac{1-\sqrt 5}{2} \text{ (the conjugate golden ratio)}
  \end{align*}$$
  &lt;/p&gt;
&lt;/details&gt;

&lt;h4 id=&quot;other-examples&quot;&gt;Other Examples&lt;/h4&gt;
&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{alignat*}{2}
  \frac{n(n-1)}{2}&amp; &amp;&amp;= \{0,0,1,3,6,10,15,21,\cdots\} &amp;&amp; \\
  \left[\frac{n(n-1)}{2}\right]&amp; &amp;&amp;=\{+,+,-,-,+,+,-,-,\cdots\} &amp;&amp; \\\\
  \frac{n^2(n-1)}{2}&amp; &amp;&amp;= \{0,0,2,9,24,50,90,630,\cdots\} &amp;&amp;\\
  \left[\frac{n^2(n-1)}{2}\right]&amp; &amp;&amp;=\{+,+,+,-,+,+,+,-,\cdots\}&amp;&amp;\\\\
  \cos \frac{n\pi}{2}&amp; &amp;&amp;= \{1,0,-1,0,1,0,-1,0,\cdots\} &amp;&amp;\\
  \left[\cos \frac{n\pi}{2}\right]&amp; &amp;&amp;=\{-,+,-,+,-,+,-,+,\cdots\}&amp;&amp;
\end{alignat*} %]]&gt;&lt;/script&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>On Culpability</title>
   <link href="http://localhost:4000/on-culpability/"/>
   <updated>2018-04-25T00:00:00-04:00</updated>
   <id>http://localhost:4000/on-culpability</id>
   <content type="html">&lt;h3 id=&quot;scenario-1&quot;&gt;Scenario 1&lt;/h3&gt;
&lt;p&gt;Consider a man driving home late after having a few too many drinks. While drunk he decides to run a red light. Nothing comes of it and he manages to arrive home safe and sound.&lt;/p&gt;

&lt;h3 id=&quot;scenario-2&quot;&gt;Scenario 2&lt;/h3&gt;
&lt;p&gt;Now imagine this same man, under the same circumstances; a man who had just as many drinks, a man who decided to drive while inebriated at the same late hour.&lt;/p&gt;

&lt;p&gt;This time he still decides to run the red light but, unlike in the other situation, he blindsides a little girl walking across the street. He puts on the brakes as soon as he sees her but, his reaction time being delayed, runs over and kills her.&lt;/p&gt;

&lt;!--more--&gt;

&lt;h3 id=&quot;result&quot;&gt;Result&lt;/h3&gt;
&lt;p&gt;Gruesome to be sure, but consider the repercussions of his actions in both cases. In one instance the man got home scot-free, with nothing more than a traffic ticket looming over his head.&lt;/p&gt;

&lt;p&gt;In the other instance however, he was charged and convicted of manslaughter.&lt;/p&gt;

&lt;p&gt;Both instances of the man made the same, poor decisions up to the point of the accident. And neither wanted to kill anybody that night. What then delineates these two situations? What makes the man a murderer in one situation and in the other just a drunk driver?&lt;/p&gt;

&lt;p&gt;Luck.&lt;/p&gt;

&lt;h3 id=&quot;why&quot;&gt;Why?&lt;/h3&gt;
&lt;p&gt;If you were to ask someone whether the one who ran the red-light with no consequence or the one who ran over a pedestrian was more guilty, you would have good reason to believe they would say the pedestrian killer. And even if we were to concede that, at least by intent, the two drunk men were equally culpable, it still just doesn’t sit right with us. How could it be that a murderer is just as evil as a reckless driver? Does it seem wrong that one be locked up for life and the other lose a star on their driver’s license?&lt;/p&gt;

&lt;p&gt;Well… I don’t know. Why is it that a 17 year old, a day before his birthday, isn’t fully responsible for his actions but once he is 18 is considered independent by the law? How can anybody be culpable if they are a product of their parents, environment, genetics, etc? What is culpability?&lt;/p&gt;

&lt;!-- HOW CAN CULPABILITY EXITTS WITHOUT FREE WILL --&gt;

&lt;p&gt;Well someone has to be culpable. You have to draw the line in the sand somewhere. Why? Well it seems that punishment may not be dealt out to punish those who have evil intent but instead, to make an affirmation that the effects their intent caused are unwanted.&lt;/p&gt;

&lt;p&gt;When a court decides that the pedestrian killer should serve 15 years in jail and the red light offender shouldn’t, they may very well be considering his intent, but they also consider the effects his intent has caused, whether it be in his control or not. A judiciary in a government, I’d say, serves to remove and make examples of those who do not follow the norms of society and who may pose danger to it. Whether or not they are more culpable then the next driver they still killed someone and so we make sure to discourage this behavior not because we care for their intent but because we care for their effects.&lt;/p&gt;

&lt;p&gt;“But wait”, many have objected, “punishing people for their actions in this way rather than offering support to correct their behavior is a misguided tactic in reducing these unwanted effects (stealing, murder, crime, etc.).” And indeed, these objectors may very well be right. So then is there another reason that such methods are employed if more effective ones exist? Is there something fundamental about punishing others for actions a society does not like? I think so.&lt;/p&gt;

&lt;p&gt;It boils down to our own feelings and behaviors. &lt;em&gt;‘Our’&lt;/em&gt; meaning human. Of course it doesn’t feel right to punish the two men in the same way, whether we lock them both up or simply take a star from both of their licenses. We &lt;em&gt;FEEL&lt;/em&gt; like they must be dealt with differently. It is ultimately our own instincts that drive our behavior as a society. The system of justice we have today is not the product not careful reasoning, but an ad-hoc solution. One begot from evolutionary processes. Processes that took time immemorial just to get a working solution, much less an optimal one…&lt;/p&gt;

&lt;!-- MORALITY TOO IS NOTHING BUT A FARCE TO COVER UP OUR INSTINCTS --&gt;

&lt;!-- EVOLTION IS A HIGYH LEVEL LOCAL OPTIMIZATION PROBLEM --&gt;
</content>
 </entry>
 
 <entry>
   <title>n-Tuples</title>
   <link href="http://localhost:4000/n-tuples/"/>
   <updated>2018-04-23T00:00:00-04:00</updated>
   <id>http://localhost:4000/n-tuples</id>
   <content type="html">&lt;p&gt;An $n$-tuple is an ordered list of $n$ elements. It is dissimilar to a set in that the order of its elements matter, it must be finite, and it can contain multiples of the same element.&lt;/p&gt;

&lt;p&gt;Here are some common names for tuples of different size:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;$n$-tuple&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Name(s)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;$0$&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;null-tuple, $0$-tuple&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;$1$&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;singleton, $1$-tuple&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;$2$&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;ordered pair, $2$-tuple&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;$3$&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;ordered triplet, $3$-tuple&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;$n$&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;$n$-tuple&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;em&gt;My &lt;a href=&quot;https://medium.com/@ozanerhansha/the-ordered-pair-and-set-theory-69aa6e2b8a32&quot;&gt;medium article&lt;/a&gt; on ordered pairs and their uses.&lt;/em&gt;&lt;/p&gt;

&lt;!--more--&gt;

&lt;h2 id=&quot;definition&quot;&gt;Definition&lt;/h2&gt;
&lt;h4 id=&quot;2-tuples&quot;&gt;$2$-tuples&lt;/h4&gt;
&lt;p&gt;We define ordered pairs, or $2$-tuples, as &lt;a href=&quot;https://en.wikipedia.org/wiki/Kazimierz_Kuratowski&quot;&gt;Kuratowski&lt;/a&gt; did:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;(x_1,x_2)=\{\{x_1\},\{x_1,x_2\}\}&lt;/script&gt;

&lt;p&gt;The above definition distinguishes $a$ from $b$ in that both sets in the right set contain $a$ but only one contains $b$. This is what gives the pair order.&lt;/p&gt;

&lt;h4 id=&quot;n-tuples&quot;&gt;$n$-tuples&lt;/h4&gt;
&lt;p&gt;Using the $2$-tuple, we can define all $n$-tuples recursively as an ordered pair of an $(n-1)$-tuple and another element. The $3$-tuple, for example, would be:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
(x_1,x_2,x_3)&amp;=((x_1,x_2),x_3)\\
&amp;=\{\{(x_1,x_2)\},\{(x_1,x_2),x_3\}\}\\
&amp;=\{\{\{\{x_1\},\{x_1,x_2\}\}\},\{\{\{x_1\},\{x_1,x_2\}\},x_3\}\}\\
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;And in general, an $n$-tuple for $n&amp;gt;2$ is defined as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;(x_1,x_2,\cdots,x_{n-1},x_n)=((x_1,x_2,\cdots,x_{n-1}),x_n)&lt;/script&gt;

&lt;p&gt;&lt;em&gt;The above definition lacks a notion of a $0$ and $1$-tuple. I can’t find a use for them that justifies their complication to the definition of $n$-tuple, so I excluded them. But see &lt;a href=&quot;https://en.wikipedia.org/wiki/Tuple#Tuples_as_nested_sets&quot;&gt;this&lt;/a&gt; for a different definition of $n$-tuples that uses the null-tuple as a base case rather than ordered pairs.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Alternatively, $n$-tuples can also be defined as functions with domains over some finite interval of the positive integers. In this sense, they would be equivalent to finite sequences.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Whatever definition is used, all that matters is that the equality of two $n$-tuples be defined as below:&lt;/p&gt;

&lt;h2 id=&quot;equality&quot;&gt;Equality&lt;/h2&gt;
&lt;p&gt;Two ordered pairs are equivalent if and only if the elements in each of their respective indices are equal. That is to say, for two $n$-tuples $X$ and $Y$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
X&amp;=(x_1,x_2,x_3,\cdots,x_n)\\
Y&amp;=(y_1,y_2,y_3,\cdots,y_n)
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;$X$ and $Y$ are only equal if:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
X=Y&amp;\equiv\forall k\in\mathbb{N}_n^* (x_k=y_k)\\
&amp;\equiv x_1=y_1 \land x_2=y_2 \land \cdots \land x_n=y_n\\
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;&lt;em&gt;Where $\mathbb{N}_n^* $ is the set of all positive integers from and including $1$ to $n$.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Note that for this definition to work, the two tuples have to be the same size. As such, tuples of different sizes will never be equal.&lt;/p&gt;

&lt;h2 id=&quot;extraction&quot;&gt;Extraction&lt;/h2&gt;
&lt;h4 id=&quot;first-element&quot;&gt;First Element&lt;/h4&gt;
&lt;p&gt;To extract the first element $\pi_1(P)$ of an ordered pair $P=(a,b)$ we can use the following construction:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\pi_1(P)=\bigcup\bigcap P=a&lt;/script&gt;

&lt;details&gt;&lt;summary&gt;Proof&lt;/summary&gt;
  &lt;b&gt;Lemma 1&lt;/b&gt;
  &lt;p&gt;To make proving the above statement easier, it would help to prove that the arbitrary union of a set of an element $\{x\}$ is that element $x$:

  $$\bigcup \{x\}=x$$

  First let's start with the definition of the arbitrary union of a set $S$:

  $$\bigcup S=\{a\mid \left(\exists b\in S\right)a\in b\}$$

  In in English this means, all elements $a$ that are contained in at least one set $b$ that are contained in $S$. (i.e the union of all the elements in $S$). Plugging $\{x\}$ in for $S$ we see:

  $$\bigcup \{x\}=\{a\mid \left(\exists b\in \{x\}\right)a\in b\}$$

  Since there is only one element in ${b}$, namely $x$, there is only one set $b$ could be: $x$. So, we can say the following:

  $$\bigcup \{x\}=\{a\mid a\in x\}$$

  And since the set of all elements in $x$ is simply that same set:

  $$\boxed{\bigcup \{x\}=x}$$
  &lt;/p&gt;
  &lt;b&gt;The Proof&lt;/b&gt;
  &lt;p&gt;

  $$\begin{align}
  \pi_1(P)&amp;amp;=\bigcup\bigcap P\\
  &amp;amp;=\bigcup\bigcap \{\{a\},\{a,b\}\}\\
  &amp;amp;=\bigcup \left(\{a\}\cap\{a,b\}\right)\\
  \end{align}$$

  Of course the only element in common between $\{a\}$ and $\{a,b\}$ is $a$ so:

  $$\pi_1(P)=\bigcup \{a\}$$

  And from Lemma 1, we know this equals:

  $$\pi_1(P)=\bigcup \{a\}=a$$

  And indeed, $a$ is the first element of the ordered pair $P$.
  &lt;/p&gt;
&lt;/details&gt;

&lt;h4 id=&quot;second-element&quot;&gt;Second Element&lt;/h4&gt;
&lt;p&gt;The second element $\pi_2(P)$ of an ordered pair $P=(a,b)$ can be found like so:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\pi_2(P)=\bigcup\{x\in\bigcup P\mid\bigcup P\not=\bigcap P\implies x\not\in\bigcap P\}=b&lt;/script&gt;

&lt;details&gt;&lt;summary&gt;Proof&lt;/summary&gt;
I'll do it later...
&lt;/details&gt;

&lt;h4 id=&quot;extracting-elements-of-n-tuples&quot;&gt;Extracting Elements of $n$-Tuples&lt;/h4&gt;
&lt;p&gt;You may have noticed that the above definitions only apply to $2$-tuples. What about $n$-tuples? Well, as it turns out, we can extract the elements of an $n$-tuple of any size by recursively using the $\pi_1$ and $\pi_2$ functions we defined above.&lt;/p&gt;

&lt;p&gt;The notation we’ll use is as follows: $\pi^n_a(P)$ is the $a$th element of the $n$-tuple $P=\left(x_1,x_2,x_3,\cdots,x_n\right)$.&lt;/p&gt;

&lt;p&gt;The first element of $P$ can be found like so:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\pi^n_1(P)=\underbrace{\pi_1\circ\cdots\circ\pi_1}_{n-1\text{ iterations}}(P)=x_1&lt;/script&gt;

&lt;p&gt;For any element &lt;em&gt;other&lt;/em&gt; than the first, we can use the following formula:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\pi^n_a(P)=\pi_2\circ\underbrace{\pi_1\circ\cdots\circ\pi_1}_{n-a\text{ iterations}}(P)=x_a&lt;/script&gt;

&lt;details&gt;&lt;summary&gt;&quot;Proof&quot; &amp;amp; Intuition&lt;/summary&gt;
Not really a proof, I just wrote down how to find the elements of $2,3,4,5$-tuples and found the pattern:

$$\begin{align}
  &amp;amp;2\text{-tuple}\left\{
    \begin{array}{l}
      \pi^2_1=\pi_1(P)\\
      \pi^2_2=\pi_2(P)\\
    \end{array}
  \right.\\
  &amp;amp;3\text{-tuple}\left\{
    \begin{array}{l}
      \pi^3_1=\pi_1\left(\pi_1\left(P\right)\right)\\
      \pi^3_2=\pi_2\left(\pi_1\left(P\right)\right)\\
      \pi^3_3=\pi_2\left(P\right)\\
    \end{array}
  \right.\\
  &amp;amp;4\text{-tuple}\left\{
    \begin{array}{l}
      \pi^4_1=\pi_1\left(\pi_1\left(\pi_1\left(P\right)\right)\right)\\
      \pi^4_2=\pi_2\left(\pi_1\left(\pi_1\left(P\right)\right)\right)\\
      \pi^4_3=\pi_2\left(\pi_1\left(P\right)\right)\\
      \pi^4_4=\pi_2\left(P\right)\\
    \end{array}
  \right.\\
  &amp;amp;5\text{-tuple}\left\{
    \begin{array}{l}
      \pi^5_1=\pi_1\left(\pi_1\left(\pi_1\left(\pi_1\left(P\right)\right)\right)\right)\\
      \pi^5_2=\pi_2\left(\pi_1\left(\pi_1\left(\pi_1\left(P\right)\right)\right)\right)\\
      \pi^5_3=\pi_2\left(\pi_1\left(\pi_1\left(P\right)\right)\right)\\
      \pi^5_4=\pi_2\left(\pi_1\left(P\right)\right)\\
      \pi^5_5=\pi_2\left(P\right)\\
    \end{array}
  \right.\\
\end{align}$$

Why are is there a conditional definition of the $a$th element of an $n$-tuple? What causes this asymmetry? Well it must be the fact that our base case in defining $n$-tuples was the ordered pair rather than some sort of $1$-tuple. Although it's possible that starting with a $1$-tuple wouldn't change this conditional...
&lt;/details&gt;

&lt;h2 id=&quot;some-uses&quot;&gt;Some Uses&lt;/h2&gt;
&lt;h4 id=&quot;cartesian-product&quot;&gt;&lt;a href=&quot;/cartesian-product&quot;&gt;Cartesian Product&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;Ordered pairs are necessary in defining the Cartesian product, which in turn are used to define relations, functions, coordinates, etc.&lt;/p&gt;

&lt;h4 id=&quot;mathematical-structures&quot;&gt;Mathematical Structures&lt;/h4&gt;
&lt;p&gt;Tuples are often used to encapsulate sets along with some operator or relation into a complete mathematical structure. One example is a graph which is defined as an ordered pair $G=(V,E)$ where $V$ is a set of vertices and $E$ a set of edges connecting those vertices. Another example is a group which is defined as an ordered pair $G=(S,\cdot)$ where $\cdot$ is some binary operation on the elements of $S$. Tuples are also used to encapsulate rings, fields, vector spaces, topological spaces, ordered sets, and so on.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Superfactorial and Hyperfactorial</title>
   <link href="http://localhost:4000/superfactorial-and-hyperfactorial/"/>
   <updated>2018-04-18T00:00:00-04:00</updated>
   <id>http://localhost:4000/superfactorial-and-hyperfactorial</id>
   <content type="html">&lt;h3 id=&quot;factorial&quot;&gt;Factorial&lt;/h3&gt;
&lt;p&gt;The factorial function $n!$ is the product of the first $n$ positive integers. We can define it as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;n!=\prod_{k=1}^{n}k=1\times2\times3\times\cdots\times n&lt;/script&gt;

&lt;p&gt;&lt;em&gt;The factorials are sequence &lt;a href=&quot;https://oeis.org/A000142&quot;&gt;A000142&lt;/a&gt; in the OEIS.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;It’s important to note that $0!$ is defined as $1$ because it is an &lt;a href=&quot;https://en.wikipedia.org/wiki/Empty_product&quot;&gt;empty product&lt;/a&gt;. This is not an arbitrary definition and in fact simplifies many formulas involving factorials.&lt;/p&gt;

&lt;p&gt;The function is a mapping from the natural numbers to itself:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;!:\mathbb{N}\rightarrow\mathbb{N}&lt;/script&gt;

&lt;p&gt;&lt;em&gt;Using &lt;a href=&quot;https://en.wikipedia.org/wiki/Analytic_continuation&quot;&gt;analytic continuation&lt;/a&gt; the factorial function can be generalized to complex numbers. The resulting function is dubbed the $\Gamma(z)$ &lt;a href=&quot;https://en.wikipedia.org/wiki/Gamma_function&quot;&gt;gamma function&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;While there is much to discuss about the factorial function, this post concerns itself with two particular extensions of the factorial. Namely the superfactorial and the hyperfactorial.&lt;/p&gt;

&lt;!--more--&gt;

&lt;h3 id=&quot;superfactorial&quot;&gt;Superfactorial&lt;/h3&gt;
&lt;p&gt;We denote the superfactorial of $n$ as $n{$}$. It is defined as the product of the first $n$ factorials. We can define it as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;n$=\prod_{k=1}^{n}k!=1!\times2!\times3!\times\cdots\times n!&lt;/script&gt;

&lt;p&gt;&lt;em&gt;The superfactorials are sequence &lt;a href=&quot;https://oeis.org/A000178&quot;&gt;A000178&lt;/a&gt; in the OEIS.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;The above definition of the superfactorial was created by Neil J.A. Sloane and Simon Plouffe (co-authors of the OEIS). Another definition of superfactorials exists: $n{$}=n!\uparrow\uparrow n!$ where the double arrows denote &lt;a href=&quot;https://en.wikipedia.org/wiki/Tetration&quot;&gt;tetration&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;And, like the factorial function, it is a map from the naturals onto itself:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;$:\mathbb{N}\rightarrow\mathbb{N}&lt;/script&gt;

&lt;p&gt;&lt;em&gt;Like factorial, the superfactorial function can be generalized to the complex numbers, resulting in $G(z)$ the &lt;a href=&quot;https://en.wikipedia.org/wiki/Barnes_G-function&quot;&gt;Barnes G-function&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;hyperfactorial&quot;&gt;Hyperfactorial&lt;/h3&gt;
&lt;p&gt;The hyperfactorial of $n$ is denoted $H(n)$ and is defined as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;H(n)=\prod_{k=1}^{n}k^k=1^1\times2^2\times3^3\times\cdots\times n^n&lt;/script&gt;

&lt;p&gt;&lt;em&gt;The hyperfactorials are sequence &lt;a href=&quot;https://oeis.org/A002109&quot;&gt;A002109&lt;/a&gt; in the OEIS.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Again, we can consider this function to be a map from the naturals onto itself:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;H:\mathbb{N}\rightarrow\mathbb{N}&lt;/script&gt;

&lt;p&gt;&lt;em&gt;The hyperfactorial too can be generalized to the complex numbers. The resulting function is known as $K(z)$ the &lt;a href=&quot;https://en.wikipedia.org/wiki/K-function&quot;&gt;K-function&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;factorial-identity&quot;&gt;Factorial Identity&lt;/h3&gt;
&lt;p&gt;It is possible to relate all three of these factorial variants (factorial, superfactorial and hyperfactorial):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;n{$}\cdot H(n)=n!^{n+1}&lt;/script&gt;

&lt;details&gt;
&lt;summary&gt;&lt;h4 class=&quot;inline&quot;&gt;Proof&lt;/h4&gt;&lt;/summary&gt;

We can prove the above statement, which we'll call $P(n)$, by induction:

$$P(n)\equiv n{$}\cdot H(n)=n!^{n+1}$$

First we multiply both sides of the equation by $(n+1)!(n+1)^{n+1}$:

$$\begin{align}n{$}\cdot H(n)&amp;amp;=n!^{n+1}\\
(n+1)!(n+1)^{n+1}&amp;amp; \ \ \ \ \ (n+1)!(n+1)^{n+1}
\end{align}$$

Now let's simplify the left hand side first. Notice that $n{\$}\cdot(n+1)!=(n+1){\$}$ and that $H(n)\cdot(n+1)^{n+1}=H(n+1)$. From this the left hand side simply becomes:

$$(n+1){$}\cdot H(n+1)$$

Now let's deal with right hand side. Notice that the expression can be rewritten as:

$$\begin{align}
n!^{n+1} \color{green}{(n+1)!}(n+1)^{n+1}&amp;amp;=n!^{n+1}\color{green}{n!(n+1)}(n+1)^{n+1}\\
&amp;amp;=n!^{n+2}(n+1)^{n+2}\\
&amp;amp;=(n+1)!^{n+2}\\
\end{align}$$

Putting the right and left hand sides back together we can see that we just proved $P(n+1)$:

$$P(n+1)\equiv(n+1){$}\cdot H(n+1)=(n+1)!^{n+2}$$

However $P(n+1)$ was proved under the assumption that $P(n)$ was true. Thus:

$$P(n)\implies P(n+1)$$

But, notice that $P(1)$ is true:

$$\begin{align}
P(1)&amp;amp;\equiv 1{$}\cdot H(1)=(1!)^{1+1}\\
&amp;amp;\equiv1\cdot1=1\\
&amp;amp;\equiv T
\end{align}$$

Because $P(1)$ is true, this means that $P(1+1)=P(2)$ is true. This implies that $P(2+1)=P(3)$ is true and so on and so forth for all integers above $1$. Thus by induction:

$$\begin{align}
&amp;amp;P(n)\implies P(n+1)\\
&amp;amp;P(1)\\
\therefore\ &amp;amp;\hline{\forall n\in \mathbb{N},\ P(n)}\\
\end{align}$$
&lt;/details&gt;
</content>
 </entry>
 
 <entry>
   <title>The Dottie Number</title>
   <link href="http://localhost:4000/dottie-number/"/>
   <updated>2018-04-16T00:00:00-04:00</updated>
   <id>http://localhost:4000/dottie-number</id>
   <content type="html">&lt;p&gt;The Dottie number, which I will denote as $\textbf{d}$, is the only real solution to the following equation:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\cos x=x&lt;/script&gt;

&lt;p&gt;&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/thumb/9/98/Dottie_number.svg/800px-Dottie_number.svg.png?style=centerme&quot; alt=&quot;graph&quot; width=&quot;440px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The above is what’s known as a transcendental equation. Equations like these usually return transcendental numbers and indeed $\textbf{d}$ is transcendental. It’s decimal expansion is as follows:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\textbf{d}=0.7390851332151606...&lt;/script&gt;

&lt;p&gt;&lt;em&gt;The Dottie number is sequence &lt;a href=&quot;https://oeis.org/A003957&quot;&gt;A003957&lt;/a&gt; in the OEIS.&lt;/em&gt;&lt;/p&gt;

&lt;!--more--&gt;

&lt;h3 id=&quot;fixed-point&quot;&gt;Fixed Point&lt;/h3&gt;
&lt;p&gt;$\textbf{d}$ is what’s called a fixed point of $\cos x$, because the cosine function maps $\textbf{d}$ to itself. As a result, repeatedly taking the cosine of $\textbf{d}$ returns the same result:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\cos \textbf{d} = \textbf{d} \implies \underbrace{\cos\circ\cos\circ\cdots\circ\cos}_{n\text{ iterations}}\ \textbf{d}=\textbf{d}&lt;/script&gt;

&lt;p&gt;$\textbf{d}$ is the $\cos$ function’s only real fixed point, but there exists infinitely many solutions to $\cos z=z$ for the complex numbers. Those solutions, however, are not attractors.&lt;/p&gt;

&lt;h3 id=&quot;universal-attractor&quot;&gt;Universal Attractor&lt;/h3&gt;
&lt;p&gt;What’s interesting about $\textbf{d}$ is that it’s not just the real fixed point of $\cos$ but also its &lt;strong&gt;universal fixed point attractor&lt;/strong&gt;. That is to say, if you take the cosine of any real number and repeatedly take the cosine of the result, you will always approach $\textbf{d}$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\forall x\in\mathbb{R} \left(\lim_{n\to\infty} \underbrace{\cos\circ\cos\circ\cdots\circ\cos}_{n\text{ iterations}}\ x=\textbf{d}\right)&lt;/script&gt;

&lt;p&gt;In fact, the above is true for a certain range of the complex numbers as well. This range forms the &lt;a href=&quot;https://en.wikipedia.org/wiki/Julia_set&quot;&gt;&lt;em&gt;Julia Set&lt;/em&gt;&lt;/a&gt; of $\cos z$.&lt;/p&gt;

&lt;details&gt;
&lt;summary&gt;&lt;h3 class=&quot;inline&quot;&gt;Proof of Transcendence &lt;/h3&gt;&lt;/summary&gt;

&lt;h4&gt;LWT&lt;/h4&gt;&lt;p&gt;
To prove $\textbf{d}$'s transcendence, we'll need to make use of the &lt;b&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Lindemann%E2%80%93Weierstrass_theorem&quot;&gt;Lindemann–Weierstrass theorem&lt;/a&gt;&lt;/b&gt; (LWT) which states:

$$\forall x\in \mathbb{A}\left(x\not= 0\implies e^x\notin\mathbb{A} \right)$$

Or in English: $e$ to the power of any non-zero algebraic number is not algebraic (i.e transcendental).
&lt;/p&gt;

&lt;h4&gt;Lemma 1&lt;/h4&gt;&lt;p&gt;
The proof will be easier if we first establish the following identity:

$$\begin{align*}
\sin^2 \textbf{d}+\cos^2 \textbf{d} = 1 \tag{Pythagorean theorem}\\
\sin^2 \textbf{d}+ \textbf{d}^2 = 1 \tag{\(\textbf{d}\) is a fixed point}\\
\sin \textbf{d} = \sqrt{1-\textbf{d}^2}
\end{align*}$$
&lt;/p&gt;

&lt;h4&gt;The Proof&lt;/h4&gt;&lt;p&gt;
Now we can prove $\textbf{d}$'s transcendence using Lemma 1 and LWT:

$$\begin{align*}
e^{i\textbf{d}}&amp;amp;=\cos \textbf{d} + i \sin \textbf{d} \tag{Euler's formula}\\
&amp;amp;=\textbf{d}+i\sin \textbf{d} \tag{\(\textbf{d}\) is a fixed point}\\
&amp;amp;=\textbf{d}+i\sqrt{1-\textbf{d}^2} \tag{Lemma 1}
\end{align*}
$$

$$\boxed{e^{i\textbf{d}}=\textbf{d}+i\sqrt{1-\textbf{d}^2}}$$

Let us assume that $\textbf{d}$ is algebraic. If this is the case then:

$$\left(\textbf{d}+i\sqrt{1-\textbf{d}^2}\right) \in \mathbb{A}$$

This is because it consists solely of algebraic numbers $\left(\textbf{d},i,1\right)$ and basic algebraic operations $\left(+,-,\times,x^2,\sqrt{x}\right)$ and thus must be root of a polynomial with rational coefficients (i.e algebraic).&lt;p&gt;&lt;/p&gt;

However, also assuming $\textbf{d}$ is algebraic, LWT tells us:

$$e^{i\textbf{d}}\notin \mathbb{A}$$

Since the right side of the boxed equation is algebraic yet LWT guarantees that the left side is transcendental (because $i\textbf{d}$ is algebraic), we are left with a contradiction. Meaning our initial assumption, that $\textbf{d}$ is algebraic, was false. Via &lt;i&gt;reductio ad absurdum&lt;/i&gt; we can conclude:

$$\begin{align}
&amp;amp;e^{i\textbf{d}}=\textbf{d}+i\sqrt{1-\textbf{d}^2} \tag{Euler's formula}\\
&amp;amp;e^{i\textbf{d}}\notin \mathbb{A} \tag{LWT}\\
&amp;amp;\left(\textbf{d}+i\sqrt{1-\textbf{d}^2}\right) \in \mathbb{A} \tag{def. of algebraic number}\\
\therefore\ &amp;amp;\hline{\textbf{d}\notin \mathbb{A}} \tag{q.e.d}\\
\end{align}$$

&lt;/p&gt;&lt;/details&gt;

&lt;h3 id=&quot;kaplans-series&quot;&gt;Kaplan’s Series&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://www.maa.org/sites/default/files/Kaplan2007-131105.pdf&quot;&gt;Kaplan&lt;/a&gt; proved that $\textbf{d}$ is equivalent to the following series:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\textbf{d} = \sum_{n=0}^{\infty}\ g^{\left(n\right)}\left(\frac{\pi}{2}\right)\frac{\left(-\pi\right)^n}{2^nn!}&lt;/script&gt;

&lt;p&gt;Where the $g^{\left(n\right)}$ is the $n$th derivative of $f^{-1}$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;g^{\left(n\right)}\left(x\right)=\frac{d^n}{dx^n}f^{-1}\left(x\right)&lt;/script&gt;

&lt;p&gt;and $f^{-1}$ is the inverse of the function $f$ which is defined as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f\left(x\right)=x-\cos x&lt;/script&gt;

&lt;p&gt;&lt;em&gt;We define it in this roundabout way because there is no explicit definition of $f^{-1}\left(x\right)$. This makes the construction of Kaplan’s series all the more interesting.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Written out, the series looks something like this:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\textbf{d} = \frac{\pi}{4} - \frac{\pi^3}{768} - \frac{\pi^5}{61440} - \frac{43\pi^7}{165150720} - \cdots&lt;/script&gt;

&lt;p&gt;We can write this more succinctly (and as Kaplan originally proved) as so:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\textbf{d} = \sum_{n=0}^{\infty}a_n\pi^{2n+1}&lt;/script&gt;

&lt;p&gt;Where $a_n$ is a sequence of rational numbers found by solving the above equation.&lt;/p&gt;

&lt;!-- Proof of Kaplan's Series --&gt;
&lt;details&gt;
  &lt;summary&gt;&lt;h3 class=&quot;inline&quot;&gt;Proof of Kaplan's Series&lt;/h3&gt;&lt;/summary&gt;
  &lt;h4&gt;Taylor Series of $f^{-1}$&lt;/h4&gt;
  &lt;p&gt;
    To start off with, consider a function $f$ that is defined as such:

    $$f\left(x\right)=x-\cos x$$

    Kaplan was able to construct his series by noticing a few interesting properties of this function and it's inverse $f^{-1}$ (which has no explicit definition). The first of which was it's zero:

    $$\begin{align}
    f\left(\textbf{d}\right)&amp;amp;=\textbf{d}-\cos \textbf{d}\\
    &amp;amp;= \textbf{d}-\textbf{d}\\
    &amp;amp;= 0
    \end{align}$$

    This implies the following about $f^{-1}$ it's inverse:

    $$f^{-1}\left(0\right) = \textbf{d}$$

    And before we move on let's rename $f^{-1}$ to $g$ to make things less cluttered:

    $$f^{-1}\left(x\right) = g\left(x\right)$$

    We now have an expression for $\textbf{d}$. It is simply the value of $g\left(0\right)$. We currently do not have an explicit definition for $g\left(x\right)$ but we can create one via a Taylor series:

    $$g\left(x\right)=\sum_{n=0}^{\infty}g^{\left(n\right)}\left(c\right)\frac{\left(x-c\right)^n}{n!}$$

    - $g^{\left(n\right)}\left(x\right)$ is the $n$th derivative of $g\left(x\right)$
    - $c$ is the point we are constructing the Taylor series about. Since we are letting $n\to\infty$ the choice of constant won't affect the outcome.
  &lt;/p&gt;

  &lt;!-- Fixed Point --&gt;
  &lt;h4&gt;Fixed point of $f$ and $g$&lt;/h4&gt;
  &lt;p&gt;
    So now let us choose a value of $c$ that will be easy to compute. Notice that:

    $$\begin{align}
    f\left(\frac{\pi}{2}\right)&amp;amp;=\frac{\pi}{2}-\cos \frac{\pi}{2}\\
    &amp;amp;= \frac{\pi}{2}-0\\
    &amp;amp;= \frac{\pi}{2}
    \end{align}$$

    This means that $\frac{\pi}{2}$ is a fixed point of $f$ and that the following is also true of its inverse $g$:

    $$g\left(\frac{\pi}{2}\right)=\frac{\pi}{2}$$
  &lt;/p&gt;

  &lt;!-- nth Derivative of f --&gt;
  &lt;h4&gt;$n$th derivative of $f$&lt;/h4&gt;
  &lt;p&gt;
    Also notice that finding the $n$th derivative of $f$ at $\frac{\pi}{2}$ is simple:

    $$\begin{align}
    f\left(x\right)&amp;amp;=x-\cos x\\
    f'\left(x\right)&amp;amp;=1+\sin x\\
    f''\left(x\right)&amp;amp;=\cos x\\
    &amp;amp;\vdots\\
    \left(\forall n&amp;gt;1\right)\ f^{(n)}\left(x\right)&amp;amp;=\frac{d^{n-2}}{dx^{n-2}}\cos x
    \end{align}$$

    Because the derivatives of $\cos x$ are cyclical, we only need to evaluate the next 3 derivatives after $f''\left(\frac{\pi}{2}\right)$. Doing this we can see the pattern:

    $$f^{(n)}\left(\frac{\pi}{2}\right)=\{\frac{\pi}{2}, 2,0,-1,0,1,0,-1,\cdots\}$$
  &lt;/p&gt;

  &lt;!-- nth Derivative of g --&gt;
  &lt;h4&gt;$n$th derivative of $g$&lt;/h4&gt;
  &lt;p&gt;
    Now knowing the $n$th derivative of $f$ at $\frac{\pi}{2}$, we can calculate the $n$th derivative of $g$ at $\frac{\pi}{2}$:

    $$\begin{align}
    f\left(g\left(x\right)\right)=x \tag{inverse func.}\\
    f'\left(g\left(x\right)\right)g'\left(x\right)=1 \tag{chain rule}\\
    g'\left(x\right)=\frac{1}{f'\left(g\left(x\right)\right)}
    \end{align}$$

    We can use the chain and product rules repeatedly to find the $n$th derivative of $g$. The second derivative, for example, can be computed by differentiated both sides of the above equation:

    $$\begin{align}
    f'\left(g\left(x\right)\right)g'\left(x\right)=1\\
    f'(g(x))g''(x) + f''(g(x))g'(x)^2 = 0\\
    f'(g(x))g''(x) = - f''(g(x))g'(x)^2\\
    g''(x) = \frac{-f''(g(x))g'(x)^2}{f'(g(x))}
    \end{align}$$

    &lt;i&gt;Repeated use of the chain rule can be generalized via the &lt;a href=&quot;https://en.wikipedia.org/wiki/Faà_di_Bruno%27s_formula&quot;&gt;Faà di Bruno's formula.&lt;/a&gt;&lt;/i&gt;
  &lt;/p&gt;

  &lt;h4&gt;Solving the Taylor Series&lt;/h4&gt;
  &lt;p&gt;
    Using $\frac{\pi}{2}$ as our value of $c$, because $g^{(n)}(\frac{\pi}{2})$ is easy to compute, we can rewrite the Taylor series for $g$ as so:

    $$g\left(x\right)=\sum_{n=0}^{\infty}g^{\left(n\right)}\left(\frac{\pi}{2}\right)\frac{\left(x-\frac{\pi}{2}\right)^n}{n!}$$

    Since we are solving for $g(0)$ which equals $\textbf{d}$ we can plug it into the above series to arrive at:

    $$\textbf{d}=\sum_{n=0}^{\infty}g^{\left(n\right)}\left(\frac{\pi}{2}\right)\frac{\left(-\pi\right)^n}{2^nn!}$$

    Now we just have to solve for each of the terms in this sequence.

    &lt;details&gt;
      &lt;summary&gt;The zeroth term is equal to:&lt;/summary&gt;
      $$g\left(\frac{\pi}{2}\right)\frac{\left(-\pi\right)^0}{2^00!}=\frac{\pi}{2}$$
    &lt;/details&gt;

    &lt;details&gt;
      &lt;summary&gt;The first term is equal to:&lt;/summary&gt;
      $$g'\left(\frac{\pi}{2}\right)\frac{\left(-\pi\right)^1}{2^11!}=\frac{-\pi}{4}$$

      Because $g'\left(\frac{\pi}{2}\right)$ can be found by plugging $\frac{\pi}{2}$ into the equation we solved earlier:

      $$\begin{align}
      g'\left(\frac{\pi}{2}\right)&amp;amp;=\frac{1}{f'\left(g\left(\frac{\pi}{2}\right)\right)}\\
      &amp;amp;=\frac{1}{f'\left(\frac{\pi}{2}\right)}\\
      &amp;amp;=\frac{1}{2}\\
      \end{align}$$
    &lt;/details&gt;

    &lt;details&gt;
      &lt;summary&gt;The second term equals:&lt;/summary&gt;

      $$g''\left(\frac{\pi}{2}\right)\frac{\left(-\pi\right)^2}{2^22!}=0$$

      Because $g''\left(\frac{\pi}{2}\right)$ can be found as such:

      $$\begin{align}
      g''(x) &amp;amp;= \frac{-f''(g(x))g'(x)^2}{f'(g(x))}\\
      &amp;amp;=\frac{-f''(\frac{\pi}{2})g'(\frac{\pi}{2})^2}{f'(\frac{\pi}{2})}\\
      &amp;amp;=-\frac{0 (\frac{1}{2})}{2}\\
      &amp;amp;=0
      \end{align}$$
    &lt;/details&gt;

    Putting these terms together we find the following sequence:

    $$\textbf{d} = \frac{\pi}{2} - \frac{\pi}{4} + 0 - \frac{\pi^3}{768} + 0 - \frac{\pi^5}{61440} - \cdots$$

    One thing to note here are that all the even derivatives of $g(\frac{\pi}{2})$ are always $0$ meaning we can ignore all the even terms of the sequence.
    &lt;p&gt;&lt;/p&gt;
    Another thing to note is that we can simplify the first two terms in the series:

    $$\frac{\pi}{2}-\frac{\pi}{4}=\frac{\pi}{4}$$

    This allows us to rewrite the series as such:

    $$\textbf{d} = \frac{\pi}{4} - \frac{\pi^3}{768} - \frac{\pi^5}{61440} - \cdots$$

    This is what allows us (and Kaplan) to state the following:

    $$\textbf{d} = \sum_{n=0}^{\infty}a_n\pi^{2n+1}$$

    Where $a_n$ is a sequence of rational numbers.
    &lt;p&gt;&lt;/p&gt;

    &lt;i&gt;As a side note, to prove this we assumed that $g$ was infinitely differentiable which is required to created a Taylor series for it. This is in fact true, but it just wasn't proved above.&lt;/i&gt;
  &lt;/p&gt;
&lt;/details&gt;

&lt;h3 id=&quot;approximating-the-dottie-number&quot;&gt;Approximating the Dottie Number&lt;/h3&gt;
&lt;h4 id=&quot;solve-kaplans-series&quot;&gt;Solve Kaplan’s series&lt;/h4&gt;
&lt;p&gt;One way is to simply calculate a specified number of terms in Kaplan’s series and sum them.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\textbf{d} \approx \frac{\pi}{4} &amp;= 0.7\color{red}{854\cdots}\\
\textbf{d} \approx \frac{\pi}{4} - \frac{\pi^3}{768} &amp;= 0.7\color{red}{450\cdots}\\
\textbf{d} \approx \frac{\pi}{4} - \frac{\pi^3}{768} - \frac{\pi^5}{61440} &amp;= 0.7\color{red}{400\cdots}\\
\textbf{d} \approx \frac{\pi}{4} - \frac{\pi^3}{768} - \frac{\pi^5}{61440} - \frac{43\pi^7}{165150720} &amp;= 0.739\color{red}{2\cdots}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;While this can provide a decent approximation, it is a very time consuming process and doesn’t allow the approximator to skip ahead to a desired accuracy (i.e to within .01%). Moreover while this series converges to $\textbf{d}$, it does so relatively slowly. To get just 17 decimal places of accuracy, one would need to solve for 25 terms of the series:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\textbf{d}\approx 0.73908 51332 15160 64\color{red}{570 711495 ...}&lt;/script&gt;

&lt;h4 id=&quot;taylor-series-of-cosine&quot;&gt;Taylor Series of Cosine&lt;/h4&gt;
&lt;p&gt;Another way to approximate $\textbf{d}$ is to simply substitute Taylor polynomials of $\cos x$ for $\cos x = x$ and solve for the zero of the resulting polynomial:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\cos x = 1 - \frac{x^2}{2!} + \frac{x^4}{4!}- \frac{x^6}{6!}+\cdots&lt;/script&gt;

&lt;p&gt;For the &lt;strong&gt;second&lt;/strong&gt; degree Taylor polynomial:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
1-\frac{x^2}{2!}=x\\
-\frac{x^2}{2}-x+1=0
\end{align}&lt;/script&gt;

&lt;p&gt;Using the quadratic formula we find:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x = \sqrt{3}-1 = 0.73\color{red}{205\cdots}&lt;/script&gt;

&lt;p&gt;For the &lt;strong&gt;fourth&lt;/strong&gt; degree Taylor polynomial:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
1-\frac{x^2}{2!}+\frac{x^4}{4!}=x \\
\frac{x^4}{24}-\frac{x^2}{2}-x+1=0
\end{align}&lt;/script&gt;

&lt;p&gt;Using the &lt;a href=&quot;https://en.wikipedia.org/wiki/Quartic_function#General_formula_for_roots&quot;&gt;quartic formula&lt;/a&gt; we find:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x = 0.73\color{red}{557\cdots}&lt;/script&gt;

&lt;p&gt;However, after the 3rd term, we run into a problem. &lt;a href=&quot;https://en.wikipedia.org/wiki/Abel–Ruffini_theorem&quot;&gt;Abel’s impossibility theorem&lt;/a&gt; states that there is no generic solution to polynomial equations above degree 4. For these polynomials a root finding algorithm has to be applied to approximate the zeros of the function. But if we have to use an approximation (root finder) just to calculate our approximation (Taylor polynomial) we might as well use the root finder on the original function: $\cos x - x = 0$.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;You may have noticed I didn’t include an exact representation of the quartic equation above. This is because, even though there exists a generic solution to quartic polynomials, it is crazily complex and not worth using practically. Click the &lt;a href=&quot;https://upload.wikimedia.org/wikipedia/commons/9/95/Quartic_Formula.jpg&quot;&gt;quartic&lt;/a&gt; link to see the equation in its entirety&lt;/em&gt;&lt;/p&gt;

&lt;h4 id=&quot;general-root-finding-algorithm&quot;&gt;General Root Finding Algorithm&lt;/h4&gt;
&lt;p&gt;The only choice we have left is to use a general root, or zero, finding algorithm like &lt;a href=&quot;https://en.wikipedia.org/wiki/Newton%27s_method&quot;&gt;Newton’s method&lt;/a&gt; or the &lt;a href=&quot;https://en.wikipedia.org/wiki/Bisection_method&quot;&gt;bisection method&lt;/a&gt; to calculate $\textbf{d}$ to a given accuracy.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>On the Grounding of Ideas</title>
   <link href="http://localhost:4000/metaphysical-nightmare/"/>
   <updated>2018-04-09T00:00:00-04:00</updated>
   <id>http://localhost:4000/metaphysical-nightmare</id>
   <content type="html">&lt;h2 id=&quot;poorly-defined-concepts&quot;&gt;Poorly Defined Concepts&lt;/h2&gt;
&lt;p&gt;What is jealousy? What is a cat? What does it mean to be free? What &lt;em&gt;are&lt;/em&gt; you?&lt;/p&gt;

&lt;p&gt;These aren’t foreign questions to us. Jealousy is an emotion we feel when we envy someone for what they have, a cat is an animal with a tail and four paws, freedom is the state of being able to do what you please, and I am a human being with thoughts and emotions.&lt;/p&gt;

&lt;p&gt;But what is jealousy &lt;em&gt;really&lt;/em&gt;? What’s a cat &lt;em&gt;really&lt;/em&gt;? What does it &lt;em&gt;really&lt;/em&gt; mean to be human?&lt;/p&gt;

&lt;p&gt;There is no formal definition of these concepts. We just know them when we see them. I know what jealousy feels like, I know what being human feels like (I hope), and I know a cat when I see it.&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;“Alright,” you might say, “maybe humanity and jealousy are too lofty to state objectively, but can’t cats be formally defined? Surely a cat has a particular genome that defines it regardless of its outward appearance, right?”&lt;/p&gt;

&lt;p&gt;But in a world where cats evolved from another species via gradual mutations to individual base pairs in their DNA (not big leaps), where do we draw the line between cat and cave-cat, so to speak.&lt;/p&gt;

&lt;p&gt;Moreover is a picture of a cat a cat? It certainly represents the notion of a cat, but it doesn’t exactly have genetic material. Plus, there is no formal way to tell whether a given picture is of a cat or not. No formula, no physical law of cats. We can tell a cat, real or drawn, from another animal using our &lt;em&gt;intuition&lt;/em&gt;, whatever that is.&lt;/p&gt;

&lt;p&gt;“But it’s easy” you might say, “just look for a tail, 4 legs, pointy ears and some fur.” What about hairless cats? a cat drawn with no tail? a cat who’s ears are down, etc. There are so many exceptions that it’s a miracle we are even able to tell cat from dog!&lt;/p&gt;

&lt;h4 id=&quot;whats-wrong-with-being-poorly-defined&quot;&gt;What’s Wrong with being Poorly Defined?&lt;/h4&gt;
&lt;p&gt;Why is this a problem? Well, without a concrete definition, the statements we can make about a concept are limited and just as hazy as the definition.&lt;/p&gt;

&lt;p&gt;Take morality. Unlike a well defined concept, you can’t point to some mathematical construction or set of rules that it follows. I can’t tell you without a doubt whether a given action was ‘moral’ or not, nor can I even define morality.&lt;/p&gt;

&lt;p&gt;Killing is certainly immoral… right? Well, except when it’s in the defense of yourself or others. What about hitting a kid as punishment? We certainly wouldn’t find that acceptable today, but it was commonplace not too long ago. How can there be an objective definition of morality if it is riddled with exceptions and even evolves over time?&lt;/p&gt;

&lt;p&gt;A chemist can give you certainty that a particular solution will erode a metal because chemistry is a formal science with mathematical definitions of all its concepts. A psychologist, however, may only have a hazy idea of why you may feel a certain emotion (whatever an emotion is) and what could have caused it.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;That said, it’s not too far-fetched to imagine an operational definition of emotions based on the amount of certain neurotransmitters in one’s brain. But of course, this is a bit of an oversimplification.&lt;/em&gt;&lt;/p&gt;

&lt;h4 id=&quot;computation&quot;&gt;Computation&lt;/h4&gt;
&lt;p&gt;Computers are a prime example of this, as they can only deal with problems that are well defined (i.e programmable via a set of steps or &lt;strong&gt;algorithm&lt;/strong&gt;). Ask it to solve the quadratic function, model an atom, or even play music and it’ll do just fine. But ask it to judge how strong an argumentative essay is and it might run into trouble. Digital music is just a sound wave sampled at a high rate, easy. But what is a good essay, formally speaking?&lt;/p&gt;

&lt;p&gt;&lt;em&gt;This problem of doing shakily defined tasks on computers can be partially dealt with via &lt;strong&gt;machine learning&lt;/strong&gt;. And the underlying philosophy behind it’s effectiveness in the real world is called the &lt;a href=&quot;http://www.deeplearningbook.org/version-2015-10-03/contents/manifolds.html&quot;&gt;manifold hypothesis&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;well-defined-concepts&quot;&gt;Well Defined Concepts&lt;/h2&gt;
&lt;p&gt;To formally deal with any idea or concept, it has to be well defined. By making our definitions more concrete we can make more precise statements about these ideas. This is how math and science are born.&lt;/p&gt;

&lt;p&gt;Take the atom. An atom is a collection of protons, neutrons and electrons which are further composed of elementary particles, all of which are governed by an exact set of equations (i.e quantum mechanics).&lt;/p&gt;

&lt;p&gt;A Cesium atom, for example, is thus completely mathematically defined. Everything about it is encapsulated in it’s wave function, no need to look at the natural world to understand it.&lt;/p&gt;

&lt;p&gt;This applies to all atoms, and thus all molecules, and further anything in the universe made of matter, like humans. This shouldn’t come as a surprise. After all, we know that math is the bedrock of science and thus of formal knowledge as a whole.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Of course, modeling even molecules (much less humans) as quantum wave functions is pretty much out of our reach computationally. As such the claim that quantum wave functions can model all aspects of our lives is solely a theoretical one.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/philosophy/metaphysical-nightmare/chain_of_abstraction.png?style=centerme&quot; alt=&quot;chain of abstraction&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Take psychology. It is really just applied biology, which is just applied chemistry, which is just applied physics, which is just (as we’ve seen) applied mathematics.&lt;/p&gt;

&lt;p&gt;This &lt;em&gt;chain of abstraction&lt;/em&gt; applies to all fields of human knowledge, but its relation to psychology and the human mind will be particularly useful to us…&lt;/p&gt;

&lt;h2 id=&quot;bridging-the-gap&quot;&gt;Bridging the Gap&lt;/h2&gt;
&lt;p&gt;Now consider the concept of love. It seems impossible to define it objectively right? Sure, it’s an evolutionary aid to sexual reproduction, but that’s far from concrete and certainly ignores our own experience of it.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;We must first ask ourselves “what is love?”
    &lt;ul&gt;
      &lt;li&gt;Well it’s an emotion, right?&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Sure, but what’s an emotion?
    &lt;ul&gt;
      &lt;li&gt;A state of mind.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;But what is &lt;em&gt;“the mind”&lt;/em&gt;?
    &lt;ul&gt;
      &lt;li&gt;The mind arises from the brain processing the information blasted into it via our senses.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;What is the brain made up of, and what comprises its state?
    &lt;ul&gt;
      &lt;li&gt;The brain is comprised of trillions of specialized cells called neurons. These neurons connect to form large &lt;em&gt;neural networks&lt;/em&gt; and communicate via electrical activity and chemicals known as neurotransmitters.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;And further neurons are comprised of different organelle and so on until we reach atoms and ultimately the fundamental particles of the universe.&lt;/p&gt;

&lt;p&gt;This relation between the most fundamental (and very well defined) concepts in our universe, like elementary particles, to the most hazy and abstract ones, like emotions, is what will allow us the bridge the between poorly defined and well defined in a somewhat, admittedly, crude way.&lt;/p&gt;

&lt;h4 id=&quot;brain-graphs&quot;&gt;Brain Graphs&lt;/h4&gt;
&lt;p&gt;Consider the human brain. It is simply a collection of interconnected neurons, no? We can represent that neural network as graph. This graph will contain a bunch (billions) of nodes, each representing a neuron, and even more (trillions) of vertices between those nodes, representing connections.&lt;/p&gt;

&lt;p&gt;Making our definition a bit more concrete, we can define any brain, in a single instant, as a graph of it’s neurons:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;B=\left(N,C\right)&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;$B$ is a graph of a brain.&lt;/li&gt;
  &lt;li&gt;$N$ is the set of all nodes in $B$ (neurons).&lt;/li&gt;
  &lt;li&gt;$C$ is the set of all connections between members of $N$ (neurons).&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;functions-of-time&quot;&gt;Functions of Time&lt;/h4&gt;
&lt;p&gt;Of course, a thought doesn’t occur in an instant, it happens over a small interval of time. So these brain graphs would actually be brain graph functions over time. Plugging in a time into the function would output the state of the brain graph at that moment in time.&lt;/p&gt;

&lt;p&gt;We can formalize this via:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;B(t)=\left(N,C(t)\right)&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;$B(t)$ is a function of the graph of a brain over time.&lt;/li&gt;
  &lt;li&gt;$N$ is the same as before (assuming the neurons in the brain stay constant during this thought)&lt;/li&gt;
  &lt;li&gt;$C$ is the a function of the set of all connections between members of $N$ (neurons) over time.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;set-of-all-of-brain-graph-functions&quot;&gt;Set of all of Brain Graph Functions&lt;/h4&gt;
&lt;p&gt;Finally we can define an idea or concept as the set $S$ of all the possible brain graph functions that (we would consider) are thinking of that idea:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;S=\left\{B \in T\mid B \text{ is a brain that is thinking of S}\right\}&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;$S$ is a particular idea (represented by a set).&lt;/li&gt;
  &lt;li&gt;$B$ is a brain graph function. (The $(t)$ is omitted because we are just talking about the function itself).&lt;/li&gt;
  &lt;li&gt;$T$ is the set of all possible brain graph functions.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Notice that in this definition we are quantifying over all &lt;em&gt;possible&lt;/em&gt; brain graphs not just the ones that have existed or will exist. Thus, for example a cat:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\text{Cat}&amp;=\left\{B \in T\mid B \text{ is a brain that is thinking of cat}\right\}\\
\text{Cat}&amp;=\left\{B_1,B_2,B_3,B_4,\cdots\right\}
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;In the example above $B_1, B_2, B_3$ and so on are all different possible human brains that are thinking about cats.&lt;/p&gt;

&lt;h2 id=&quot;how-does-this-definition-even-help&quot;&gt;How does this Definition even Help?&lt;/h2&gt;
&lt;p&gt;Remember when I said formally defining concepts allows us to make formal statements about them? Well that was only partially true. While this ‘set of all particle configuration’ definition is more formal than your standard notion of jealousy, it’s not exactly helpful in trying to understand the nature of emotion of people’s interactions with each other.&lt;/p&gt;

&lt;p&gt;So then what’s the real point? The real point is purely a semantic one. A scientist/philosopher might like to associate some physical meaning to ideas like emotion or creativity. Thus, we can be rest assured that everything imaginable can be considered physical in some sense. Even if that physicality is purely expressed as your brain thinking about said concept, it’s still just particles following the same physical laws.&lt;/p&gt;

&lt;h4 id=&quot;an-aside&quot;&gt;An Aside&lt;/h4&gt;
&lt;p&gt;The idea I presented above is, of course, ridiculous and riddled with problems. It is just meant to serve as a rough idea of how one might formalize the notion of an “idea” or “concept” when , at first, it may seem that it only exists in our minds. Indeed these concepts do only exist in our minds, but our minds are purely physical (unless you’re a &lt;a href=&quot;https://plato.stanford.edu/entries/dualism/&quot;&gt;duelist&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;One such problem I would like to somewhat address is the apparent circularity of saying “the notion of cat is encompassed by the set of all brains that are thinking of the concept of cat.” This isn’t as bad as it sounds because it should really be phrased as “the notion of cat is encompassed by the set of all brains that are thinking of &lt;strong&gt;what we consider&lt;/strong&gt; the concept of cat.” This definition draws on our own subjective ideas of cats to objectively define them. (In this case &lt;em&gt;our&lt;/em&gt; might represent an average of all human opinions or something of the sort.)&lt;/p&gt;

&lt;p&gt;Another, more obvious, problem is that a certain concept or idea that occurs in a human mind may not be isolated in their brains. The human mind is a complex mechanism whose function is a product of one’s body, senses, and general environment. This problem can be solved by simply replacing the idea of brain graph with the set of cells that represent the human body or even set of particles/wavefunction that comprise that human body if necessary. The brain graph formulation I used above is simply for convenience.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Drag Force</title>
   <link href="http://localhost:4000/drag-force/"/>
   <updated>2018-03-27T00:00:00-04:00</updated>
   <id>http://localhost:4000/drag-force</id>
   <content type="html">&lt;h3 id=&quot;general-info&quot;&gt;General Info&lt;/h3&gt;
&lt;p&gt;The drag force, or fluid friction, is a force that opposes the motion of an object through a fluid/gas, similar to dry &lt;a href=&quot;\friction&quot;&gt;friction&lt;/a&gt;. It can also be used to describe the resistance between a fluid in another fluid.&lt;/p&gt;

&lt;p&gt;When the fluid is air, we refer to the force as aerodynamic drag or “air resistance.” When it’s water we call it hydrodynamic drag.&lt;/p&gt;

&lt;h4 id=&quot;non-conservative-force&quot;&gt;Non-Conservative Force&lt;/h4&gt;
&lt;p&gt;Like friction, the drag force is a non-conservative force meaning the work done by it is path dependent. And, also like friction, drag converts some energy into heat, thus it does not conserve mechanical energy.
$\renewcommand{\vec}[1]{\mathbf{#1}}$&lt;/p&gt;

&lt;h3 id=&quot;drag-equation&quot;&gt;Drag Equation&lt;/h3&gt;
&lt;p&gt;The magnitude of the drag force is given by:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;F_D=\frac{\rho C_D A v^2}{2}&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;$F_D$ is the magnitude of the drag force.&lt;/li&gt;
  &lt;li&gt;$\rho$ is density of the fluid.&lt;/li&gt;
  &lt;li&gt;$C_D$ is the drag coefficient of the fluid.&lt;/li&gt;
  &lt;li&gt;$A$ is the cross-sectional area of the object (or more accurately the area of a projection of the object onto a plane perpendicular to the velocity).&lt;/li&gt;
  &lt;li&gt;$v$ is the magnitude of the velocity.&lt;/li&gt;
&lt;/ul&gt;

&lt;!--more--&gt;

&lt;p&gt;The direction of drag is opposite to that of the object’s velocity:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{\vec{F_D}}=-\hat{\vec{v}}&lt;/script&gt;

&lt;p&gt;Combining these two equations, we can write the drag force as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\vec{F_D}=\frac{-\rho C_D A v^2}{2}\hat{\vec{v}}=\frac{-\rho C_D A\|\vec{v}\|}{2}\vec{v}&lt;/script&gt;

&lt;h4 id=&quot;power-to-overcome-drag&quot;&gt;Power to Overcome Drag&lt;/h4&gt;
&lt;p&gt;We can find the power required to overcome the drag force by the dot product of drag and the velocity of the object. And since the drag force is always oriented the same way as velocity, we can say:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P_d=\vec{F_D}\cdot\vec{v}=\frac{\rho C_DAv^3}{2}&lt;/script&gt;

&lt;h4 id=&quot;terminal-velocity&quot;&gt;Terminal Velocity&lt;/h4&gt;
&lt;p&gt;When an object is in free-fall in a liquid, gravity is counteracted by drag until it reaches an equilibrium. At this equilibrium the net force on the object, $\vec{F_{net}}$, equals zero and the object’s velocity stays constant:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/physics/terminal_velocity.png?style=centerme&quot; alt=&quot;terminalvel&quot; /&gt;&lt;/p&gt;

&lt;p&gt;By solving for the velocity when the sum of the forces equals zero, we arrive at the terminal velocity $v_t$:&lt;/p&gt;

&lt;details&gt;&lt;summary&gt;Derivation&lt;/summary&gt;&lt;p&gt;$$\begin{align*}
F_D-F_g=0 \tag{drag and gravity cancel out}\\
F_D=F_g\\
\frac{\rho C_DAv^2}{2}=mg\\
v^2=\frac{2mg}{\rho C_DAv^2}
\end{align*}$$&lt;/p&gt;&lt;/details&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\boxed{v_t=\sqrt{\frac{2mg}{\rho C_dA}}}&lt;/script&gt;

&lt;p&gt;&lt;em&gt;This terminal velocity is more aptly described as a terminal speed, since it is just a magnitude.&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;drag-coefficient&quot;&gt;Drag Coefficient&lt;/h3&gt;
&lt;p&gt;The drag coefficient is a proportionality constant used in the drag equation and is measured empirically. Objects with more blunt faces have higher drag coefficients and objects with more streamlined designs have lower ones. Here is a diagram of different drag coefficients in a particular fluid:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/physics/drag_coefficients.png?style=centerme&quot; alt=&quot;dragCOF&quot; width=&quot;400px&quot; /&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Friction</title>
   <link href="http://localhost:4000/friction/"/>
   <updated>2018-03-26T00:00:00-04:00</updated>
   <id>http://localhost:4000/friction</id>
   <content type="html">&lt;h3 id=&quot;general-info&quot;&gt;General Info&lt;/h3&gt;
&lt;!-- &amp; Table of Contents --&gt;
&lt;p&gt;Dry Friction is a force that opposes the motion of two surfaces that slide against each other. This force is proportional to the normal force of the object.&lt;/p&gt;

&lt;!-- - [Amontons' Laws of Dry Friction](#amontons-laws-of-dry-friction)
- [From Static to Kinetic Friction](#from-static-to-kinetic-friction)
- [Static Friction](#static-friction)
- [Kinetic Friction](#kinetic-friction)
- [Coefficient of Friction](#coefficient-of-friction) --&gt;

&lt;h4 id=&quot;where-does-friction-come-from&quot;&gt;Where does Friction come from?&lt;/h4&gt;
&lt;p&gt;The frictional force is not fundamental and is instead a useful abstraction of the complex electromagnetic interactions between two surfaces that come into contact with each other. The forces created by these interactions can, in a variety of cases, be thought of as a singular force that is proportional to the normal force.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/physics/friction_between_surfaces.jpg?style=centerme&quot; alt=&quot;complexfriction&quot; width=&quot;450px&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;intuition-and-newtons-first-law&quot;&gt;Intuition and Newton’s First Law&lt;/h4&gt;
&lt;p&gt;The frictional force is responsible for filling the gap between our everyday intuition of motion (that it eventually stops) and Newton’s first law (that an object in motion remains in motion, unless acted upon by an outside force). The ‘outside force’ that retards motion in the case many of everyday objects is indeed friction.
$\renewcommand{\vec}[1]{\mathbf{#1}}$
&lt;!--more--&gt;&lt;/p&gt;

&lt;h3 id=&quot;amontons-laws-of-dry-friction&quot;&gt;Amontons’ Laws of Dry Friction&lt;/h3&gt;
&lt;p&gt;The empirical laws that govern friction were (re)discovered by French physicist Guillaume Amontons:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Amontons’ First Law&lt;/strong&gt;: The force of friction is directly proportional to the applied load.
    &lt;ul&gt;
      &lt;li&gt;The more an object is pushed into a surface (normal force) the more it interacts with that surface (friction).&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Amontons’ Second Law&lt;/strong&gt;: The force of friction is independent of the apparent area of contact.
    &lt;ul&gt;
      &lt;li&gt;While more surface area means more interactions with the surface (friction), having more area means the applied load is more distributed, offsetting the increase in area.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Amontons’ Third Law (&lt;strong&gt;Coulomb’s Law of Friction&lt;/strong&gt;): Kinetic friction is independent of the sliding velocity.
    &lt;ul&gt;
      &lt;li&gt;Kinetic friction takes over after static friction has been overcome and object starts to move, but once its in motion friction is constant.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;from-static-to-kinetic-friction&quot;&gt;From Static to Kinetic Friction&lt;/h3&gt;
&lt;p&gt;There are two types of friction: static friction which acts on an object at rest, and kinetic friction which acts on an object in motion. When the force of static friction reaches a maximum, kinetic friction takes over:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://3.bp.blogspot.com/-5pk-uJYub_Y/VFe1UdylnYI/AAAAAAAAFoI/8NyK6iU-GFo/s1600/graph-static-kinetic-friction.png?style=centerme&quot; alt=&quot;graph&quot; width=&quot;500px&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;static-friction&quot;&gt;Static Friction&lt;/h3&gt;
&lt;p&gt;An object at rest on a surface will resist motion on the surface until a certain maximum force has been reached. The magnitude of that max force is given by:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;F_{s,max}=\mu_sN&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;$F_{s,max}$ is the magnitude of the maximum force of static friction.&lt;/li&gt;
  &lt;li&gt;$\mu_s$ is the coefficient of static friction.&lt;/li&gt;
  &lt;li&gt;$N$ is the magnitude of the normal force on the object (the component of the total force on the object that is normal to the plane it’s resting on).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;At any other point before that max force has been reached, the force of static friction is equal in magnitude and opposite in direction to the component of the total force on the object that is parallel to the plane it’s resting on, effectively canceling out any acceleration across the surface:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\vec{F_s}=-\vec{F_{\parallel}}&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;$\vec{F_s}$ is the force of static friction.&lt;/li&gt;
  &lt;li&gt;$\vec{F_\parallel}$ is the total force on the object parallel to the surface.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This parallel force can be described in terms of the normal force which is orthogonal to the surface:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\vec{F_{\parallel}}=\vec{F_{net}}-\vec{N}&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;$\vec{F_{net}}$ is the total force applied to the object.&lt;/li&gt;
  &lt;li&gt;$\vec{N}$ is the normal force on the object.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;work-done-by-static-friction&quot;&gt;Work done by Static Friction&lt;/h4&gt;
&lt;p&gt;Notice that because static friction cancels out all motion across a particular surface, the force does not do any work (at least from the reference frame of the two surfaces).&lt;/p&gt;

&lt;h3 id=&quot;kinetic-friction&quot;&gt;Kinetic Friction&lt;/h3&gt;
&lt;p&gt;An object in motion on a surface will experience kinetic friction that is proportional to the normal force on the object and independent of its velocity. The magnitude of this force is given by:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;F_{k}=\mu_kN&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;$F_{k}$ is the magnitude of the force of static friction.&lt;/li&gt;
  &lt;li&gt;$\mu_k$ is the coefficient of kinetic friction.&lt;/li&gt;
  &lt;li&gt;$N$ is the magnitude of the normal force on the object.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Kinetic friction is pointed in the opposite direction of the object’s velocity:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{\vec{F_k}}=-\hat{\vec{v}}&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;$\hat{\vec{F_k}}$ is the unit vector of the object’s velocity.&lt;/li&gt;
  &lt;li&gt;$\hat{\vec{v}}$ is the unit vector of the object’s velocity.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Putting these together we find that the force of kinetic friction is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\vec{F_{k}}=-\mu_kN\hat{\vec{v}}=\frac{-\mu_kN}{\|\vec{v}\|}\vec{v}&lt;/script&gt;

&lt;p&gt;&lt;em&gt;Note that this does not violate Coulomb’s Law of Friction because it refers only to the magnitude of kinetic friction.&lt;/em&gt;&lt;/p&gt;

&lt;h4 id=&quot;non-conservative-force-and-thermal-energy&quot;&gt;Non-Conservative Force and Thermal Energy&lt;/h4&gt;
&lt;p&gt;Friction is non-conservative force, meaning that the work done by kinetic friction is dependent on the path taken. Friction also causes heat energy to be released into the system, meaning it does not conserve mechanical energy.&lt;/p&gt;

&lt;p&gt;To calculate the thermal energy created by kinetic friction, a line integral through the path taken must be used:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;E_{th}=\int_C\vec{F_k}(\vec{x})\cdot d\vec{x}&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;$\vec{F_k}(\vec{x})$ is a vector field of the force of kinetic friction.&lt;/li&gt;
  &lt;li&gt;$\vec{x}$ is the position of the object.&lt;/li&gt;
  &lt;li&gt;$C$ is the path the object took.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;coefficient-of-friction&quot;&gt;Coefficient of Friction&lt;/h3&gt;
&lt;p&gt;The proportionality constant between the frictional force and the normal force is called the coefficient of static/kinetic friction and is denoted $\mu_s$ and $\mu_k$ respectively. This constant differs depending on the 2 surfaces involved and, as a result of being a abstract approximation of complex interactions, cannot be derived from first principles and is instead measured empirically.&lt;/p&gt;

&lt;p&gt;Below are some common coefficients of static and kinetic friction:
&lt;img src=&quot;http://hadron.physics.fsu.edu/~crede/TEACHING/PHY2048C/Calendar/W6_D1/Friction%20Coefficients_files/friction-coeffs.gif&quot; alt=&quot;table&quot; width=&quot;600px&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;angle-of-repose&quot;&gt;Angle of Repose&lt;/h4&gt;
&lt;p&gt;When an object is placed on a ramp, the object will overcome static friction and slide down the ramp at some angle $\theta$. As it turns out, the tangent of this angle is equivalent to the coefficient of static friction:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/8/85/Free_body.svg?style=centerme&quot; alt=&quot;angle of repose&quot; /&gt;&lt;/p&gt;

&lt;details&gt;&lt;summary&gt;Derivation&lt;/summary&gt;&lt;p&gt;

$$\begin{align*}
N=mg\cos\theta \tag{force normal to the ramp}\\
\mu_sN=mg\sin\theta \tag{$F_k$ at the moment of slipping}\\
\mu_smg\cos\theta=mg\sin\theta\\
\mu_s=\frac{\sin\theta}{\cos\theta}=\tan\theta
\end{align*}$$&lt;/p&gt;&lt;/details&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\boxed{\tan\theta=\mu_s}&lt;/script&gt;
</content>
 </entry>
 
 <entry>
   <title>Classical Physics</title>
   <link href="http://localhost:4000/classical-physics/"/>
   <updated>2018-03-24T00:00:00-04:00</updated>
   <id>http://localhost:4000/classical-physics</id>
   <content type="html">&lt;p&gt;Classical physics refers to the branches of physics that were conceived of prior to quantum mechanics and Einsteinian relativity. These branches of physics are more than sufficient for non-relativistic speeds and non-quantum scales. Moreover, they serve as starting points for understanding and deriving modern physics.&lt;/p&gt;

&lt;h2 id=&quot;classical-mechanics&quot;&gt;Classical Mechanics&lt;/h2&gt;
&lt;p&gt;Classical mechanics is the study of the motion of massive bodies. It is usually split into two main branches:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;/kinematics&quot;&gt;Kinematics&lt;/a&gt; - The description of translational motion through space. Other types of motion include:
    &lt;ul&gt;
      &lt;li&gt;Rotational Kinematics&lt;/li&gt;
      &lt;li&gt;Simple Harmonic Motion (Oscillatory Motion)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Dynamics - The causes of motion. Main topics include:
    &lt;ul&gt;
      &lt;li&gt;Forces&lt;/li&gt;
      &lt;li&gt;Work, Power &amp;amp; Energy&lt;/li&gt;
      &lt;li&gt;Linear Momentum&lt;/li&gt;
      &lt;li&gt;Rotational Dynamics&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;!--more--&gt;

&lt;h4 id=&quot;reformulations&quot;&gt;Reformulations&lt;/h4&gt;
&lt;p&gt;While Newtonian mechanics, as described by the &lt;strong&gt;&lt;em&gt;principia&lt;/em&gt;&lt;/strong&gt;, was the first formulation of classical mechanics, there exists others. Namely Lagrangian Mechanics and Hamiltonian mechanics. These formulations are instrumental in developing modern theories like quantum mechanics.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://en.citizendium.org/images/thumb/f/f5/Classical_mechanics_timeline.PNG/800px-Classical_mechanics_timeline.PNG?style=centerme&quot; alt=&quot;formulations&quot; width=&quot;500px&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;classical-electromagnetism&quot;&gt;Classical Electromagnetism&lt;/h2&gt;
&lt;p&gt;Classical electromagnetism studies the properties and interactions of electric charges, magnets, and the electromagnetic force in general. There are 4 main topics here:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Electrostatics
    &lt;ul&gt;
      &lt;li&gt;Electric Charge&lt;/li&gt;
      &lt;li&gt;Electric Fields&lt;/li&gt;
      &lt;li&gt;Electric Potential&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Magnetostatics&lt;/li&gt;
  &lt;li&gt;Electrodynamics&lt;/li&gt;
  &lt;li&gt;Electrical Networks
    &lt;ul&gt;
      &lt;li&gt;Capacitance&lt;/li&gt;
      &lt;li&gt;Current&lt;/li&gt;
      &lt;li&gt;Resistance&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;thermodynamics&quot;&gt;Thermodynamics&lt;/h2&gt;
&lt;p&gt;Thermodynamics is the study of the workings d effects of heat, or &lt;strong&gt;thermal energy&lt;/strong&gt;, in a system. Some main topics include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The 4 Laws of Thermodynamics&lt;/li&gt;
  &lt;li&gt;Entropy&lt;/li&gt;
  &lt;li&gt;Temperature&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Playing Atari Pong with Reinforcement Learning</title>
   <link href="http://localhost:4000/reinforcement-learning-pong/"/>
   <updated>2018-03-18T00:00:00-04:00</updated>
   <id>http://localhost:4000/reinforcement-learning-pong</id>
   <content type="html">&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;
&lt;p&gt;In 2013 the relatively new AI startup DeepMind released their paper &lt;a href=&quot;https://arxiv.org/pdf/1312.5602.pdf&quot;&gt;&lt;em&gt;Playing Atari with Deep Reinforcement Learning&lt;/em&gt;&lt;/a&gt; detailing an artificial neural network that was able to play, not 1, but 7 Atari games with human and even super-human level proficiency. What made this paper so astounding was the fact that it was a single, general purpose neural network (a &lt;strong&gt;general artificial intelligence&lt;/strong&gt; if you will) that could be trained to play all these games rather than 7 separate ones.&lt;/p&gt;

&lt;p&gt;If this wasn’t enough, in 2015 they blew the machine learning community, and everyone else considering the news coverage, away with their paper &lt;a href=&quot;https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf&quot;&gt;&lt;em&gt;Human-level control through deep reinforcement learning&lt;/em&gt;&lt;/a&gt; in which they construct what they call a &lt;strong&gt;Deep Q Network&lt;/strong&gt; (DQN) to play &lt;em&gt;42&lt;/em&gt; different Atari games, all of varying complexity, with performance that exceeded a professional human player.&lt;a href=&quot;https://deepmind.com/research/dqn/&quot;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;

&lt;!--more--&gt;

&lt;h2 id=&quot;q-learning&quot;&gt;Q-Learning&lt;/h2&gt;
&lt;p&gt;The researchers at Google’s DeepMind achieved this stunning success with a type of machine learning called &lt;strong&gt;reinforcement learning&lt;/strong&gt; and more specifically &lt;strong&gt;Q-learning&lt;/strong&gt;. In essence, the goal of Q-learning is to approximate some ideal function $Q(s,a)$ that outputs a reward (how good we are doing at the task), where $s$ is a possible state of the environment/game/etc. and $a$ is a possible action to take in that state. If we had such a function, or even a good approximation, we could simply plug in our current state and choose whatever action will maximize $Q$ which would then maximize how well we perform the task. To approximate this function, the researches used a convolutional neural network (CNN) and trained it using Q-learning, thus creating a Deep Q Network. You can read more about Q-learning and DQNs &lt;a href=&quot;https://ai.intel.com/demystifying-deep-reinforcement-learning/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;By implementing Q-learning in a convolutional neural network (CNN) they create a DQN capable of predicting what actions to take based on the current state of the game.&lt;/p&gt;

&lt;h2 id=&quot;policy-gradients&quot;&gt;Policy Gradients&lt;/h2&gt;
&lt;p&gt;That said, Q-learning isn’t the only way to achieve these results. Another popular type of reinforcement learning is what known as &lt;strong&gt;policy gradients&lt;/strong&gt;. This method is more direct and conceptually simpler than Q-learning. Essentially, you input the current state, action taken, and reward given at every step and optimize the network accordingly.&lt;/p&gt;

&lt;p&gt;And make no mistake, while simpler, policy networks can be just as good as DQNs. In fact, when tuned correctly, they perform even better than DQNs. Don’t believe me? Just ask the authors of the original papers themselves:
&lt;a href=&quot;https://arxiv.org/pdf/1602.01783.pdf&quot;&gt;Asynchronous Methods for Deep Reinforcement Learning&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;my-attempt&quot;&gt;My Attempt&lt;/h2&gt;
&lt;!-- Encouraged by the aforementioned research, I thought I would attempt to create an ANN capable of playing pong using reinforcement learning. Using OpenAI's [Gym](https://gym.openai.com) package to model [Pong](https://gym.openai.com/envs/Pong-v0/), and Google's [*TensorFlow*](https://www.tensorflow.org/) library to construct the network, I'll attempt to explain the code, its results, and its accuracy. --&gt;
&lt;p&gt;Encouraged by the aforementioned research, I thought I would attempt to create an ANN capable of playing pong using reinforcement learning. To do this I used OpenAI’s &lt;a href=&quot;https://gym.openai.com&quot;&gt;Gym&lt;/a&gt; package to model &lt;a href=&quot;https://gym.openai.com/envs/Pong-v0/&quot;&gt;Pong&lt;/a&gt;, and Google’s &lt;a href=&quot;https://www.tensorflow.org/&quot;&gt;&lt;em&gt;TensorFlow&lt;/em&gt;&lt;/a&gt; library to construct the network.&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;https://github.com/ozanerhansha/NeuralNetworks/tree/master/src/pongRL&quot;&gt;code&lt;/a&gt; for this network, dubbed PongNet, is in my &lt;a href=&quot;https://github.com/ozanerhansha/NeuralNetworks&quot;&gt;NeuralNetwork&lt;/a&gt; repository on GitHub.&lt;/p&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;
&lt;p&gt;Learning starts to appear after 1500 games (a game goes on until one player reaches 20 points) and it reaches a 50% win-rate at around 8000 games. More testing needs to be done to see the maximum accuracy of this particular network.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/projects/pongAI/pongdata.png?style=centerme&quot; alt=&quot;pongdata&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Below is the network (on a good day) playing against the same bot it trained with for 10,000 games.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/projects/pongAI/pongai.gif?style=centerme&quot; alt=&quot;bc&quot; /&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Kinematics</title>
   <link href="http://localhost:4000/kinematics/"/>
   <updated>2018-03-17T00:00:00-04:00</updated>
   <id>http://localhost:4000/kinematics</id>
   <content type="html">&lt;!-- Make vectors bold rather than arrow headed --&gt;
&lt;p&gt;$\renewcommand{\vec}[1]{\mathbf{#1}}$
Translational Kinematics describes the motion of objects through space over time. Using an object’s &lt;a href=&quot;/position&quot;&gt;position&lt;/a&gt;, velocity and acceleration vectors, it is possible to predict where it will be in the future and where it was in the past. Where an object gets that velocity/acceleration is what dynamics seeks to explain.&lt;/p&gt;

&lt;h3 id=&quot;variable-acceleration&quot;&gt;Variable Acceleration&lt;/h3&gt;
&lt;p&gt;To calculate the motion of an object with continuous acceleration, simply refer to the definitions of velocity and acceleration:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
\vec{v}=\frac{d\vec{x}}{dt}\\
\vec{a}=\frac{d\vec{v}}{dt}
\end{align}&lt;/script&gt;

&lt;p&gt;Integrating these equations we get:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
\vec{x}=\int\vec{v} \,dt+\vec{x}_0\\
\vec{v}=\int\vec{a} \,dt+\vec{v}_0
\end{align}&lt;/script&gt;

&lt;!--more--&gt;

&lt;ul&gt;
  &lt;li&gt;$\vec{x}$ is the object’s position as a function of time.&lt;/li&gt;
  &lt;li&gt;$\vec{x}_0$ is the initial position, at &lt;script type=&quot;math/tex&quot;&gt;t=0&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;$\vec{v}$ is the object’s velocity as a function of time.&lt;/li&gt;
  &lt;li&gt;$\vec{v}_0$ is the initial velocity, at &lt;script type=&quot;math/tex&quot;&gt;t=0&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;$\vec{a}$ is the object’s acceleration as a function of time.&lt;/li&gt;
  &lt;li&gt;$t$ is time&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;constant-acceleration&quot;&gt;Constant Acceleration&lt;/h3&gt;
&lt;p&gt;When acceleration is assumed to be constant, as is the case for many physical systems, the following kinematic equations can be derived:&lt;/p&gt;

&lt;!-- #### Position Independent Equation
$$\begin{align}
\vec{v}&amp;=\int\vec{a} \,dt+\vec{v}_0 \tag{integral def. of \(\vec{v}\)}\\
&amp;=\vec{a}\int dt+\vec{v}_0 \tag{\(\vec{a}\) is constant}\\
&amp;=\vec{a}t+\vec{v}_0
\end{align}$$ --&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Position Independent Equation&lt;/strong&gt;&lt;/summary&gt;
&lt;p&gt;$$\begin{align}
\vec{v}&amp;amp;=\int\vec{a} \,dt+\vec{v}_0 \tag{integral def. of \(\vec{v}\)}\\
&amp;amp;=\vec{a}\int dt+\vec{v}_0 \tag{\(\vec{a}\) is constant}\\
&amp;amp;=\vec{a}t+\vec{v}_0
\end{align}$$&lt;/p&gt;
&lt;p&gt;$$\boxed{\vec{v}=\vec{v}_0+\vec{a}t}$$&lt;/p&gt;
&lt;/details&gt;

&lt;!-- #### Velocity Independent Equation
$$\begin{align}
\vec{v}&amp;=\vec{a}t+\vec{v}_0 \tag{\(\vec{x}\) independent Eq.}\\
\vec{x}&amp;=\int\vec{v} \,dt+\vec{x}_0 \tag{integral def. of \(\vec{x}\)}\\
&amp;=\int(\vec{a}t+\vec{v}_0) \,dt+\vec{x}_0 \\
&amp;=\int\vec{a}t\,dt + \int\vec{v}_0 \,dt+\vec{x}_0 \\
&amp;=\vec{a}\int t \,dt + \vec{v}_0t+\vec{x}_0 \tag{\(\vec{a}\) is constant}\\
&amp;=\frac{\vec{a}t^2}{2}+\vec{v}_0t+\vec{x}_0
\end{align}$$

$$\boxed{\vec{x}=\frac{\vec{a}t^2}{2}+\vec{v}_0t+\vec{x}_0}$$ --&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Velocity Independent Equation&lt;/strong&gt;&lt;/summary&gt;
&lt;p&gt;$$\begin{align}
\vec{v}&amp;amp;=\vec{a}t+\vec{v}_0 \tag{\(\vec{x}\) independent Eq.}\\
\vec{x}&amp;amp;=\int\vec{v} \,dt+\vec{x}_0 \tag{integral def. of \(\vec{x}\)}\\
&amp;amp;=\int(\vec{a}t+\vec{v}_0) \,dt+\vec{x}_0 \\
&amp;amp;=\int\vec{a}t\,dt + \int\vec{v}_0 \,dt+\vec{x}_0 \\
&amp;amp;=\vec{a}\int t \,dt + \vec{v}_0t+\vec{x}_0 \tag{\(\vec{a}\) is constant}\\
&amp;amp;=\frac{\vec{a}t^2}{2}+\vec{v}_0t+\vec{x}_0
\end{align}$$&lt;/p&gt;
&lt;p&gt;$$\boxed{\vec{x}=\vec{x}_0+\vec{v}_0t+\frac{\vec{a}t^2}{2}}$$&lt;/p&gt;
&lt;/details&gt;

&lt;!-- #### Acceleration Independent Equation
$$\begin{align}
\vec{v}&amp;=\vec{a}t+\vec{v}_0 \tag{\(\vec{x}\) independent Eq.}\\
\vec{a}&amp;=\frac{\vec{v}-\vec{v_0}}{t}\\
\vec{x}&amp;=\frac{\vec{a}t^2}{2}+\vec{v}_0t+\vec{x}_0 \tag{\(\vec{v}\) independent Eq.}\\
&amp;=\frac{\frac{\vec{v}-\vec{v_0}}{t}t^2}{2}+\vec{v}_0t+\vec{x}_0\\
&amp;=\frac{\vec{v_0}+\vec{v}}{2}t+\vec{x}_0\\
\end{align}$$

$$\boxed{\vec{x}=\frac{\vec{v_0}+\vec{v}}{2}t+\vec{x}_0}$$ --&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Acceleration Independent Equation&lt;/strong&gt;&lt;/summary&gt;
&lt;p&gt;$$\begin{align}
\vec{v}&amp;amp;=\vec{a}t+\vec{v}_0 \tag{\(\vec{x}\) independent Eq.}\\
\vec{a}&amp;amp;=\frac{\vec{v}-\vec{v_0}}{t}\\
\vec{x}&amp;amp;=\frac{\vec{a}t^2}{2}+\vec{v}_0t+\vec{x}_0 \tag{\(\vec{v}\) independent Eq.}\\
&amp;amp;=\frac{\frac{\vec{v}-\vec{v_0}}{t}t^2}{2}+\vec{v}_0t+\vec{x}_0\\
&amp;amp;=\frac{\vec{v_0}+\vec{v}}{2}t+\vec{x}_0\\
\end{align}$$&lt;/p&gt;
&lt;p&gt;$$\boxed{\vec{x}=\vec{x}_0+\frac{\vec{v_0}+\vec{v}}{2}t}$$&lt;/p&gt;
&lt;/details&gt;

&lt;!-- #### Time Independent Equation
$$\begin{align}
&amp;\vec{v}=\vec{a}t+\vec{v}_0 \tag{\(\vec{x}\) independent Eq.}\\
&amp;{\vec{a}}t=\vec{v}-\vec{v}_0\\
&amp;\vec{x}=\frac{\vec{v_0}+\vec{v}}{2}t+\vec{x}_0 \tag{\(\vec{a}\) independent Eq.}\\
&amp;2(\vec{x}-\vec{x}_0)=(\vec{v}+\vec{v}_0)t\\
\end{align}$$

$$\begin{align}
2\vec{a}\cdot(\vec{x}-\vec{x}_0)&amp;=(\vec{v}+\vec{v}_0)\cdot\vec{a}t\\
&amp;=(\vec{v}+\vec{v}_0)\cdot(\vec{v}-\vec{v}_0) \tag{foil dot product}\\
&amp;=\vec{v} \cdot \vec{v} - \vec{v}_0 \cdot \vec{v}_0\\
&amp;=\left \| \vec{v} \right \|^2-\left \| \vec{v}_0 \right \|^2\\
\end{align}$$

$$\boxed{\left \| \vec{v} \right \|^2 = 2\vec{a}\cdot(\vec{x}-\vec{x}_0)+\left \| \vec{v}_0 \right \|^2}$$ --&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Time Independent Equation&lt;/strong&gt;&lt;/summary&gt;
&lt;p&gt;$$\begin{align}
&amp;amp;\vec{v}=\vec{a}t+\vec{v}_0 \tag{\(\vec{x}\) independent Eq.}\\
&amp;amp;{\vec{a}}t=\vec{v}-\vec{v}_0\\
&amp;amp;\vec{x}=\frac{\vec{v_0}+\vec{v}}{2}t+\vec{x}_0 \tag{\(\vec{a}\) independent Eq.}\\
&amp;amp;2(\vec{x}-\vec{x}_0)=(\vec{v}+\vec{v}_0)t\\
\end{align}$$

$$\begin{align}
2\vec{a}\cdot(\vec{x}-\vec{x}_0)&amp;amp;=(\vec{v}+\vec{v}_0)\cdot\vec{a}t\\
&amp;amp;=(\vec{v}+\vec{v}_0)\cdot(\vec{v}-\vec{v}_0) \tag{foil dot product}\\
&amp;amp;=\vec{v} \cdot \vec{v} - \vec{v}_0 \cdot \vec{v}_0\\
&amp;amp;=\left \| \vec{v} \right \|^2-\left \| \vec{v}_0 \right \|^2\\
\end{align}$$&lt;/p&gt;

&lt;p&gt;$$\boxed{\left \| \vec{v} \right \|^2 = 2\vec{a}\cdot(\vec{x}-\vec{x}_0)+\left \| \vec{v}_0 \right \|^2}$$&lt;/p&gt;
&lt;/details&gt;

&lt;h3 id=&quot;book-keeping&quot;&gt;Book Keeping&lt;/h3&gt;
&lt;p&gt;There are a couple of things we can do to clean up this set of 4 equations before we display them all together.&lt;/p&gt;

&lt;h4 id=&quot;displacement-vs-position&quot;&gt;Displacement vs. Position&lt;/h4&gt;
&lt;p&gt;While these equations describe the movement of an object in terms of its initial and current position, $\vec{x}_0$ and $\vec{x}$ respectively, it is common to think about kinematics in terms of &lt;strong&gt;displacement&lt;/strong&gt; or change in distance instead. The change in distance $\Delta\vec{x}$ is denoted:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Delta\vec{x}=\vec{x}-\vec{x}_0&lt;/script&gt;

&lt;h4 id=&quot;average-velocity&quot;&gt;Average Velocity&lt;/h4&gt;
&lt;p&gt;If we look at the acceleration independent equation, we can see it contains the expression $\frac{\vec{v_0}+\vec{v}}{2}$. As it turns out, when acceleration is constant, this expression is actually equivalent to the object’s &lt;strong&gt;average velocity&lt;/strong&gt; $\overline{\vec{v}}$:&lt;/p&gt;

&lt;details&gt;
&lt;summary&gt;Derivation&lt;/summary&gt;
&lt;p&gt;$$\begin{align}
&amp;amp;\overline{\vec{v}}=\frac{\vec{x}-\vec{x}_0}{t} \tag{def. of \(\overline{\vec{v}}\)}\\
&amp;amp;\overline{\vec{v}}t=\vec{x}-\vec{x}_0\\
&amp;amp;\vec{x}-\vec{x}_0=\frac{\vec{a}t^2}{2}+\vec{v}_0t \tag{\(\vec{v}\) independent Eq.}\\
&amp;amp;\overline{\vec{v}}t=\frac{\vec{a}t^2}{2}+\vec{v}_0t\\
&amp;amp;\overline{\vec{v}}=\frac{\vec{a}t}{2}+\vec{v}_0\\
&amp;amp;\vec{v}=\vec{a}t+\vec{v}_0 \tag{\(\vec{x}\) independent Eq.}\\
&amp;amp;\vec{a}t=\vec{v}-\vec{v}_0\\
&amp;amp;\overline{\vec{v}}=\frac{\vec{v}-\vec{v}_0}{2}+\vec{v}_0=\frac{\vec{v_0}+\vec{v}}{2}
\end{align}$$&lt;/p&gt;
&lt;/details&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\boxed{\overline{\vec{v}}=\frac{\vec{v_0}+\vec{v}}{2}}&lt;/script&gt;

&lt;h4 id=&quot;dot-product&quot;&gt;Dot Product&lt;/h4&gt;
&lt;p&gt;When deriving the time independent kinematic equation, we used the dot product when manipulating the expressions $(\vec{v}+\vec{v}_0)\cdot \vec{a}$ and $(\vec{v}+\vec{v}_0)\cdot(\vec{v}-\vec{v}_0)$. We were justified in doing so because the dot product is both commutative and distributive.&lt;/p&gt;

&lt;p&gt;The dot product of a vector with itself is that vector’s &lt;strong&gt;squared norm&lt;/strong&gt; and is often denoted as such:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\vec{v}\cdot\vec{v}=\left \| \vec{v} \right \|^2=v^2&lt;/script&gt;

&lt;h4 id=&quot;the-kinematic-equations&quot;&gt;The Kinematic Equations&lt;/h4&gt;
&lt;p&gt;Taking into account average velocity, the simpler dot product notation and using displacement instead of position, we can rewrite the kinematic equations for constant acceleration as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{gather}
&amp;\vec{v}=\vec{v}_0+\vec{a}t \tag{\(\vec{x}\) independent}\\
&amp;\Delta\vec{x}=\vec{v}_0t+\frac{\vec{a}t^2}{2} \tag{\(\vec{v}\) independent}\\
&amp;\Delta\vec{x}=\overline{\vec{v}}t=\frac{\vec{v_0}+\vec{v}}{2}t \tag{\(\vec{a}\) independent}\\
&amp;v^2 = v_0^2+2\vec{a}\cdot\Delta\vec{x} \tag{t independent}
\end{gather} %]]&gt;&lt;/script&gt;

&lt;h3 id=&quot;graphing&quot;&gt;Graphing&lt;/h3&gt;
&lt;p&gt;When the position, velocity and acceleration functions are graphed with respect to time, their graphs are related by the kinematic equations.&lt;/p&gt;

&lt;h4 id=&quot;general-info&quot;&gt;General Info&lt;/h4&gt;
&lt;p&gt;For any graph of $\vec{x}(t)$&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Slope of the secant line created by two points is the average velocity during that time interval.&lt;/li&gt;
  &lt;li&gt;Slope of line tangent to curve at time &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt; is the object’s instantaneous velocity at $t$.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For any graph of $\vec{v}(t)$&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Slope of the secant line created by two points is the average acceleration during that time interval.&lt;/li&gt;
  &lt;li&gt;Slope of line tangent to curve at time &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt; is the object’s instantaneous acceleration at &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;Area under a region of the curve from $[a,b]$ is the object’s displacement during $[a,b]$.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For any graph of $\vec{a}(t)$&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Area under a region of the curve from $[a,b]$ is the object’s change in velocity during $[a,b]$.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;an-object-at-rest&quot;&gt;An Object at Rest&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/assets/physics/kinematics/kinematics_at_rest.png?style=centerme&quot; alt=&quot;graphs&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;$\vec{x}(t) = c$, where c is a constant.&lt;/li&gt;
  &lt;li&gt;$\vec{v}(t)=0$&lt;/li&gt;
  &lt;li&gt;$\vec{a}(t)=0$&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;an-object-with-constant-velocity&quot;&gt;An Object with Constant Velocity&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/assets/physics/kinematics/kinematics_const_vel.png?style=centerme&quot; alt=&quot;graphs&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;$\vec{x}(t)$ is linear.&lt;/li&gt;
  &lt;li&gt;$\vec{v}(t) = c$, where c is a constant.&lt;/li&gt;
  &lt;li&gt;$\vec{a}(t)=0$&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;an-object-with-constant-acceleraion&quot;&gt;An Object with Constant Acceleraion&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/assets/physics/kinematics/kinematics_const_accel.png?style=centerme&quot; alt=&quot;graphs&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;$\vec{x}(t)$ is quadratic.&lt;/li&gt;
  &lt;li&gt;$\vec{v}(t)$ is linear.&lt;/li&gt;
  &lt;li&gt;$\vec{a}(t) = c$, where c is a constant.&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Position</title>
   <link href="http://localhost:4000/position/"/>
   <updated>2018-03-15T00:00:00-04:00</updated>
   <id>http://localhost:4000/position</id>
   <content type="html">&lt;p&gt;The position of a particle or object in &lt;strong&gt;space&lt;/strong&gt; is defined as a point on an n-dimensional &lt;strong&gt;Cartesian coordinate system&lt;/strong&gt;. This point can equivalently be thought of as a vector.&lt;/p&gt;

&lt;h3 id=&quot;classical-physics&quot;&gt;Classical Physics&lt;/h3&gt;
&lt;p&gt;In classical physics, a position or displacement $\vec{x}$ is a 3-dimensional vector with components $\left(x,y,z\right)$.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Position is used to refer to a specific point in a coordinate space while displacement refers to the distance traveled from an initial point (usually this point is the origin making position and displacement equivalent)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The magnitude of displacement, or net &lt;strong&gt;distance&lt;/strong&gt;, is calculated the same as any other vector:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\left|\vec{x}\right|=\sqrt{x^2+y^2+z^2}&lt;/script&gt;

&lt;!--more--&gt;

&lt;h4 id=&quot;displacement-function&quot;&gt;Displacement Function&lt;/h4&gt;
&lt;p&gt;An object’s position can be described as a function of time, $\vec{x}(t)$. This position function would thus map from time to 3D-space:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\vec{x}:\mathbb{R}\rightarrow \mathbb{R}^3&lt;/script&gt;

&lt;p&gt;Taking the derivative of an object’s displacement function with respect to $t$ provides the object’s &lt;a href=&quot;&quot;&gt;velocity&lt;/a&gt; function, $\vec{v}(t)$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\mathrm{d}}{\mathrm{d}t}\vec{x}(t)=\vec{v}(t)&lt;/script&gt;

&lt;!-- Click [here]() for a list of the repeated time derivatives of displacement. --&gt;

&lt;h3 id=&quot;relativity&quot;&gt;Relativity&lt;/h3&gt;
&lt;p&gt;In relativity, position is only invariant in a particular non-inertial reference frame. This necessitates a notion of position that is invariant with respect to space-time. This 4-dimensional position (3 spatial dimensions and 1 time) is called 4-position, or more commonly as an &lt;strong&gt;event&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;An event is defined as a 4-dimensional vector with components $\left(t,x,y,z\right)$. Relativity shows that events that occur at the same time in one reference frame, don’t necessarily happen at the same time in another. This is known as &lt;strong&gt;relativity of simultaneity&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;In special relativity the distance between two positions is not invariant in different reference frames. This brings us to the &lt;strong&gt;spacetime interval&lt;/strong&gt;. The spacetime interval, $(\Delta s)^2$ between two events stays constant in all reference frames:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;(\Delta s)^2=(c\Delta t)^2-(\Delta x)^2-(\Delta y)^2-(\Delta z)^2&lt;/script&gt;

&lt;h3 id=&quot;quantum-mechanics&quot;&gt;Quantum Mechanics&lt;/h3&gt;
&lt;p&gt;In quantum mechanics, the state of a particle is given (and completely described) by its &lt;strong&gt;wavefunction&lt;/strong&gt; $\psi(\mathbf{x}, t)$. With $\mathbf{x}$ being a point in 3-space and $t$ being a point in time. $\psi$ returns a complex valued probability amplitude.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\psi:\mathbb{R^4}\rightarrow \mathbb{C}&lt;/script&gt;

&lt;p&gt;The probability of observing a particle at position $x$ at time $t$ is given by $\left \vert \psi \right \vert^2$. This implies that, in the one dimensional case, for a given time $t$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\int_{-\infty}^{\infty}\left | \psi(x) \right |^2 dx=1&lt;/script&gt;

&lt;p&gt;Because the particle has to be somewhere in space, the probabilities must sum to 1. We integrate from $-\infty$ to $\infty$ since the particle is a wave spread out over all of space. This means that the particle could possibly be anywhere in the universe, just with an unfathomably low probability. With some vector calculus, this can extended to the 3D case we see in the real world.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Physics</title>
   <link href="http://localhost:4000/physics/"/>
   <updated>2018-03-14T00:00:00-04:00</updated>
   <id>http://localhost:4000/physics</id>
   <content type="html">&lt;p&gt;Physics is the branch of science that studies the properties and mechanics of matter and energy in  spacetime and their interaction with the 4 fundamental forces.&lt;/p&gt;

&lt;p&gt;There are 3 main divisions of physics:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#classical-physics&quot;&gt;Classical Physics&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#special--general-relativity&quot;&gt;Special &amp;amp; General Relativity&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#quantum-mechanics-qm&quot;&gt;Quantum Mechanics&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/5/56/Modernphysicsfields.svg?style=centerme&quot; alt=&quot;physics&quot; /&gt;&lt;/p&gt;

&lt;!--more--&gt;

&lt;h3 id=&quot;classical-physics&quot;&gt;Classical Physics&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;/classical-physics&quot;&gt;Classical physics&lt;/a&gt; is a catch all for the theories of physics that predated the relativistic and quantum theories of physics we have today. They very adequately describe the interactions and mechanics of everyday objects, fluids, gasses, electricity, magnetism, and even celestial bodies.&lt;/p&gt;

&lt;p&gt;The 3 main branches of pre-Einsteinian physics are:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;/classical-physics#classical-mechanics&quot;&gt;Classical Mechanics&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/classical-physics#classical-electromagnetism&quot;&gt;Classical Electromagnetism&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/classical-physics#thermodynamics&quot;&gt;Thermodynamics&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;special--general-relativity&quot;&gt;Special &amp;amp; General Relativity&lt;/h3&gt;
&lt;h4 id=&quot;special-relativity-sr&quot;&gt;Special Relativity (SR)&lt;/h4&gt;
&lt;p&gt;When the speeds of the objects being studied approach the speed of light, classical physics no longer suffices to accurately describe physical phenomena. In this case it’s necessary to use Einstein’s theory of special relativity, which is wholly derived from two postulates:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The laws of physics are the same for all non-accelerating frames of reference.&lt;/li&gt;
  &lt;li&gt;The speed of light is constant in all reference frames.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The theory models the 3 dimensions of space and 1 dimension of time as a singular 4D spacetime. It explains various high speed phenomena as well the inconsistencies between classical mechanics and the classical theory of electromagnetism.&lt;/p&gt;

&lt;p&gt;SR has been widely successful and has even been incorporated with quantum mechanics to form quantum field theory. That said, special relativity doesn’t explain or deal with the force of gravity, keeping spacetime flat rather than curved.&lt;/p&gt;

&lt;h4 id=&quot;general-relativity-gr&quot;&gt;General Relativity (GR)&lt;/h4&gt;
&lt;p&gt;This shortcoming led to the theory of general relativity. It generalizes special relativity and Newton’s law of gravitation into a coherent theory where the curvature of spacetime itself becomes responsible for the force of gravity. Unfortunately, unlike special relativity, there has been no successful marriage of general relativity and quantum mechanics. A quantum theory of gravity remains a major goal of modern physics.&lt;/p&gt;

&lt;h3 id=&quot;quantum-mechanics-qm&quot;&gt;Quantum Mechanics (QM)&lt;/h3&gt;
&lt;p&gt;When considering matter and energy on very small scales, both classical and relativistic physics break down. This led to the creation of quantum mechanics, which mathematically grounds the mechanics and temporal evolution of subatomic particles. Quantum mechanics itself is not a theory of a particular type of particle like bosons or photons. Think of it as an analogue to classical mechanics which cover the motion and forces that act upon an object.&lt;/p&gt;

&lt;h4 id=&quot;quantum-field-theory&quot;&gt;Quantum Field Theory&lt;/h4&gt;
&lt;p&gt;By combining quantum mechanics and special relativity, we are left with a framework for creating quantum theories of subatomic particles. QFTs like quantum electrodynamics and chromodynamics both describe the interactions of a particular type of fundamental force (electromagnetism and the strong nuclear force respectively). These theories, via QM, model particles as perturbations (waves) on an underlying field that permeates all of space.&lt;/p&gt;

&lt;!-- &lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Quantum Field Theory&lt;/strong&gt;&lt;/summary&gt;
&lt;p&gt;By combining quantum mechanics and special relativity, we are left with a framework for creating quantum theories of subatomic particles. QFTs like quantum electrodynamics and chromodynamics both describe the interactions of a particular type of fundamental force (electromagnetism and the strong nuclear force respectively). These theories, via QM, model particles as perturbations (waves) on an underlying field that permeates all of space.&lt;/p&gt;
&lt;/details&gt; --&gt;

&lt;!-- #### Quantum Electrodynamics (QED)
Quantum electrodynamics is the QFT of the electromagnetic force. It is the quantum analogue to classical electrodynamics and completely describes the interactions between matter and the electromagnetic force (which is mediated by photons). --&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Quantum Electrodynamics (QED)&lt;/strong&gt;&lt;/summary&gt;
&lt;p&gt;Quantum electrodynamics is the QFT of the electromagnetic force. It is the quantum analogue to classical electrodynamics and completely describes the interactions between matter and the electromagnetic force (which is mediated by photons).&lt;/p&gt;
&lt;/details&gt;

&lt;!-- #### Electroweak Theory (EWT)
Electroweak theory is a QFT that provides a unified description of both the electromagnetic and weak nuclear force (The weak force being responsible for the radioactive decay of atoms).

EWT is thus a generalization of QED that adds on the weak force. Right after the big bang, the universe was hot enough that these two forces were one indistinguishable force. The theory describes the electroweak force before that time and the separate electromagnetic and weak forces after that time. --&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Electroweak Theory (EWT)&lt;/strong&gt;&lt;/summary&gt;
&lt;p&gt;Electroweak theory is a QFT that provides a unified description of both the electromagnetic and weak nuclear force (The weak force being responsible for the radioactive decay of atoms).&lt;/p&gt;

&lt;p&gt;EWT is thus a generalization of QED that adds on the weak force. Right after the big bang, the universe was hot enough that these two forces were one indistinguishable force. The theory describes the electroweak force before that time and the separate electromagnetic and weak forces after that time.&lt;/p&gt;
&lt;/details&gt;

&lt;!-- #### Quantum Chromodynamics (QCD)
Quantum chromodynamics is the QFT of the strong nuclear force. This theory describes the interactions between quarks and gluons. The strong nuclear force is the force that keeps the hadrons (which are made up of quarks and gluons) in atomic nuclei bonded together. --&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Quantum Chromodynamics (QCD)&lt;/strong&gt;&lt;/summary&gt;
&lt;p&gt;Quantum chromodynamics is the QFT of the strong nuclear force. This theory describes the interactions between quarks and gluons. The strong nuclear force is the force that keeps the hadrons (which are made up of quarks and gluons) in atomic nuclei bonded together.&lt;/p&gt;
&lt;/details&gt;

&lt;h4 id=&quot;standard-model-of-physics&quot;&gt;Standard Model of Physics&lt;/h4&gt;
&lt;p&gt;The Standard Model is the combination of QED, EQT, and QCD into one unified theory of electromagnetism, the weak nuclear force, and the strong nuclear force. It is humanity’s ultimate theory of the universe. That said, it still lacks a description of gravity. Such a theory of quantum gravity would allow for the creation of a “Theory of Everything” (TOE) that describes all four fundamental forces in tandem.
&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/0/00/Standard_Model_of_Elementary_Particles.svg?style=centerme&quot; alt=&quot;standardmodel&quot; /&gt;&lt;/p&gt;

&lt;!-- &lt;details open&gt;
&lt;summary&gt;&lt;strong&gt;Standard Model of Physics&lt;/strong&gt;&lt;/summary&gt;
&lt;p&gt;The Standard Model is the combination of all the QFT into one unified theory of electromagnetism, the weak nuclear force, and the strong nuclear force. It is humanity's ultimate theory of the universe. That said, it still lacks a description of gravity. Such a theory of quantum gravity would allow for the creation of a &quot;Theory of Everything&quot; (TOE) that describes all four fundamental forces in tandem.&lt;/p&gt;
&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/0/00/Standard_Model_of_Elementary_Particles.svg?style=centerme&quot; alt=&quot;standard model pic&quot;&gt;&lt;/img&gt;
&lt;/details&gt; --&gt;
</content>
 </entry>
 
 <entry>
   <title>Minkowski Spacetime</title>
   <link href="http://localhost:4000/minkowski-spacetime/"/>
   <updated>2018-01-23T00:00:00-05:00</updated>
   <id>http://localhost:4000/minkowski-spacetime</id>
   <content type="html">&lt;h2 id=&quot;what-is-spacetime&quot;&gt;What is Spacetime?&lt;/h2&gt;
&lt;p&gt;Before we talk about Minkowski Spacetime, we need to address what it means to combine space and time.&lt;/p&gt;

&lt;p&gt;Imagine the Earth orbiting the sun. At every point in time it will be in a different point in its orbit:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.pitt.edu/~jdnorton/teaching/HPS_0410/chapters/spacetime/planet3d.gif?style=centerme&quot; alt=&quot;orbit2D&quot; /&gt;&lt;/p&gt;

&lt;p&gt;If we stack these slices of time in a third (temporal) dimension, it would look like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.pitt.edu/~jdnorton/teaching/HPS_0410/chapters/spacetime/planet4d.gif?style=centerme&quot; alt=&quot;orbit3D&quot; /&gt;&lt;/p&gt;

&lt;!-- *It looks similar to euler's formula, $e^{i\theta}=\cos \theta + i\sin\theta$, with space being the complex plane and time being the angle $\theta$. The imaginary part will come in handy later.* --&gt;

&lt;p&gt;This is an example of a 3D spacetime. It has 2 dimensions of space and 1 of time. Our universe has 3 space and 1 time dimension, making our spacetime (referred to as &lt;em&gt;Minkowski Spacetime&lt;/em&gt;) 4 dimensional.&lt;/p&gt;

&lt;!--more--&gt;

&lt;h4 id=&quot;why-do-this&quot;&gt;Why do this?&lt;/h4&gt;
&lt;p&gt;Spacetime serves as a useful mathematical and intuitive model of the universe that we can use to reason about physics, particularly Einstein’s special relativity. Indeed, it was Einstein’s former teacher &lt;a href=&quot;https://en.wikipedia.org/wiki/Hermann_Minkowski&quot;&gt;Hermann Minkowski&lt;/a&gt; that came up with spacetime as a tool to understand his pupil’s breakthrough theory.&lt;/p&gt;

&lt;h2 id=&quot;definition&quot;&gt;Definition&lt;/h2&gt;
&lt;p&gt;Formally speaking, Minkowski spacetime (which I’ll simply refer to as &lt;em&gt;spacetime&lt;/em&gt;) is an &lt;a href=&quot;\abstract-algebra&quot;&gt;algebraic structure&lt;/a&gt; that represents space purely in mathematical terms. In particular it is a 4D &lt;a href=&quot;https://en.wikipedia.org/wiki/Vector_space&quot;&gt;vector space&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Points in this space are represented by vectors and are referred to as &lt;strong&gt;events&lt;/strong&gt;. The naming stems from the fact that these points represents both moments in time as well as locations in space.&lt;/p&gt;

&lt;!-- Remove link to vector space wikipedia --&gt;
&lt;!-- *If you don't know what a vector space is, I've written a post about it [here](\2018\01\20\vector-spaces).* --&gt;

&lt;p&gt;An event, usually denoted $s$, is represented by a 4D vector where $x,y,z,t$ are real numbers:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;s=(x,y,z,ict)&lt;/script&gt;

&lt;p&gt;You may have noticed that the last coordinate, which corresponds to time, is multiplied by a factor of $ic$. $i$, as we know, is the imaginary unit and multiplying it by time makes the fourth coordinate an imaginary number. The fourth coordinate always being an imaginary number means that spacetime consists of 3 real spatial dimensions and one imaginary temporal dimension.&lt;/p&gt;

&lt;p&gt;$c$ represents the speed of light (299,792,458 m/s) and when multiplied by a time value (like seconds) it gives the distance a photon would travel in that time. This is necessary to convert from time units to length units. It wouldn’t really make sense to have a position vector with different units for dimensions, as we’ll see…&lt;/p&gt;

&lt;h4 id=&quot;an-example&quot;&gt;An Example&lt;/h4&gt;
&lt;p&gt;Say you walk 2m forward out of your house, walk 3 meters to the right, and jump (an impressive) 1 meter into the air. Let’s also say that at the moment you were 1 meter in the air, 6 seconds had passed.&lt;/p&gt;

&lt;p&gt;Setting the event of you leaving your house as the origin, the coordinates of the event of you jumping in the air are:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;x = 2m (forward)&lt;/li&gt;
  &lt;li&gt;y = 3m (to the right)&lt;/li&gt;
  &lt;li&gt;z = 1m (in the air)&lt;/li&gt;
  &lt;li&gt;t = 6s (had passed since you left the house)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Thus that event in spacetime is represented by the ordered quadruplet:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
  s &amp;= (2,3,1,6\text{c}i) \\
  &amp;= (2,3,1,1.80i\times 10^9) \\
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;Note that the time coordinate is much bigger in magnitude due to how large the speed of light is.&lt;/p&gt;

&lt;p&gt;In this framework you can, loosely, consider 1 second to be equivalent to 299,792,458 imaginary meters.&lt;/p&gt;

&lt;h2 id=&quot;proper-distance&quot;&gt;Proper Distance&lt;/h2&gt;
&lt;p&gt;The distance between two points in space is given as such:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Delta d=\sqrt{(\Delta x)^2+(\Delta y)^2+(\Delta z)^2}&lt;/script&gt;

&lt;p&gt;Where $\Delta x,\Delta y,\Delta z$ represent the distance between the two $x/y/z$ coordinates of the two points. (i.e $\Delta x = x_2 - x_1$)&lt;/p&gt;

&lt;p&gt;But special relativity tells us that space and time are relative and so it might be useful to have a notion of ‘distance’ for events in space&lt;em&gt;time&lt;/em&gt; and not just space.&lt;/p&gt;

&lt;p&gt;This analogue is called &lt;strong&gt;proper distance&lt;/strong&gt; (denoted $\Delta s$) and we obtain it just like we did the one above. Square each coordinate, add them together, and take that sum’s square root:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Delta s=\sqrt{(\Delta x)^2+(\Delta y)^2+(\Delta z)^2-\text{c}^2(\Delta t)^2}&lt;/script&gt;

&lt;p&gt;This measure of ‘distance’ is similar to the standard spatial one, barring the strange looking last term $-\text{c}^2(\Delta t)^2$. Upon further consideration, however, this term makes just as much sense as the others:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
(x)^2&amp;=x^2 \\
(y)^2&amp;=y^2 \\
(z)^2&amp;=z^2 \\
(i\text{c}t)^2&amp;=i^2\text{c}^2t^2=-\text{c}^2 t^2
\end{align*} %]]&gt;&lt;/script&gt;

&lt;h4 id=&quot;an-example-1&quot;&gt;An Example&lt;/h4&gt;
&lt;p&gt;Say you fly a spaceship 3000km into space from earth (the origin), let’s call that the y-axis, in the span of 1ms (this is a very fast spaceship).&lt;/p&gt;

&lt;p&gt;So since earth is the origin these are the change in coordinates:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;$\Delta x = 0m$&lt;/li&gt;
  &lt;li&gt;$\Delta y = 4000km = 4000000m$&lt;/li&gt;
  &lt;li&gt;$\Delta z = 0m$&lt;/li&gt;
  &lt;li&gt;$\Delta t = 1ms = .01s$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Thus the proper distance between the origin and that event in spacetime is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
  \Delta s &amp;= \sqrt{0^2+(3.0\times 10^6)^2+0^2+((.01)\text{c}i)^2} \text{ meters} \\
  &amp;= \sqrt{(4.0\times 10^6)^2+(2.997i\times 10^6)^2} \text { meters} \\
  &amp;= \sqrt{(4.0\times 10^6)^2-(2.997\times 10^6)^2} \text { meters} \\
  &amp;= 2.65\times 10^6 \text { meters} \\
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;Remember this distance isn’t through space but spacetime, so it doesn’t have quite the same meaning as you may first think.&lt;/p&gt;

&lt;h4 id=&quot;proper-time&quot;&gt;Proper Time&lt;/h4&gt;
&lt;p&gt;If you take a good look at the formula for proper distance you’ll notice a problem: What if radicand is negative?&lt;/p&gt;

&lt;p&gt;Take the example of jumping in the air form earlier. If we use the proper distance formula with it we get:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
  \Delta s &amp;= \sqrt{2^2+3^2+1^2+(6\text{c}i)^2} \text{ meters} \\
  &amp;= \sqrt{4+9+1+(1.799i\times 10^9)^2} \text { meters} \\
  &amp;= \sqrt{4+9+1-(1.799\times 10^9)^2} \text { meters} \\
  &amp;= \sqrt{-3.236\times 10^{18}} \text { meters} \\
  &amp;= 1.799i\times 10^9 \text { meters} \\
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;While we did accept the idea of imaginary time earlier, we cannot accept an imaginary proper distance. I mean the whole point was to get rid of the imaginary part right? So where did we go wrong? Well as it turns out, spacetime is a bit more complicated than I first led on (how surprising).&lt;/p&gt;

&lt;p&gt;Two events in spacetime have the property of being either spacelike seperated or timelike seperated. The difference between them is essentially whether the two events could be causally connected (they are close enough to be affected by a particle traveling at the speed of light). See &lt;a href=&quot;https://en.wikipedia.org/wiki/Light_cone&quot;&gt;light cones&lt;/a&gt; for more.&lt;/p&gt;

&lt;p&gt;The proper distance is the formula used for &lt;em&gt;spacelike&lt;/em&gt; separated events. Another quantity, known as &lt;a href=&quot;https://en.wikipedia.org/wiki/Proper_time&quot;&gt;proper time&lt;/a&gt;, is used to measure the length between two &lt;em&gt;timelike&lt;/em&gt; events in spacetime.&lt;/p&gt;

&lt;h2 id=&quot;asides&quot;&gt;Asides&lt;/h2&gt;
&lt;p&gt;What I’ve described in this post is a very basic notion of spacetime and how one may codify events in it. There are many more aspects of spacetime and events in it like 4-velocities (the spacetime equivalent to velocity), 4-accelerations, proper time, Lorentz transformations, etc.&lt;/p&gt;

&lt;p&gt;Also note that our normal notion of distance and time still apply to spacetime. All that’s different is the fact that space and time are not invariant. That is to say, the regular old spatial &lt;em&gt;distance&lt;/em&gt; between two events can change depending on your reference frame, but the &lt;em&gt;proper distance&lt;/em&gt; between them is constant in all reference frames.&lt;/p&gt;

&lt;h4 id=&quot;regarding-imaginary-time&quot;&gt;Regarding Imaginary Time&lt;/h4&gt;
&lt;p&gt;You may have felt a little uneasy when I introduced time as being an “imaginary 4th dimension” of spacetime or a second being equivalent to 299,792,458 imaginary meters. If so, you aren’t alone. Physicists aren’t so fond of concepts like imaginary time and so they’ve devised ways of getting around that, namely with something called a &lt;a href=&quot;https://en.wikipedia.org/wiki/Metric_signature&quot;&gt;metric signature&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This has its benefits but, for our purposes, Minkowski spacetime is easier to understand if we consider time to be an imaginary dimension. It melds well with the definition of proper length.&lt;/p&gt;

&lt;p&gt;Moreover, I think considering time as an imaginary dimension provides a useful analogy for its ephemeral nature. We can’t &lt;em&gt;move&lt;/em&gt; through time like we can through space. This is analogous to us not being able to directly measure imaginary quantities like we can with real ones.&lt;/p&gt;

&lt;!-- #### Regarding Vector Spaces
Also while Minkowski Spacetime is generally considered a vector space, if the set of values that the coordinates come from aren't frost he same Field of numbers (the space coordinates come from the real numbers and the time coordinate comes from the imaginary numbers) then it technically can't be considered a vector space. This is another blow to the imaginary time version of spacetime. --&gt;
</content>
 </entry>
 
 <entry>
   <title>Compression, Images, and FileToPNG</title>
   <link href="http://localhost:4000/filetopng/"/>
   <updated>2018-01-16T00:00:00-05:00</updated>
   <id>http://localhost:4000/filetopng</id>
   <content type="html">&lt;h2 id=&quot;what-is-filetopng&quot;&gt;What is FileToPNG?&lt;/h2&gt;
&lt;p&gt;FileToPNG is a tool I wrote in Java that converts the raw binary representation of any file into a corresponding PNG image representation. The resulting PNG (being a lossless file type) can be sent through text, email, etc. before finally being reconstructed back into its original form.&lt;/p&gt;

&lt;p&gt;The project’s GitHub repository can be found &lt;a href=&quot;https://github.com/ozanerhansha/FileToPNG&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;While FileToPNG’s main purpose was to be a tool that allowed for the sending of files over image only communication channels, I’ve found that, based on what type of files are put into it, the resulting images show some interesting patterns. I will elaborate on a couple of these findings in this post.&lt;/p&gt;

&lt;!--more--&gt;

&lt;h2 id=&quot;the-main-algorithm&quot;&gt;The Main Algorithm&lt;/h2&gt;
&lt;p&gt;While I have programmed a primitive UI (the code for which can be found &lt;a href=&quot;https://github.com/ozanerhansha/FileToPNG/blob/master/src/Main.java&quot;&gt;here&lt;/a&gt;) to make using the tool easier, it’s not the main focus of FileToPNG and is really just boilerplate Java.&lt;/p&gt;

&lt;p&gt;The real meat of the program is the conversion functions for PNG to File and vice versa. The code for this can be found &lt;a href=&quot;https://github.com/ozanerhansha/FileToPNG/blob/master/src/GFile.java&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The algorithm goes something like this:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Convert the file into a string of bytes (8 bits = 1 byte).&lt;/li&gt;
  &lt;li&gt;Read through this list and make a pixel out of every 3 bytes. This can be done because a pixel is comprised of 3 colors (red, green, and blue) each with a value from 0-255. These 256 values can be represented in 8 bits (because 2&lt;sup&gt;8&lt;/sup&gt;=256).&lt;/li&gt;
  &lt;li&gt;Once a list of pixels (which should be about a 1/3 of the size of the byte list) has been made, create a square image (in .png format) out of these pixels.&lt;/li&gt;
  &lt;li&gt;Account for files whose bytes aren’t a multiple of 3. (I used special ‘2 byte’ and ‘1 byte’ pixels that have different alpha values to demarcate them)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Reconstructing the images into files is as simple as reversing this process.&lt;/p&gt;

&lt;h4 id=&quot;side-note-on-function-mapping&quot;&gt;Side Note on Function Mapping&lt;/h4&gt;
&lt;p&gt;You can think of the program as a function that maps the set of all &lt;a href=&quot;https://en.wikipedia.org/wiki/Kleene_star&quot;&gt;finite bit strings&lt;/a&gt; to the set of all square images. Because this image is a square of pixels with 4 layers (red, green, blue, and alpha) we can represent it as the set of all rank 3 tensors with size $n\times n\times 4$ and integer elements from $0$ to $255$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{FileToPNG}:\{0,1\}^+\to \left(\mathbb{N}_{255}\right)^{n\times n\times 4}&lt;/script&gt;

&lt;h4 id=&quot;side-note-on-exclusion-of-alpha-channel&quot;&gt;Side Note on Exclusion of Alpha Channel&lt;/h4&gt;
&lt;p&gt;You may notice that while .png files have 4 color channels (red, green, blue and alpha), I only use 3 to store the data of the file and leave the alpha channel to indicate whether a given pixel only holds 1 or 2 bytes of information rather than 3.&lt;/p&gt;

&lt;p&gt;This is obviously inefficient and I’m sure there are better ways of representing the file as a png while simultaneously taking advantage of all 4 channels, effectively saving 25% more space for each pixel. One way to do this might have to do with the dimensions of the image. There is no reason for the image to be a square.&lt;/p&gt;

&lt;p&gt;I haven’t don anything about this because, in its current state, the algorithm is better suited to exploring the representations of the data it encodes rather than efficiently compressing them in image form. Having the alpha channel available would make the resulting images (and information they represent) harder to visualize.&lt;/p&gt;

&lt;h2 id=&quot;properties-of-png-representations&quot;&gt;Properties of PNG Representations&lt;/h2&gt;
&lt;p&gt;FileToPNG doesn’t do any sort of encrypting or compression of the file’s data (as of yet). As such, the PNG representation it spits out can give us an unfiltered look at the file’s structure.&lt;/p&gt;

&lt;h3 id=&quot;text-files&quot;&gt;Text Files&lt;/h3&gt;
&lt;p&gt;Take text files for example. Their content, of course, varies widely depending on the length and topic of discussion. But, even if they don’t use the same language, they all still use the same set of Unicode characters which all have the same configuration of bits. Meaning they share some similarities:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/projects/filetopng/text_diagram.png?style=centerme&quot; alt=&quot;Text in FileToPNG&quot; width=&quot;500px&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;music-files&quot;&gt;Music Files&lt;/h3&gt;
&lt;p&gt;This one was kind of surprising. When I took an .mp3 file of a song and put it through the program, the resulting image was seemingly random:&lt;/p&gt;

&lt;!-- ![mp3](/assets/projects/filetopng/song_mp3.png?style=centerme){:width=&quot;300px&quot;}

### 1MB file of random noise for comparison:
![Random Noise](/assets/projects/filetopng/random_data.png?style=centerme){:width=&quot;300px&quot;} --&gt;

&lt;p&gt;&lt;img src=&quot;/assets/projects/filetopng/random_music.png?style=centerme&quot; alt=&quot;mp3&quot; width=&quot;700px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I was disappointed at first, but then I realized that randomness is fundamentally information packed. That is to say, if the PNG representation had some sort of overarching structure, then that would imply that there is still some redundancy in the file that could be compressed, thereby making the file smaller and more random looking.&lt;/p&gt;

&lt;p&gt;This lines up with the fact that mp3 files are &lt;strong&gt;lossy&lt;/strong&gt; (i.e they sacrifice perfect quality for a smaller size).&lt;/p&gt;

&lt;p&gt;The natural step forward at that point was to get a .wav version of the song, which is &lt;strong&gt;lossless&lt;/strong&gt;, and see if it had any structure. And, as expected, it did:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/projects/filetopng/song_wav.png?style=centerme&quot; alt=&quot;wav&quot; width=&quot;600px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Note the repeating bands of different ‘frequencies.’ The image was also much bigger in area due to being lossless (the size difference isn’t reflected here because the image would extend out of the webpage).&lt;/p&gt;

&lt;h4 id=&quot;qualitative-measure-of-information-density&quot;&gt;Qualitative Measure of Information Density&lt;/h4&gt;
&lt;p&gt;It is in this way that we can examine how “densely packed” with information a given file is. If we see patterns in the colors or structure of the image, we know there can be more compression done. However, if the image is indistinguishable from random noise, then it’s probably rich in information.&lt;/p&gt;

&lt;h3 id=&quot;nintendo-64-roms&quot;&gt;Nintendo 64 ROMs&lt;/h3&gt;
&lt;p&gt;As a final test, I thought I would try converting a couple of Nintendo 64 games (in file form) that were on my desktop to PNGs and, unsurprisingly, they share many similarities:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/projects/filetopng/n64_diagram.png?style=centerme&quot; alt=&quot;N64 Roms in FileToPNG&quot; width=&quot;500px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Interested, I did some research into N64 cartridges and found out that all ‘ROM dumps’ (files that were created by copying a physical N64 cartridge onto a computer) have what’s called a &lt;em&gt;header&lt;/em&gt;. This header includes custodial information like the game’s version, internal name, and other bits of information that might be interesting to a game historian. These headers are the black boxes at the top of &lt;em&gt;Super Mario 64&lt;/em&gt; and &lt;em&gt;Mario Kart 64&lt;/em&gt; respectively.&lt;/p&gt;

&lt;p&gt;You may also notice that the &lt;em&gt;Super Mario 64&lt;/em&gt; rom is smaller than the &lt;em&gt;Mario Kart 64&lt;/em&gt; one, implying that it has a smaller file size. Apparently, N64 games came on a variety of hardware with different storage capacities and capabilities. This may seem normal nowadays but in an era where all games had to work on the same piece of limited hardware, this variability is pretty amazing.&lt;/p&gt;

&lt;p&gt;Another similarity can be spotted after the last black bar, at then bottom of the images. Past this point, both the ROMs consist of ‘garbage data’ meant to fill up the game cartridge they were stored on. You can tell because this section looks more like the random sample then the rest of the picture.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Breast Cancer Classification</title>
   <link href="http://localhost:4000/breast-cancer-classification/"/>
   <updated>2018-01-15T00:00:00-05:00</updated>
   <id>http://localhost:4000/breast-cancer-classification</id>
   <content type="html">&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;
&lt;p&gt;I made this model to test how easily I could use external datasets to create and train a neural network with Google’s &lt;a href=&quot;https://www.tensorflow.org/&quot;&gt;&lt;em&gt;TensorFlow&lt;/em&gt;&lt;/a&gt; library. In this post I’ll attempt to explain the code, its results, and its accuracy. The &lt;a href=&quot;https://github.com/ozanerhansha/NeuralNetworks/blob/master/src/test/bcDiagnosis.py&quot;&gt;entire program&lt;/a&gt; is in my &lt;a href=&quot;https://github.com/ozanerhansha/NeuralNetworks&quot;&gt;NeuralNetwork&lt;/a&gt; repository on GitHub as well as at the end of this post.&lt;/p&gt;

&lt;h2 id=&quot;the-training-data&quot;&gt;The Training Data&lt;/h2&gt;
&lt;p&gt;All the training data comes from the &lt;a href=&quot;http://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+%28diagnostic%29&quot;&gt;Wisconsin Breast Cancer Data Set&lt;/a&gt;, hosted by the University of California’s &lt;a href=&quot;http://archive.ics.uci.edu/ml/index.php&quot;&gt;Machine Learning Repository&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The training data consists of 569 subjects each with 3 values (mean, worst value, and standard error) for each of 10 categories:&lt;/p&gt;

&lt;!--more--&gt;

&lt;ul&gt;
  &lt;li&gt;Radius (mean of distances from center to points on the perimeter)&lt;/li&gt;
  &lt;li&gt;Texture (standard deviation of gray-scale values)&lt;/li&gt;
  &lt;li&gt;Perimeter&lt;/li&gt;
  &lt;li&gt;Area&lt;/li&gt;
  &lt;li&gt;Smoothness (local variation in radius lengths)&lt;/li&gt;
  &lt;li&gt;Compactness $(\frac{perimeter^2}{area} - 1)$&lt;/li&gt;
  &lt;li&gt;Concavity (severity of concave portions of the contour)&lt;/li&gt;
  &lt;li&gt;Concave points (number of concave portions of the contour)&lt;/li&gt;
  &lt;li&gt;Symmetry&lt;/li&gt;
  &lt;li&gt;Fractal dimension (measure of edge complexity)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;The meaning of these values is irrelevant to our purpose. All we need to know is that we can use these $10\times3=30$ values to predict whether the given breast cancer sample is malignant or benign.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;All the training examples are stored on a CSV (Comma Separated Value) file called &lt;code class=&quot;highlighter-rouge&quot;&gt;wdbc.data&lt;/code&gt;, so our first job is to import the file into our program:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;os&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;#Used for finding file path&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;#Get dataset file path&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dirname&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__file__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;file_name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'wdbc.data'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;file_object&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;file_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'r'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;cleaning-the-data&quot;&gt;Cleaning the Data&lt;/h2&gt;
&lt;p&gt;Now that it’s imported, we can start ‘cleaning’ the data (that is turning this large string of numbers into a list of training examples labeled  &lt;strong&gt;benign&lt;/strong&gt; or &lt;strong&gt;malignant&lt;/strong&gt;).&lt;/p&gt;

&lt;p&gt;First off we have to split up the dataset, which is currently just one massive string, into a bunch of strings. One for each subject (patient):&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;#Split dataset into separate points (as strings)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;string_points&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;file_object&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;string_points&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If you take a look at the actual &lt;a href=&quot;https://github.com/ozanerhansha/NeuralNetworks/blob/master/src/test/wdbc.data&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;wdbc.data&lt;/code&gt;&lt;/a&gt; file, you’ll see that some of the subjects are clustered in groups of benign and malignant. If we trained the network with the subjects in this order, it would bias the network’s guesses. To avoid this, we’ll shuffle the subjects:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shuffle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;string_points&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;#Randomize (avoid bias)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Next we create a list of &lt;strong&gt;feature vectors&lt;/strong&gt;, each with 30 entries. A feature vector is simply a list of all the data we have about a particular problem, or in this case a particular subject. Its purpose is to be transformed into the desired answer by the neural network. It’s the &lt;strong&gt;input&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;We will also create a list of $y$ labels (&lt;code class=&quot;highlighter-rouge&quot;&gt;[0,1]&lt;/code&gt; for benign and &lt;code class=&quot;highlighter-rouge&quot;&gt;[1,0]&lt;/code&gt; for malignant) that correspond with the list of feature vectors. This is the desired &lt;strong&gt;output&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;We’ll be using the &lt;a href=&quot;http://www.numpy.org&quot;&gt;numpy&lt;/a&gt; library here, so let’s import it as well:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;num_input_features&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;30&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;#30 data points per subject&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;#Initialize dataset class arrays&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;point_array&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;empty&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_input_features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y_labels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;empty&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;#Trim data points&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#Format as np.arrays&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#Add to class arrays (Benign or Malignant)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;point&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;string_points&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;point&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;point&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;','&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'M'&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;point&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;#if malignant&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;y_labels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;#if benign (can only be labeled 'B' or 'M')&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;y_labels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;point&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;point&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;#trim for only the 10 important features&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;temp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;point&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;#convert to numpy array&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;temp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;temp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;astype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;#cast as float array&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;point_array&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;point_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;temp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;All that’s left is to split up the subjects (the list of feature vectors) into training and testing sets. Testing using the same data as the network was trained on encourages the network to memorize the data rather than generalize the dataset make meaningful predictions.&lt;/p&gt;

&lt;p&gt;Of the 569 subjects, the first 400 will be used to train the network while the remaining 169 will be used to test the network’s accuracy:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;#Split training and testing data&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;experience_matrix&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;point_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;400&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;experience_matrix_y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;400&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;test_matrix&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;point_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;400&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;test_matrix_y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;400&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;constructing-the-network&quot;&gt;Constructing the Network&lt;/h2&gt;
&lt;p&gt;A single layer perceptron network seems to do well enough in fitting the test data, so we’ll build one here. The standard model for such a network is given by:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{y}=\text{softmax}(Wx+b)&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;$x$ is the input (a 30 dimensional vector of all the cancer cell’s statistical values).&lt;/li&gt;
  &lt;li&gt;$\hat{y}$ is the network’s approximation of $y$, the right answer. (a $2$D vector with the first/second value corresponding to its confidence that the cell is benign/malignant).&lt;/li&gt;
  &lt;li&gt;$W$ is a matrix of weights (the matrix is $30\times2$ so that it transforms $30$ dimensional vectors into $2$ dimensional ones).&lt;/li&gt;
  &lt;li&gt;$b$ is a vector of ‘bias’ values (a $2$D vector that allows the network more freedom when training, similar to the y-intercept in a linear equation).&lt;/li&gt;
  &lt;li&gt;The $\text{softmax}$ function is the network’s &lt;a href=&quot;https://en.wikipedia.org/wiki/Activation_function&quot;&gt;activation function&lt;/a&gt;, which introduces a nonlinearity to the network. This is integral for any neural network to learn from the data it’s provided. The function also equalizes the network’s confidence predictions so that they add up to 100%. The mathematical description of softmax is:&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/thumb/5/5b/Mplwp_logistic_function.svg/1280px-Mplwp_logistic_function.svg.png&quot; alt=&quot;softmax graph&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here’s what the model looks like in python with &lt;a href=&quot;https://www.tensorflow.org/&quot;&gt;TensorFlow&lt;/a&gt;:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;tensorflow&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;#nx10 Matrix (Input)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;placeholder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_input_features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#10x1 Matrix (Weights)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Variable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_input_features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#1x2 vector (Bias)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Variable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#A scalar (x*W + b)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;y_noSoftmax&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matmul&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;#Apply softmax (scales y_noSoftmax to be between 0 &amp;amp; 1)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y_hat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;softmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_noSoftmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'y_Hat'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;em&gt;Note that we’ve computed $Wx+b$ first before computing its $\text{softmax}$. Separating these steps will allow us to compute the error function more easily with the TensorFlow library later on.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;training-the-network&quot;&gt;Training the Network&lt;/h2&gt;
&lt;p&gt;Now that we’ve assembled the network, we need to train it using the training data, or &lt;code class=&quot;highlighter-rouge&quot;&gt;experience_matrix&lt;/code&gt;, and the associated training labels we made earlier.&lt;/p&gt;

&lt;p&gt;TensorFlow works by creating what is called a computational graph from which it can calculate and find derivatives of every value in the network, thereby allowing it to optimize (i.e teach) the neural network via backpropagation &lt;a href=&quot;http://colah.github.io/posts/2015-08-Backprop/&quot;&gt;(Christopher Olah’s blog has a great post on this)&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;And so, the network won’t start until we create a new graph (session) on which all the training calculations can run:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;#Session&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;InteractiveSession&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;#Create Session&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;global_variables_initializer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;#Init Variables&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Next we create the actual training model. First we create a placeholder for the labels of the training data, $y$.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;#nx2 Matrix (One-Hot Vector, Label Data)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;placeholder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'y_Labeled'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Then we create a loss function. A loss function is basically a measure of how off the network is in its guesses. This means the smaller the output of this function, the more accurate our network becomes. So, naturally, if we minimize this function we will have effectively &lt;em&gt;trained&lt;/em&gt; the network. This is deep learning in a nutshell. Here we use &lt;strong&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Cross_entropy&quot;&gt;Cross Entropy&lt;/a&gt;&lt;/strong&gt;, as it works nicely with the softmax layer we have at the end.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;#Loss Function (cross entropy between y and y_hat)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;cross_entropy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reduce_mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;softmax_cross_entropy_with_logits&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;logits&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_noSoftmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Loss_Function'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Finally we create the training step. This is where we choose what optimization method to use. In this case we’ll use the tried and true &lt;strong&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Gradient_descent&quot;&gt;Gradient Descent&lt;/a&gt;&lt;/strong&gt;. I reccommend you look into it yourself, but the gist is that we take partial derivatives of the cost function with respect to every weight variable in the network then slightly adjust them in the direction that would minimize the cost.&lt;/p&gt;

&lt;p&gt;This algorithm approaches the local minimum rapidly at first but then slows down once it has gotten close. Here’s a visualization:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/projects/breast-cancer/gradient_descent_3D.gif?style=centerme&quot; alt=&quot;SGD&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And here it is in tensorflow:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;#Performs Gradient Descent on loss function&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;train_step&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;GradientDescentOptimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;minimize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;cross_entropy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Train_Step'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now that we have a training step (a single iteration of gradient descent) we can just run that operation, say, 1000 times for each of the 400 training examples.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;#Run train step repeatedly&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
  &lt;span class=&quot;c&quot;&gt;#Run training step on the entire batch&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;train_step&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feed_dict&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;experience_matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;experience_matrix_y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Also note that in the code above, this is where we finally plug our training data (&lt;code class=&quot;highlighter-rouge&quot;&gt;experience_matrix&lt;/code&gt;) and labels (&lt;code class=&quot;highlighter-rouge&quot;&gt;experience_matrix_y&lt;/code&gt;) into the network. That means that everything we’ve done (in terms of constructing/training the model) can be generalized to many other machine learning problems.&lt;/p&gt;

&lt;h2 id=&quot;assessing-the-network&quot;&gt;Assessing the Network&lt;/h2&gt;
&lt;p&gt;To assess how accurate our network is, we simply have to plug in our testing data (&lt;code class=&quot;highlighter-rouge&quot;&gt;test_matrix&lt;/code&gt;) and labels (&lt;code class=&quot;highlighter-rouge&quot;&gt;test_matrix_y&lt;/code&gt;) into the network and pick the catagory, benign or malignant, that it is most confident in.&lt;/p&gt;

&lt;p&gt;First we’ll make a list of Booleans which represent whether the network got the right answer or not:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;#Returns whether model made correct prediction (List of booleans)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;correct_prediction&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;equal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;argmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_hat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;argmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'isCorrect'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Then we average all the results together to arrive at an percent accuracy:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;#Average of correct prediction (%Accuracy)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;accuracy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reduce_mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cast&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;correct_prediction&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Accuracy'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Finally, we just plug in our testing data and labels and print the accuracy (with a bit of pretty formatting) to the console.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;#Print Accuracy&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Accuracy:&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'{0:.2&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;}'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;accuracy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;eval&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feed_dict&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_matrix_y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;})))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;When we finally run the code our output should look something like this:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Accuracy: 90.53%&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Accuracy: 91.12%&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Accuracy: 88.17%&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Because we randomized our sample at the start, the accuracy of the network varies slightly each run.&lt;/p&gt;

&lt;p&gt;That said the slight increase/decrease in accuracy are just products of how the weights work out with the examples. As such any perceived improvement is probably coincidental and don’t reflect how accurate the network will be in the face of new samples. To get an accurate view of the network’s ability, running it multiple times and taking the average of its accuracy is probably your best bet.&lt;/p&gt;

&lt;p&gt;Doing this for our network yields about a &lt;strong&gt;90% accuracy&lt;/strong&gt;. Not bad. For reference, a monkey (that is, a random process) would classify the cells correctly 50% of the time (there are only two categories). That’s a 40% increase over randomly guessing!&lt;/p&gt;

&lt;h2 id=&quot;the-full-code&quot;&gt;The Full Code&lt;/h2&gt;
&lt;p&gt;This is all the code put together and is how it appears on my &lt;a href=&quot;https://github.com/ozanerhansha/NeuralNetworks&quot;&gt;NeuralNetwork&lt;/a&gt; repository. All the imports have been moved to the top of the program:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;s&quot;&gt;'''
Created on Jun 6, 2017
@author: Ozaner Hansha
'''&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;os&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;tensorflow&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;random&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;#Get dataset file path&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dirname&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__file__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;file_name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'wdbc.data'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;file_object&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;file_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'r'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;#Split dataset into separate points (as strings)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;string_points&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;file_object&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;string_points&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shuffle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;string_points&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;#Randomize (avoid bias)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;num_input_features&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;30&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;#Initialize dataset class arrays&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;point_array&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;empty&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_input_features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y_labels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;empty&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;#Trim data points&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#Format as np.arrays&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#Add to class arrays (Benign or Malignant)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;point&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;string_points&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;point&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;point&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;','&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'M'&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;point&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;#if malignant&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;y_labels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;#if benign (can only be labeled 'B' or 'M')&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;y_labels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;point&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;point&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;#trim for only the 10 important features&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;temp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;point&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;#convert to numpy array&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;temp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;temp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;astype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;#cast as float array&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;point_array&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;point_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;temp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;#Split training and testing data&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;experience_matrix&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;point_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;400&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;experience_matrix_y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;400&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;test_matrix&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;point_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;400&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;test_matrix_y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;400&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# End of data import/cleanup. Begin construction of neural network&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name_scope&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Hidden_Layer'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
  &lt;span class=&quot;c&quot;&gt;#nx10 Matrix (Input)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;placeholder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_input_features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
  &lt;span class=&quot;c&quot;&gt;#10x1 Matrix (Weights)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Variable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_input_features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;
  &lt;span class=&quot;c&quot;&gt;#1x2 vector (Bias)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Variable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;
  &lt;span class=&quot;c&quot;&gt;#A scalar (x*W + b)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;y_noSoftmax&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matmul&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;#Apply softmax (scales y_noSoftmax to be between 0 &amp;amp; 1)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y_hat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;softmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_noSoftmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'y_Hat'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;#Session&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;InteractiveSession&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;#Create Session&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;global_variables_initializer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;#Init Variables&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;#Training Model&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name_scope&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Training'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
  &lt;span class=&quot;c&quot;&gt;#nx2 Matrix (One-Hot Vector, Label Data)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;placeholder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'y_Labeled'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;c&quot;&gt;#Loss Function (cross entropy between y and y_hat)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;cross_entropy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reduce_mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;softmax_cross_entropy_with_logits&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;logits&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_noSoftmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Loss_Function'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;c&quot;&gt;#Performs Gradient Descent on loss function&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;train_step&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;GradientDescentOptimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;minimize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;cross_entropy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Train_Step'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;#Run train step repeatedly&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
  &lt;span class=&quot;c&quot;&gt;#Run training step on that batch&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;train_step&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feed_dict&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;experience_matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;experience_matrix_y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;#Evaluation&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name_scope&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Validation'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
  &lt;span class=&quot;c&quot;&gt;#Returns whether model made correct prediction (List of booleans)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;correct_prediction&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;equal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;argmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_hat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;argmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'isCorrect'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;c&quot;&gt;#Average of correct prediction (%Accuracy)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;accuracy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reduce_mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cast&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;correct_prediction&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Accuracy'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;#Print Accuracy&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Accuracy:&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'{0:.2&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;}'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;accuracy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;eval&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feed_dict&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_matrix_y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;})))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
</content>
 </entry>
 
 <entry>
   <title>Binary Prefixes</title>
   <link href="http://localhost:4000/binary-prefixes/"/>
   <updated>2018-01-05T00:00:00-05:00</updated>
   <id>http://localhost:4000/binary-prefixes</id>
   <content type="html">&lt;h2 id=&quot;decimal-prefixes&quot;&gt;Decimal Prefixes&lt;/h2&gt;
&lt;p&gt;When dealing with very large or very small amounts, it is common to append, or rather &lt;em&gt;pre&lt;/em&gt;pend, one of the SI (metric system) prefixes to whatever unit is being used. For example, the average human weighs $62000\text{g}$ but because that number is cumbersome to use, we usually append &lt;em&gt;kilo-&lt;/em&gt; (the SI prefix meaning a thousand) to the gram unit and say $62\text{kg}$ instead.&lt;/p&gt;

&lt;p&gt;This system works just fine for all sorts of units of measure like mass, length, energy, etc. But when it’s applied to units of information, specifically binary ones like bits and bytes, a problem arises…&lt;/p&gt;

&lt;!--more--&gt;

&lt;h2 id=&quot;binary-prefixes&quot;&gt;Binary Prefixes&lt;/h2&gt;
&lt;p&gt;You may have bought a hard drive or even a new phone and found that there is a discrepancy between the amount of storage shown on the package and the maximum amount displayed when you look in the settings.&lt;/p&gt;

&lt;p&gt;So what causes this disparity?&lt;/p&gt;

&lt;p&gt;The difference between a GB (gigabyte) and a GiB (gibibyte).&lt;/p&gt;

&lt;p&gt;Historically when a computer scientist wrote kB, for example, they didn’t mean $1000$ bytes. They meant $1024$ bytes. This is because computers operate in binary which is based of powers of two. In the kilobyte’s case, $2^{10} = 1024$ which is almost $1000$. This made it a close enough approximation of memory in a computer which was based on powers of $2$.&lt;/p&gt;

&lt;p&gt;This changed around 1998 when the International Electrotechnical Commission (IEC) and other regulatory organizations created a new set of prefixes to be used as binary prefixes. The US National Institute of Standards and Technology (NIST) followed suit and required that the SI prefixes only be used in the decimal sense. Below is a table of the metric prefixes vs the binary ones.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Decimal Prefix (SI)&lt;/th&gt;
      &lt;th&gt;Value&lt;/th&gt;
      &lt;th&gt;Value (1000)&lt;/th&gt;
      &lt;th&gt;Binary Prefix (IEC)&lt;/th&gt;
      &lt;th&gt;Value&lt;/th&gt;
      &lt;th&gt;Value (1024)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;kilo (k)&lt;/td&gt;
      &lt;td&gt;10&lt;sup&gt;3&lt;/sup&gt;&lt;/td&gt;
      &lt;td&gt;1000&lt;/td&gt;
      &lt;td&gt;kibi (ki)&lt;/td&gt;
      &lt;td&gt;2&lt;sup&gt;10&lt;/sup&gt;&lt;/td&gt;
      &lt;td&gt;1024&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;mega (M)&lt;/td&gt;
      &lt;td&gt;10&lt;sup&gt;6&lt;/sup&gt;&lt;/td&gt;
      &lt;td&gt;1000&lt;sup&gt;2&lt;/sup&gt;&lt;/td&gt;
      &lt;td&gt;mebi (Mi)&lt;/td&gt;
      &lt;td&gt;2&lt;sup&gt;20&lt;/sup&gt;&lt;/td&gt;
      &lt;td&gt;1024&lt;sup&gt;2&lt;/sup&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;giga (G)&lt;/td&gt;
      &lt;td&gt;10&lt;sup&gt;9&lt;/sup&gt;&lt;/td&gt;
      &lt;td&gt;1000&lt;sup&gt;3&lt;/sup&gt;&lt;/td&gt;
      &lt;td&gt;gibi (Gi)&lt;/td&gt;
      &lt;td&gt;2&lt;sup&gt;30&lt;/sup&gt;&lt;/td&gt;
      &lt;td&gt;1024&lt;sup&gt;3&lt;/sup&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;tera (T)&lt;/td&gt;
      &lt;td&gt;10&lt;sup&gt;12&lt;/sup&gt;&lt;/td&gt;
      &lt;td&gt;1000&lt;sup&gt;4&lt;/sup&gt;&lt;/td&gt;
      &lt;td&gt;tebi (Ti)&lt;/td&gt;
      &lt;td&gt;2&lt;sup&gt;40&lt;/sup&gt;&lt;/td&gt;
      &lt;td&gt;1024&lt;sup&gt;4&lt;/sup&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;peta (P)&lt;/td&gt;
      &lt;td&gt;10&lt;sup&gt;15&lt;/sup&gt;&lt;/td&gt;
      &lt;td&gt;1000&lt;sup&gt;5&lt;/sup&gt;&lt;/td&gt;
      &lt;td&gt;pebi (Pi)&lt;/td&gt;
      &lt;td&gt;2&lt;sup&gt;50&lt;/sup&gt;&lt;/td&gt;
      &lt;td&gt;1024&lt;sup&gt;5&lt;/sup&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;exa (E)&lt;/td&gt;
      &lt;td&gt;10&lt;sup&gt;18&lt;/sup&gt;&lt;/td&gt;
      &lt;td&gt;1000&lt;sup&gt;6&lt;/sup&gt;&lt;/td&gt;
      &lt;td&gt;exbi (Ei)&lt;/td&gt;
      &lt;td&gt;2&lt;sup&gt;60&lt;/sup&gt;&lt;/td&gt;
      &lt;td&gt;1024&lt;sup&gt;6&lt;/sup&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;zetta (Z)&lt;/td&gt;
      &lt;td&gt;10&lt;sup&gt;21&lt;/sup&gt;&lt;/td&gt;
      &lt;td&gt;1000&lt;sup&gt;7&lt;/sup&gt;&lt;/td&gt;
      &lt;td&gt;zebi (Zi)&lt;/td&gt;
      &lt;td&gt;2&lt;sup&gt;70&lt;/sup&gt;&lt;/td&gt;
      &lt;td&gt;1024&lt;sup&gt;7&lt;/sup&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;yotta (Y)&lt;/td&gt;
      &lt;td&gt;10&lt;sup&gt;24&lt;/sup&gt;&lt;/td&gt;
      &lt;td&gt;1000&lt;sup&gt;8&lt;/sup&gt;&lt;/td&gt;
      &lt;td&gt;yobi (Yi)&lt;/td&gt;
      &lt;td&gt;2&lt;sup&gt;80&lt;/sup&gt;&lt;/td&gt;
      &lt;td&gt;1024&lt;sup&gt;8&lt;/sup&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;So problem solved, right? Well no. Most people have never heard of a kibibyte (kiB), mebibyte (MiB), or gibibyte (Gib) and probably never will. Hardware manufacturers know this and, rather than deal with the consumer’s perception of information storage, opt to just use the closest decimal prefix.&lt;/p&gt;

&lt;p&gt;That said, there are a growing number of software and hardware applications that make use of binary prefixes.&lt;/p&gt;
</content>
 </entry>
 

</feed>