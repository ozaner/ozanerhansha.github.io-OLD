<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="https://www.w3.org/2005/Atom">

 <title>Ozaner</title>
 <link href="http://localhost:4000/atom.xml" rel="self"/>
 <link href="http://localhost:4000/"/>
 <updated>2018-08-28T12:06:58-04:00</updated>
 <id>http://localhost:4000</id>
 <author>
   <name>Ozaner Hansha</name>
   <email></email>
 </author>

 
 <entry>
   <title>Iverson Bracket</title>
   <link href="http://localhost:4000/iverson-bracket/"/>
   <updated>2018-08-27T00:00:00-04:00</updated>
   <id>http://localhost:4000/iverson-bracket</id>
   <content type="html">&lt;p&gt;The Iverson bracket, denoted $[P]$, is a function that evaluates to $1$ if the given proposition $P$ is true and $0$ if it’s false. That is to say:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
[P]={\begin{cases}1&amp;{P}\\0&amp;{\neg P}\end{cases}} %]]&gt;&lt;/script&gt;

&lt;p&gt;The Iverson bracket can also be used with predicates:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
[P(x)]={\begin{cases}1&amp;{P(x)}\\0&amp;{\neg P(x)}\end{cases}} %]]&gt;&lt;/script&gt;

&lt;p&gt;Below I give two, separate, formal definitions of the Iverson bracket. One for the propositional case and one for the predicate cases.&lt;/p&gt;

&lt;!--more--&gt;

&lt;h2 id=&quot;propositional-definition&quot;&gt;Propositional Definition&lt;/h2&gt;
&lt;p&gt;We can think of the notation $[P]$ as being shorthand for the following:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;[P]\equiv\bigcup\{x\in 2\mid(P\wedge x=1) \vee (\neg P\wedge x=0)\}&lt;/script&gt;

&lt;p&gt;&lt;em&gt;Where $2=\{0,1\}$ as per its construction in the &lt;a href=&quot;\natural-numbers&quot;&gt;natural numbers&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The set builder notation above will end up being $\{0\}$ or $\{1\}$ depending on whether $P$ is true. The arbitrary union, denoted $\bigcup$, outside of the set ‘unwraps’ the singleton set so that we are left with just $0$ or $1$.&lt;/p&gt;

&lt;p&gt;This allows us to truly equate $[P]$ with some value in the set-theoretic sense. Notice though, $[P]$ is not a function and is instead a shorthand for writing $0$ or $1$.&lt;/p&gt;

&lt;h2 id=&quot;predicate-definition&quot;&gt;Predicate Definition&lt;/h2&gt;
&lt;p&gt;While we could simply use the same definition above and replace all instances of $P$ with $P(x)$, it would be more useful to define $[P(x)]$ as a full blown set-theoretic function.&lt;/p&gt;

&lt;p&gt;And indeed, assuming $x$ is an element of some domain $S$ (i.e $x\in S$), we can do just that. However, to aid in defining $[P(x)]$  we’ll call it $Q(x)$ for the time being:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Q\equiv\{(x,y)\mid(P(x)\wedge y=1)\vee(\neg P(x)\wedge y=0)\}&lt;/script&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Proof of Function&lt;/strong&gt;&lt;/summary&gt;
We can see that $Q$ is a set of ordered pairs $(x,y)$ with $x\in S$ and $y\in 2$. This implies that $Q\subset S\times 2$. However, this only shows that $[P(x)]$ is a &lt;a href=&quot;\relations&quot;&gt;relation&lt;/a&gt;.
&lt;p&gt;&lt;/p&gt;

To show that $Q$ is a function, we must show that it is right-unique. This should be clear as for any given $x$, $y=0\oplus y=1$. This is because both $P(x)$ and $\neg P(x)$ can't be true due to the law of the excluded middle.
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt;

&lt;p&gt;And so $Q$ is a function $Q:S\to 2$. We can now say $Q(x)\equiv [P(x)]$, to return to our original notation.&lt;/p&gt;

&lt;h2 id=&quot;arithmetic-properties&quot;&gt;Arithmetic Properties&lt;/h2&gt;
&lt;p&gt;Below are some of the immediate properties of the Iverson bracket. We can think of these equalities as bridges between the first order logic notions of truth (true vs. false) and the arithmetic/computational notion of booleans ($0$ vs. $1$).&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$[\neg P]=1-[P]$&lt;/li&gt;
  &lt;li&gt;$[P\wedge Q]=[P][Q]$&lt;/li&gt;
  &lt;li&gt;$[P\vee Q]=[P]+[Q]-[P][Q]$&lt;/li&gt;
  &lt;li&gt;$[P\oplus Q]=([P]-[Q])^2$&lt;/li&gt;
  &lt;li&gt;$[P\rightarrow Q]=1-[P]-[Q]+[P][Q]$&lt;/li&gt;
  &lt;li&gt;$[P\equiv Q]=1-([P]-[Q])^2$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;The last three can be solved via substitution of the first three and normal propositional calculus.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;examples&quot;&gt;Examples&lt;/h2&gt;
&lt;p&gt;Iverson brackets are, in many cases, a more natural way to express certain conditional functions or properties.&lt;/p&gt;

&lt;details open=&quot;&quot;&gt;
&lt;summary&gt;&lt;strong&gt;Kronecker Delta&lt;/strong&gt;&lt;/summary&gt;
The Iverson bracket generalizes the Kronecker delta function $\delta_{ij}$ to any proposition $P$ rather than the specific proposition $i=j$. We can express the Kronecker delta function in terms of the Iverson bracket as so:

$$\delta_{ij}=[i=j]$$
&lt;/details&gt;

&lt;details open=&quot;&quot;&gt;
&lt;summary&gt;&lt;strong&gt;Trichotomy of $\mathbb{R}$&lt;/strong&gt;&lt;/summary&gt;
Iverson brackets afford a much simpler way to convey the property of trichotomy, or any list of mutually exclusive conditions.

$$[a&amp;lt;b]+[a=b]+[a&amp;gt;b]=1$$
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Sign Function&lt;/strong&gt;&lt;/summary&gt;
$$\operatorname{sgn}(x)=[x&amp;gt;0]-[x&amp;lt;0]$$
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Absolute Value&lt;/strong&gt;&lt;/summary&gt;
$$ {\begin{aligned}|x|&amp;amp;=x\cdot \operatorname {sgn}(x)\\&amp;amp;=x([x&amp;gt;0]-[x&amp;lt;0])\end{aligned}}$$
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Heaviside Step Function&lt;/strong&gt;&lt;/summary&gt;
$$H(x)=[x&amp;gt;0]$$
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Min and Max&lt;/strong&gt;&lt;/summary&gt;
$$\min(x,y)=x[x\leq y]+y[x&amp;gt;y]$$

$$\max(x,y)=x[x&amp;gt;y]+y[x\leq y]$$
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Ceiling and Floor&lt;/strong&gt;&lt;/summary&gt;
$$\lceil x\rceil =\sum _{n}n\cdot [n-1&amp;lt;x\leq n]$$

$$\lfloor x\rfloor =\sum _{n}n\cdot [n\leq x&amp;lt;n+1]$$
&lt;/details&gt;
</content>
 </entry>
 
 <entry>
   <title>Asymptotic Equivalence</title>
   <link href="http://localhost:4000/asymptotic-equivalence/"/>
   <updated>2018-08-26T00:00:00-04:00</updated>
   <id>http://localhost:4000/asymptotic-equivalence</id>
   <content type="html">&lt;p&gt;Asymptotic analysis is a way of describing the limiting behavior of functions. Two functions are &lt;strong&gt;asymptotically equivalent&lt;/strong&gt; if they have the same limiting behavior, in a sense they are &lt;em&gt;proportional at infinity&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;This boils down to whether or not the functions have the same &lt;em&gt;most significant term&lt;/em&gt;, that is the term that grows the quickest as $x$ increases. For example in the function $x^3+5x^2+x$ this role falls on the $x^3$ term as the other terms become insignificant as $x\to\infty$.&lt;/p&gt;

&lt;p&gt;Some important results in asymptotic analysis include the prime number theorem and Stirling’s approximation.&lt;/p&gt;

&lt;!--more--&gt;

&lt;h2 id=&quot;definition&quot;&gt;Definition&lt;/h2&gt;
&lt;p&gt;Asymptotic equivalence is an equivalence relation, denoted $\sim$, on the set of $\mathbb{R}$-valued functions (or $\mathbb{C}$, $\mathbb{Z}^+$, etc.) and is defined as such:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(x)\sim g(x)\equiv\lim_{x\to\infty}{\frac{f(x)}{g(x)}}=1&lt;/script&gt;

&lt;p&gt;The definition can also be extended to limits not just to infinity but any constant:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(x)\sim g(x) \ \ (\text{as }x\to c)\equiv\lim_{x\to c}{\frac{f(x)}{g(x)}}=1&lt;/script&gt;

&lt;p&gt;creating a family of equivalence relations, indexed by some real number $c$.&lt;/p&gt;

&lt;h4 id=&quot;examples&quot;&gt;Examples&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;$x^2\sim x^2+2x$ (most significant terms are same)&lt;/li&gt;
  &lt;li&gt;$4n!\sim 4n!+3n^2$ (most significant terms are same)&lt;/li&gt;
  &lt;li&gt;$2x^2\not\sim x^2$ (different constants on most significant terms)&lt;/li&gt;
  &lt;li&gt;$x^3\not\sim x^4$ (different most significant terms)&lt;/li&gt;
  &lt;li&gt;$n!\sim \sqrt{2\pi n}\left(\frac{n}{e}\right)^n$ (Stirling’s approximation)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;proof-of-equivalence-relation&quot;&gt;Proof of Equivalence Relation&lt;/h2&gt;
&lt;p&gt;To qualify as an equivalence relation, a given &lt;a href=&quot;\relations&quot;&gt;relation&lt;/a&gt; must satisfy the three properties given below:&lt;/p&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Reflexivity&lt;/strong&gt;&lt;/summary&gt;
Asymptotic equivalence is reflexive meaning that for all functions $f\sim f$. This is obvious as:

$$\forall f:\lim_{x\to\infty}{\frac{f(x)}{f(x)}}=1$$

&lt;i&gt;Assuming $f(x)$ doesn't approach $0$.&lt;/i&gt;
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Symmetry&lt;/strong&gt;&lt;/summary&gt;
Asymptotic equivalence is symmetric meaning that $f\sim g\implies g\sim f$. To prove this notice that the only term that &quot;survives&quot; after the limit is taken of both $f$ and $g$ is the most significant one:

$$\lim_{x\to\infty}{\frac{f(x)}{g(x)}}=\frac{\lim\limits_{x\to \infty}{f(x)}}{\lim\limits_{x\to \infty}{g(x)}}=\frac{f_s(x)}{g_s(x)}$$

&lt;i&gt;Where $f_s$ and $g_s$ represent the most significant terms of $f$ and $g$ respectively.&lt;/i&gt;

Notice that $f(x)\sim g(x)$ is the same as saying $f_s/g_s=1$. And so we can deduce the following:

$$\frac{f_s(x)}{g_s(x)}=1=\frac{g_s(x)}{f_s(x)}=\lim_{x\to\infty}{\frac{g(x)}{f(x)}}\equiv g(x)\sim f(x)$$

Thus we have proved $f\sim g\implies g\sim f$.
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Transitivity&lt;/strong&gt;&lt;/summary&gt;
I'll do this later, but this should be clear as asymptotic equivalence forms equivalence classes characterized by the most significant terms of functions.
&lt;/details&gt;

&lt;h2 id=&quot;relation-to-algorithmic-complexity&quot;&gt;Relation to Algorithmic Complexity&lt;/h2&gt;
&lt;p&gt;When dealing with the time and space complexity of algorithms, asymptotic analysis (either in the form defined above or via &lt;strong&gt;Bachmann–Landau notation&lt;/strong&gt;) is used to compare the efficiency of different algorithms by stripping away unnecessary constants and minor terms that correspond to the different preconditions and runtime environments that these algorithms may run on.&lt;/p&gt;

&lt;p&gt;In particular, the limit definitions of the Bachmann–Landau notation are very similar to that of asymptotic equivalence. In fact, we can phrase Big $O$, $\Omega$, and $\Theta$ notation in terms of asymptotic growth.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(x)\in O(g(x))\equiv \exists c\not=0:cg(x)\text{ grows asymptotically faster than } f(x)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(x)\in \Omega(g(x))\equiv \exists c\not=0:cg(x)\text{ grows asymptotically slower than } f(x)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(x)\in \Omega(g(x))\equiv \exists c\not=0:cg(x) \sim f(x)&lt;/script&gt;
</content>
 </entry>
 
 <entry>
   <title>Proportionality</title>
   <link href="http://localhost:4000/proportionality/"/>
   <updated>2018-08-17T00:00:00-04:00</updated>
   <id>http://localhost:4000/proportionality</id>
   <content type="html">&lt;h2 id=&quot;a-review-of-proportionality&quot;&gt;A Review of Proportionality&lt;/h2&gt;
&lt;p&gt;Recall that two expressions, which we can consider &lt;strong&gt;functions&lt;/strong&gt;, are proportional if there exists some constant that when multiplied by one of the functions makes them equivalent. This relationship is usually denoted $y\propto x$ or equivalently as $y=kx$ where $k$ is some &lt;strong&gt;proportionality constant&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;For example, a circle’s circumference $C$ is proportional to it’s diameter $d$. We can write this as $C\propto d$ and even further, with the knowledge that the constant between them is $\pi$, we can write $C=\pi d$&lt;/p&gt;

&lt;!--more--&gt;

&lt;h4 id=&quot;finer-points-and-motivation&quot;&gt;Finer Points and Motivation&lt;/h4&gt;
&lt;p&gt;Notice that proportionality satisfies the three criterion for an equivalence relation: reflexivity, symmetry, and transitivity (we’ll prove this later). By imagining proportionality as a relation (or as we’ll see a family of relations) on the set of real/complex valued functions, we can begin to formally define it.&lt;/p&gt;

&lt;h2 id=&quot;definition-on-mathbbr-valued-functions&quot;&gt;Definition on $\mathbb{R}$-Valued Functions&lt;/h2&gt;
&lt;p&gt;Proportionality is a family of equivalence relations on the set(s) of real valued functions (i.e the set of functions $f:S\to\mathbb{R}$ where $S$ is an arbitrary set):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\propto_{\mathbb{R}^S}\equiv\{(x,y)\in(\mathbb{R}^S)^2\mid\exists k\in \mathbb{R}:kx=y\}&lt;/script&gt;

&lt;p&gt;And because $x\propto_{\mathbb{R}^S}y$ is shorthand for $(x,y)\in\propto_{\mathbb{R}^S}$ we can say:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x\propto_{\mathbb{R}^S}y\equiv(\exists k\in \mathbb{R})\ kx=y&lt;/script&gt;

&lt;p&gt;Notice that the set of functions from $S$ to $\mathbb{R}$ forms a vector space with its underlying field being the real numbers. Every choice of $S$ produces a different proportionality relation, hence it being a family.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;The notation $\propto_{\mathbb{R}^S}$ was chosen instead of $\propto_S$ in order to match the more general definition of proportionality given below.&lt;/em&gt;&lt;/p&gt;

&lt;h4 id=&quot;other-number-systems&quot;&gt;Other Number Systems&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;$\mathbb{C,Q}$: Note that the above definition works just as well with both complex and rational valued functions and scalars. This is because, like the real numbers, they both form fields with inverse elements.&lt;/li&gt;
  &lt;li&gt;$\mathbb{Z,N}$: Unlike the complex, real, and rational numbers, the integers and natural numbers do not form fields because they do not have inverses under multiplication. As such, proportionality loses the reflexive property because the constant $k^{-1}$ does not exist for any integer. Proportionality can still be used, just not as an equivalence relation.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;generalized-definition&quot;&gt;Generalized Definition&lt;/h2&gt;
&lt;p&gt;We can generalize this definition to not just the set of real-valued functions, but any vector space. Given a vector space with vectors $V$, an underlying field $F$, and a valid scalar multiplication $\cdot$, we can define the following equivalence relation:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\propto_V\equiv\{(\mathbf{x},\mathbf{y})\in V^2\mid\exists k\in F:k\mathbf{x}=\mathbf{y}\}&lt;/script&gt;

&lt;p&gt;And because $x\propto_Vy$ is shorthand for $(x,y)\in\propto_V$ we can say:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{x}\propto_V \mathbf{y}\equiv(\exists k\in F)\ k\mathbf{x}=\mathbf{y}&lt;/script&gt;

&lt;h4 id=&quot;notation&quot;&gt;Notation&lt;/h4&gt;
&lt;p&gt;When the context is clear, we can omit the subscript denoteing what vector space the relation is acting upon and just write $\propto$. This is usually the case when relating two functions, real valued or otherwise, since proportionality is most commonly used in fields like physics where the domain of discourse is very clear.&lt;/p&gt;

&lt;h2 id=&quot;proof-of-equivalence-relation&quot;&gt;Proof of Equivalence Relation&lt;/h2&gt;
&lt;p&gt;Using the general definition above, we will prove $\propto$ forms an equivalence relation by showing it has the reflexive, symmetric, and transitive properties.&lt;/p&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Reflexivity&lt;/strong&gt;&lt;/summary&gt;
Proportionality is reflexive, i.e $\mathbf{v}\propto \mathbf{v}$ for all $\mathbf{v}\in V$, because every vector space has an identity scalar that when multiplied by any of its vectors, produces that same vector.

$$(\exists e\in F)(\forall \mathbf{v}\in V)\ e\mathbf{v}=\mathbf{v}$$

This is enough to prove reflexivity because if $\mathbf{x}=\mathbf{y}$ then the definition becomes:

$$\mathbf{x}\propto_V \mathbf{x}\equiv(\exists k\in F)\ k\mathbf{x}=\mathbf{x}$$

and there will always exist a $k\in F$, that satisfies $k\mathbf{x}=\mathbf{x}$ and it will always be the identity scalar $e$ (aka the number $1$ in the case of the complex/real/etc. numbers).
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Symmetry&lt;/strong&gt;&lt;/summary&gt;
Proportionality is symmetric, i.e $\mathbf{u}\propto \mathbf{v}\implies \mathbf{v}\propto \mathbf{u}$ for all $\mathbf{u},\mathbf{v}\in V$, because every scalar in a vector space has a multiplicative inverse:

$$(\forall s\in F)(\exists s^{-1}\in F)\ ss^{-1}=e$$

&lt;i&gt;Where $e$ is the identity scalar. Also note that a field's multiplication is commutative.&lt;/i&gt;&lt;p&gt;&lt;/p&gt;

So assuming two vectors are proportional $\mathbf{x}\propto_V \mathbf{y}$ there must exist some $k\in F$ that satisfies $k\mathbf{x}=\mathbf{y}$. If we multiply both sides by $k^{-1}$, which too must exist as we've stated, we are left with:

$$\begin{align}
k\mathbf{x}&amp;amp;=\mathbf{y}\\
 k^{-1}k\mathbf{x}&amp;amp;=k^{-1}\mathbf{y}\\
e\mathbf{x}&amp;amp;=k^{-1}\mathbf{y}\\
\mathbf{x}&amp;amp;=k^{-1}\mathbf{y}
\end{align}$$

Notice that, because $k^{-1}\in F$, this is precisely the definition of $\mathbf{y}\propto \mathbf{x}$. And so we have shown that for any two vectors in $V$ that if $\mathbf{x}\propto \mathbf{y}$ then $\mathbf{y}\propto \mathbf{x}$. And that if the proportionality constant of $\mathbf{x}\propto \mathbf{y}$ is $k$ then the constant between $\mathbf{y}\propto \mathbf{x}$ is $k^{-1}$.
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Transitivity&lt;/strong&gt;&lt;/summary&gt;
The last property to prove is transitivity. Proportionality being transitive means that $\mathbb{u}\propto\mathbb{v}\wedge\mathbb{v}\propto\mathbb{w}\implies\mathbb{u}\propto\mathbb{w}$. This can be shown by noting:

$$\mathbb{u}\propto\mathbb{v}\iff k_1\mathbb{u}=\mathbb{v}$$

$$\mathbb{v}\propto\mathbb{w}\iff k_2\mathbb{v}=\mathbb{w}$$

So if we have $\mathbb{u}\propto\mathbb{v}\wedge\mathbb{v}\propto\mathbb{w}$ then we also know there exists two constants $k_1$ and $k_2$ that correspond to each. Now we can simply substitute $k_1\mathbb{u}$ in for $\mathbb{v}$ in the following way:

$$\begin{align}
k_1\mathbb{u}&amp;amp;=\mathbb{v}\\
k_2\mathbb{v}&amp;amp;=\mathbb{w}\\
k_2k_1\mathbb{u}&amp;amp;=\mathbb{w}\\
\end{align}$$

Because field multiplication is associative, the scalar $k_2k_1$ is the proportionality constant between $\mathbb{u}$ and $\mathbb{w}$ implying that $\mathbb{u}\propto\mathbb{w}$. And thus we have proved the transitivity of the proportional relation.
&lt;/details&gt;
</content>
 </entry>
 
 <entry>
   <title>Relations</title>
   <link href="http://localhost:4000/relations/"/>
   <updated>2018-08-12T00:00:00-04:00</updated>
   <id>http://localhost:4000/relations</id>
   <content type="html">&lt;!-- ## What is a Relation?
Relations are used to correlate elements of different sets in some way. They are a more general form of a function, which can only relate elements to a single output.

#### Example
Say I had a set of tops $T$, pants $P$, and shoes $S$. I could define another set $R$ that is a collection of all the combinations of clothing that go together (a set of ordered triplets). This set would be called a relation, because it *relates* pieces of clothing that go well together by virtue of that combination being an element of $R$. --&gt;

&lt;h2 id=&quot;definition&quot;&gt;Definition&lt;/h2&gt;
&lt;p&gt;A &lt;strong&gt;relation&lt;/strong&gt; $R$ on a set of sets $S_1,S_2,S_3,\cdots$ is a subset of their &lt;a href=&quot;/cartesian-product&quot;&gt;&lt;strong&gt;cartesian product&lt;/strong&gt;&lt;/a&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;R\subseteq S_1\times S_2\times S_3\times \cdots&lt;/script&gt;

&lt;p&gt;Relations are used to establish $n$-way relationships between elements of the same or different sets (ex. there is a two way relationship between $2$ and $5$ in that $2&amp;lt;5$).&lt;/p&gt;

&lt;h4 id=&quot;arity&quot;&gt;Arity&lt;/h4&gt;
&lt;p&gt;The arity of a relation is the number of sets it is a relation on (i.e the &lt;a href=&quot;/cartesian-product#arity&quot;&gt;arity of the cartesian product&lt;/a&gt; it is a subset of). Thus, a relation on $2$ sets is a &lt;em&gt;bi&lt;/em&gt;nary relation, $3$ sets makes a &lt;em&gt;tri&lt;/em&gt;nary relation and, in general, $n$ sets make an $n$-ary relation.&lt;/p&gt;

&lt;p&gt;When $n$ is infinite the relation is &lt;strong&gt;infinitary&lt;/strong&gt; as opposed to &lt;strong&gt;finitary&lt;/strong&gt; for finite $n$.&lt;/p&gt;

&lt;!--more--&gt;

&lt;h4 id=&quot;existence-in-zfc&quot;&gt;Existence in ZFC&lt;/h4&gt;
&lt;p&gt;We &lt;a href=&quot;/cartesian-product#existence-in-zfc&quot;&gt;proved that cartesian products existed&lt;/a&gt; in another post and so, because relations are subsets of cartesian products, we only need to invoke the axiom of subset to prove their existence.&lt;/p&gt;

&lt;h2 id=&quot;notation&quot;&gt;Notation&lt;/h2&gt;
&lt;p&gt;Relations are common mathematical objects and thus have special notation regarding their use:&lt;/p&gt;

&lt;h4 id=&quot;n-ary-relations&quot;&gt;$n$-ary Relations&lt;/h4&gt;
&lt;p&gt;For $n$-ary relations it is common to write:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Rabcd\cdots \equiv (a,b,c,d,\cdots)\in R&lt;/script&gt;

&lt;p&gt;where $R$ is the relation in question and $(a,b,c,d,\cdots)$ are elements of the cartesian product $R$ is a subset of.&lt;/p&gt;

&lt;h4 id=&quot;binary-relations&quot;&gt;Binary Relations&lt;/h4&gt;
&lt;p&gt;While you can write $Rab$ for binary relations, it is more common to write:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;aRb\equiv (a,b)\in R&lt;/script&gt;

&lt;p&gt;This notation is used for almost all common relations like $5&amp;gt;3$ or $ABC \cong ADC$.&lt;/p&gt;

&lt;h2 id=&quot;properties-of-binary-relations&quot;&gt;Properties of Binary Relations&lt;/h2&gt;
&lt;p&gt;The most common type of relations are binary. This is in part due to the fact that many higher arity relations can be broken up into a collection of binary relations.&lt;/p&gt;

&lt;!-- This is in part because many relations can be [*curried*](https://en.wikipedia.org/wiki/Currying) into several different binary relations --&gt;

&lt;p&gt;A binary relation on two different sets $A$ and $B$ is called &lt;em&gt;heterogenous&lt;/em&gt; when $A\not=B$ and &lt;em&gt;homogenous&lt;/em&gt; when $A=B$. Homogenous relations are also called &lt;strong&gt;relations on a set&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Below is a list of common classifications of binary relations based on what properties they satisfy. In all the definitions below, we will assume $R\subset A\times B$ and $a\in A, b\in B$:&lt;/p&gt;

&lt;h3 id=&quot;uniqueness-properties&quot;&gt;Uniqueness Properties&lt;/h3&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Injective&lt;/strong&gt;&lt;/summary&gt;
A relation is called injective if for all $b$ on the right side, the $a$ on the left side is unique in the expression $aRb$. More formally, $R$ is injective if:

$$(\forall a_1,a_2\in A, \forall b\in B)\ a_1Rb\wedge a_2Rb\implies a_1=a_2$$

Because of this property, injective relations are also called &lt;b&gt;left-unique&lt;/b&gt;.
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Functional&lt;/strong&gt;&lt;/summary&gt;
A relation is functional if for all $a$ on the left side, the $b$ on the right is unique. Relations that fulfill this property are called &lt;b&gt;functions&lt;/b&gt; and are said to have a unique output $b$ for a given input $a$. Formally this means:

$$(\forall a\in A, \forall b_1,b_2\in B)\ aRb_1\wedge aRb_2\implies b_1=b_2$$

Similar to injective relations, a functional relation is also called &lt;b&gt;right-unique&lt;/b&gt;.
&lt;/details&gt;

&lt;!-- &lt;details&gt;
&lt;summary&gt;&lt;strong&gt;One-to-One&lt;/strong&gt;&lt;/summary&gt;
One-to-One functions are relations that are both functional and injective. These functions map every element in their domain to a unique element in the range. These are also called &lt;b&gt;injective functions&lt;/b&gt; because being functional is implied in the classification 'function'.
&lt;/details&gt; --&gt;

&lt;h3 id=&quot;totality-properties&quot;&gt;Totality Properties&lt;/h3&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Left-Total&lt;/strong&gt;&lt;/summary&gt;
A left-total relation means that for every element in $A$ there is at least one element in $B$ that it is related to:

$$(\forall a\in A,\exists b\in B)\ aRb$$

Note that all functions are automatically left-total, but not all left-total relations are functions.
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Surjective&lt;/strong&gt;&lt;/summary&gt;
A relation is surjective every element in $B$ is related to at least one element in $A$:

$$(\exists a\in A,\forall b\in B)\ aRb$$

Similar to left-total relations, surjective relations are also called &lt;b&gt;right-total&lt;/b&gt;. When a function is both injective and surjective it forms a bijection.
&lt;/details&gt;

&lt;h3 id=&quot;other-properties&quot;&gt;Other Properties&lt;/h3&gt;
&lt;p&gt;The following properties only apply to homogenous relations (i.e $R\subset X\times X$).&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Reflexive and Irreflexive&lt;/strong&gt;&lt;/summary&gt;
A relation is reflexive if all elements relate to themselves:

$$(\forall x\in X)\ xRx$$

Some example of this are the less than or equal to $\le$ and the divides $\mid$ relations. Relations that don't relate &lt;i&gt;any&lt;/i&gt; element to themselves are called &lt;b&gt;irreflexive&lt;/b&gt;:

$$(\forall x\in X)\ \neg(xRx)$$

An example of this is the $lt$ relation.
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Symmetric&lt;/strong&gt;&lt;/summary&gt;
A relation is symmetric if $xRy$ means $yRx$ as well:

$$(\forall x,y\in X)\ xRy \implies yRx$$

Some examples of this include the [proportionality](/proportionality) $\propto$ of functions and similarity $\sim$ of geometric objects.
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Anti-Symmetric&lt;/strong&gt;&lt;/summary&gt;
A relation is anti-symmetric if when $xRy$ and $yRx$ are both true, then $x=y$:

$$(\forall x,y\in X)\ xRy\wedge yRx \implies x=y$$

Some examples of this include the $\le$ relation,  

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Transitive&lt;/strong&gt;&lt;/summary&gt;
A relation is transitive if $xRy$ and $yRz$ means $xRz$:

$$(\forall x,y,z\in X)\ xRy \wedge yRz\implies xRz$$

Relations like $\le$ and similarity $\sim$ are transitive.
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Trichotomous&lt;/strong&gt;&lt;/summary&gt;
A relation is trichotomus if for any two element in $X$ either $xRy$, $yRx$, or $x=y$ holds. But only 1 of those three options:

$$(\forall x,y\in X)\ (xRy \oplus yRx \oplus x=y) \wedge \neg(xRy \wedge yRx \wedge x=y)$$

The most common example of this is as a property of the real numbers under the $\lt$ or $\gt$ relations. In other words, any real number is either greater than, lesser than, &lt;i&gt;xor&lt;/i&gt; equal to any other number.
&lt;/details&gt;

&lt;h2 id=&quot;calculus-of-relations&quot;&gt;Calculus of Relations&lt;/h2&gt;
&lt;p&gt;This is a list of different operations and transformations on one or more sets.&lt;/p&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Union and Intersection&lt;/strong&gt;&lt;/summary&gt;
The union of two relations that are on the same sets is equivalent to 'adding/or-ing' them together. For example, the union of the $\lt$ and the $=$ relations is $\le$. Intersection has similar behavior except it displays an 'and' effect.
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Complement&lt;/strong&gt;&lt;/summary&gt;
The complement of a relation $R\subset A\times B$ is simply all the of the ordered pairs in $A\times B$ that are &lt;i&gt;not&lt;/i&gt; in $R$. We can think of this as the ordinary complement of the relation denoted $R^\complement$ where $A\times B$ is the universal set:

$$R^\complement=\{(a,b)\in A\times B\mid (a,b)\not\in R\}$$

For example, the complement of less than or equal to is greater than: $\le^\complement=\gt$. The complement of &quot;is a parent of&quot; is &quot;is not a parent of&quot;.
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Logical Negation of a Relation&lt;/strong&gt;&lt;/summary&gt;
Notice that negating a relation is equivalent to asserting its complement (assuming both elements being related are in the universal set). Put more formally, if $a\in A$ and $b\in B$:

$$\neg(aRb)\equiv aR^\complement b$$

It is this negating property that allows us to replace statements like $\neg(a\le b)$ with  $a\gt b$.
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Composition&lt;/strong&gt;&lt;/summary&gt;
The composition of two relations $R\subset A\times B$ and $S\subset B\times C$ is a sort of 'product' denoted $S\circ R\subset A\times C$:

$$S\circ R=\{(a,c)\in A\times C\mid\exists b:(a,b)\in R\wedge(b,c)\in S\}$$

&lt;i&gt;Notice that relation composition is associative: $X\circ (Y\circ Z)=(X\circ Y)\circ Z$&lt;/i&gt;&lt;p&gt;&lt;/p&gt;

An intuitive example of this can be found in kinship relations. The composition &quot;is parent of&quot; $\circ$ &quot;is father of&quot; returns the new relation &quot;is grandfather of&quot;.
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Converse&lt;/strong&gt;&lt;/summary&gt;
The converse of a relation $R$ is denoted $R^\top$ and is simply the opposite of $R$. In other words, if $(x,y)$ is in a relation then $(y,x)$ is in its converse. Formally, for a relation $R\subset A\circ B$:

$$R^\top=\{(b,a)\in B\times A\mid (a,b)\in R\}$$

&lt;i&gt;Note that this means that the converse is idempotent: $(R^\top)^\top=R$. It also respects composition: $(R\circ L)^\top=L^\top\circ R^\top$.&lt;/i&gt;&lt;p&gt;&lt;/p&gt;

For example, the converse of the greater than relation $\ge^\top$ is $\le$. Similarly, the converse of the relation &quot;is a child of&quot; is &quot;is a parent of&quot;.
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Inverse&lt;/strong&gt;&lt;/summary&gt;
Some relations are &lt;i&gt;invertible&lt;/i&gt; meaning that for a relation $R$ there exists another relation $X$ such that $X\circ R=I$, called &lt;b&gt;left-invertible&lt;/b&gt;, or if there exists a relation $Y$ such that $R\circ Y=I$, called &lt;b&gt;right-invertible&lt;/b&gt;. When both the left and right inverses concincide, the inverse $R^{-1}$ is simply equivalent to the converse $R^\top$. &lt;p&gt;&lt;/p&gt;

For example, the composition &quot;is child of&quot; $\circ$ &quot;is parent of&quot; returns the identity relation (i.e &quot;is you&quot;). As such, they are inverses of each other.
&lt;/details&gt;
</content>
 </entry>
 
 <entry>
   <title>Boolean Logic in Linear Algebra</title>
   <link href="http://localhost:4000/boolean-logic-in-linear-algebra/"/>
   <updated>2018-07-18T00:00:00-04:00</updated>
   <id>http://localhost:4000/boolean-logic-in-linear-algebra</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Computation is usually formulated via boolean logic, which in turn makes use of logical connectives like $\wedge,\vee, \neg$ along with the binary digits $0$ and $1$, representing false and true respectively.&lt;/p&gt;

&lt;p&gt;There is, however, an alternative: linear algebra. By representing $0$ and $1$ as vectors and logical operations/gates as matrices, we can define computation in the language of linear algebra.&lt;/p&gt;

&lt;p&gt;While at first this may seem to be nothing but a novel construction, reformulating computation in terms of linear algebra is precisely what opens the door to quantum computing, which is essentially a more general form of what’s shown below.&lt;/p&gt;

&lt;!--more--&gt;

&lt;h2 id=&quot;0-and-1-states&quot;&gt;0 and 1 states&lt;/h2&gt;
&lt;p&gt;We represent $0$ and $1$ as two orthonormal vectors:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;|0\rangle = \begin{pmatrix}
                  1 \\
                  0 \\
                \end{pmatrix}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;|1\rangle = \begin{pmatrix}
                  0 \\
                  1 \\
                \end{pmatrix}&lt;/script&gt;

&lt;p&gt;&lt;em&gt;Notice my use of bra-ket notation above. This is the standard notation used in quantum mechanics, and thus quantum computing. As such, I’ll be using it here for consistency as well as to denote the difference between the state $|0\rangle$ and the number $0$.&lt;/em&gt;&lt;/p&gt;

&lt;h4 id=&quot;an-example&quot;&gt;An Example&lt;/h4&gt;
&lt;p&gt;As an example, let’s use the &lt;code class=&quot;highlighter-rouge&quot;&gt;NOT&lt;/code&gt; operator on the $|1\rangle$ bit:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{pmatrix}
    0 &amp; 1 \\
    1 &amp; 0
  \end{pmatrix}
  \begin{pmatrix}
    0 \\
    1 \\
  \end{pmatrix}=
  \begin{pmatrix}
    1 \\
    0 \\
  \end{pmatrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;or in symbolic notation, where $X$ represents the &lt;code class=&quot;highlighter-rouge&quot;&gt;NOT&lt;/code&gt; gate:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;X|1\rangle=|0\rangle&lt;/script&gt;

&lt;h2 id=&quot;unary-gates&quot;&gt;Unary Gates&lt;/h2&gt;
&lt;p&gt;Since a bit has only $2$ possible input states and $2$ possible output states, there are exactly $2^2=4$ unary bit operations. We can represent them as matrices in the following way:&lt;/p&gt;

&lt;!-- #### Identity --&gt;
&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Identity&lt;/strong&gt;&lt;/summary&gt;
The identity gate takes an input and returns it as is (i.e $f(x)=x$). To represent it we simply use the $2 \times 2$ identity matrix:

$$
  I_2 = \begin{pmatrix}
    1 &amp;amp; 0 \\
    0 &amp;amp; 1
  \end{pmatrix}
$$

And indeed, if we apply the gate to both $|0\rangle$ and $|1\rangle$ we find:

$$I_2|0\rangle=|0\rangle$$

$$I_2|1\rangle=|1\rangle$$
&lt;/details&gt;

&lt;!-- #### Negation --&gt;
&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Negation (NOT)&lt;/strong&gt;&lt;/summary&gt;
The negation gate takes an input and flips it (i.e $f(x)=\neg x$). We can represent it with the following matrix:

$$
  X = \begin{pmatrix}
    0 &amp;amp; 1 \\
    1 &amp;amp; 0
  \end{pmatrix}
$$

Applying the gate to both $|0\rangle$ and $|1\rangle$ we find:

$$X|0\rangle=|1\rangle$$

$$X|1\rangle=|0\rangle$$
&lt;/details&gt;

&lt;!-- #### Constant 0 --&gt;
&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Constant 0&lt;/strong&gt;&lt;/summary&gt;
Outputs $0$ regardless of input (i.e $f(x)=0$). We can represent it with the following matrix:

$$
  C_0 = \begin{pmatrix}
    1 &amp;amp; 1 \\
    0 &amp;amp; 0
  \end{pmatrix}
$$

Applying the gate to both $|0\rangle$ and $|1\rangle$ we find:

$$C_0|0\rangle=|0\rangle$$

$$C_0|1\rangle=|0\rangle$$
&lt;/details&gt;

&lt;!-- #### Constant 1 --&gt;
&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Constant 1&lt;/strong&gt;&lt;/summary&gt;
Outputs $1$ regardless of input (i.e $f(x)=1$). We can represent it with the following matrix:

$$
  C_1 = \begin{pmatrix}
    0 &amp;amp; 0 \\
    1 &amp;amp; 1
  \end{pmatrix}
$$

Applying the gate to both $|0\rangle$ and $|1\rangle$ we find:

$$C_1|0\rangle=|1\rangle$$

$$C_1|1\rangle=|1\rangle$$
&lt;/details&gt;

&lt;h2 id=&quot;multiple-bits&quot;&gt;Multiple Bits&lt;/h2&gt;
&lt;p&gt;Before we can introduce the matrices that correspond to binary operations, we need a way of representing multiple bits as a single vector. This is achieved via the &lt;strong&gt;tensor product&lt;/strong&gt; of the $|0\rangle$ and $|1\rangle$ vectors. For example, the two bit state $|10\rangle$ can be constructed as follows:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{pmatrix}
    0 \\
    1 \\
  \end{pmatrix}
  \otimes
  \begin{pmatrix}
    1 \\
    0 \\
  \end{pmatrix}=
  \begin{pmatrix}
    0\begin{pmatrix}
      1 \\
      0 \\
    \end{pmatrix} \\
    1\begin{pmatrix}
      1 \\
      0 \\
    \end{pmatrix} \\
  \end{pmatrix}=
  \begin{pmatrix}
    0 \\
    0 \\
    1 \\
    0 \\
  \end{pmatrix}&lt;/script&gt;

&lt;p&gt;It can also be written out symbolically as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;|1\rangle\otimes|0\rangle=|10\rangle&lt;/script&gt;

&lt;p&gt;Notice that $10_2=2$ and that the only $1$ in the $|10\rangle$ vector is at the $2$nd index (indexing starts at $0$). Indeed, it holds true in general that the vector representation of $n$ bits will be a $2^n$ dimensional vector with $0$’s everywhere except for a $1$ at the $i$th index, where $i$ is the value of the bit string:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;|a\rangle\otimes|b\rangle\otimes|c\rangle\otimes\cdots=|abc\cdots\rangle&lt;/script&gt;

&lt;!-- *As a side note, the tensor product of two rank $2$ tensors (vectors) $a$ and $b$ is equivalent to $ab^\top$.* --&gt;

&lt;h2 id=&quot;binary-gates&quot;&gt;Binary Gates&lt;/h2&gt;
&lt;p&gt;Now that we can represent bit strings as vectors, it is possible construct operations that take in $2$ bits and output $1$ bit. While there are technically $2^{2^2}=16$ possible binary bit operations, we’ll only construct the more useful ones like &lt;code class=&quot;highlighter-rouge&quot;&gt;AND&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;OR&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;XOR&lt;/code&gt;, etc.&lt;/p&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;OR&lt;/strong&gt;&lt;/summary&gt;
The &lt;code&gt;OR&lt;/code&gt; gate represents logical disjunction (i.e $f(x,y)=x\vee y$) and is represented by the following matrix:

$$
  \text{OR} = \begin{pmatrix}
    1 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\
    0 &amp;amp; 1 &amp;amp; 1 &amp;amp; 1
  \end{pmatrix}
$$

Applying the gate to all two bit states we find:

$$\text{OR}|00\rangle=|0\rangle$$

$$\text{OR}|01\rangle=|1\rangle$$

$$\text{OR}|10\rangle=|1\rangle$$

$$\text{OR}|11\rangle=|1\rangle$$
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;AND&lt;/strong&gt;&lt;/summary&gt;
The &lt;code&gt;AND&lt;/code&gt; gate represents logical conjunction (i.e $f(x,y)=x\wedge y$) and is represented by the following matrix:

$$
  \text{AND} = \begin{pmatrix}
    1 &amp;amp; 1 &amp;amp; 1 &amp;amp; 0 \\
    0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1
  \end{pmatrix}
$$

Applying the gate to all two bit states we find:

$$\text{AND}|00\rangle=|0\rangle$$

$$\text{AND}|01\rangle=|0\rangle$$

$$\text{AND}|10\rangle=|0\rangle$$

$$\text{AND}|11\rangle=|1\rangle$$
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;XOR&lt;/strong&gt;&lt;/summary&gt;
The &lt;code&gt;XOR&lt;/code&gt; gate represents exclusive disjunction (i.e $f(x,y)=x\oplus y$) and is represented by the following matrix:

$$
  \text{XOR} = \begin{pmatrix}
    1 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1 \\
    0 &amp;amp; 1 &amp;amp; 1 &amp;amp; 0
  \end{pmatrix}
$$

Applying the gate to all two bit states we find:

$$\text{XOR}|00\rangle=|0\rangle$$

$$\text{XOR}|01\rangle=|1\rangle$$

$$\text{XOR}|10\rangle=|1\rangle$$

$$\text{XOR}|11\rangle=|0\rangle$$
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Equality (XNOR)&lt;/strong&gt;&lt;/summary&gt;
Equality checks if two bits are equivalent (i.e $f(x,y)=x\iff y$) and is represented by the following matrix:

$$
  \text{IFF} = \begin{pmatrix}
    1 &amp;amp; 1 &amp;amp; 1 &amp;amp; 0 \\
    0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1
  \end{pmatrix}
$$

&lt;i&gt;Notice that equality is equivalent to the negation of XOR, &lt;code&gt;XNOR&lt;/code&gt; (i.e $x\leftrightarrow y=\neg(x\oplus y)$) meaning all the $0$'s and $1$'s in the &lt;code&gt;XOR&lt;/code&gt; matrix are simply swapped to form the equality one.&lt;/i&gt;

Applying the gate to all two bit states we find:

$$\text{IFF}|00\rangle=|1\rangle$$

$$\text{IFF}|01\rangle=|0\rangle$$

$$\text{IFF}|10\rangle=|0\rangle$$

$$\text{IFF}|11\rangle=|1\rangle$$
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Implication (IF)&lt;/strong&gt;&lt;/summary&gt;
Material implication is a statement of one variable's dependence on another (i.e $f(x,y)=x\implies y$). It's more commonly referred to as an &lt;code&gt;IF&lt;/code&gt; statement in computer science.

$$
  \text{IF} = \begin{pmatrix}
    0 &amp;amp; 1 &amp;amp; 0 &amp;amp; 0 \\
    1 &amp;amp; 0 &amp;amp; 1 &amp;amp; 1
  \end{pmatrix}
$$

Applying the gate to all two bit states we find:

$$\text{IF}|00\rangle=|1\rangle$$

$$\text{IF}|01\rangle=|0\rangle$$

$$\text{IF}|10\rangle=|1\rangle$$

$$\text{IF}|11\rangle=|1\rangle$$
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt;

&lt;p&gt;You’ll notice that the $i$th column of a binary gate matrix is either the $|0\rangle$ or $|1\rangle$ vector depending on what the gate outputs given the binary form of $i$ (ex. the $2$nd index corresponds to the input $10_2$). Since these gates only have $2^2=4$ possible inputs, there are $4$ corresponding columns in their matrix representation.&lt;/p&gt;

&lt;h2 id=&quot;putting-it-together&quot;&gt;Putting it Together&lt;/h2&gt;
&lt;p&gt;We can now appreciate this new formulation of binary computation for what it is, an isomorphism from booleans to vectors, logical connectives to matrices, and function composition to matrix multiplication.&lt;/p&gt;

&lt;p&gt;Here’s an example, the half-adder:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/comp_sci/half_adder.png?style=centerme&quot; alt=&quot;half-adder&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We can represent this algebraically as $S=X\oplus Y$ and $C=X\wedge Y$, where $S$ is the &lt;em&gt;sum&lt;/em&gt; bit and $C$ is the &lt;em&gt;carry&lt;/em&gt; bit.&lt;/p&gt;

&lt;p&gt;Remember, however, that a bit string is represented as the tensor product of its component bits. As such, we must find the tensor product of the matrices represented by the logical operations:&lt;/p&gt;

&lt;!-- $$
  S=\text{XOR}(X\otimes Y)
$$

$$
  C=\text{AND}(X\otimes Y)
$$ --&gt;

&lt;!-- &lt;details&gt;
&lt;summary&gt;Explicit Form&lt;/summary&gt;

$$
S=\begin{pmatrix}
  1 &amp; 0 &amp; 0 &amp; 1 \\
  0 &amp; 1 &amp; 1 &amp; 0
\end{pmatrix}
\left(
\begin{pmatrix}
  a \\
  b \\
\end{pmatrix}
\otimes
\begin{pmatrix}
  c \\
  d \\
\end{pmatrix}
\right)
$$

$$
  C = \begin{pmatrix}
    1 &amp; 1 &amp; 1 &amp; 0 \\
    0 &amp; 0 &amp; 0 &amp; 1
  \end{pmatrix}
  \left(
  \begin{pmatrix}
    a \\
    b \\
  \end{pmatrix}
  \otimes
  \begin{pmatrix}
    c \\
    d \\
  \end{pmatrix}
  \right)
$$

&lt;/details&gt; --&gt;

&lt;details&gt;
&lt;summary&gt;&quot;Proof&quot;&lt;/summary&gt;

&lt;img src=&quot;/assets/comp_sci/half_adder_proof.jpg&quot; style=&quot;centerme&quot; alt=&quot;half adder proof&quot; /&gt;

&lt;/details&gt;

&lt;h4 id=&quot;shortcut&quot;&gt;Shortcut&lt;/h4&gt;
&lt;p&gt;While we could multiply through all the matrices and solve for what the matrix representation of a given boolean circuit is, there is a way to bypass all this. Just like how we constructed the unary and binary logic gates, we can simply construct a &lt;strong&gt;truth table&lt;/strong&gt; of what inputs give what outputs. We then construct the matrix from this, setting the $i$th column as the output of the half-adder given the input $i$. Doing this we arrive at the following:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\text{Half-Adder}=
\begin{pmatrix}
  1 &amp; 0 &amp; 0 &amp; 0 \\
  0 &amp; 0 &amp; 0 &amp; 1 \\
  0 &amp; 1 &amp; 1 &amp; 0 \\
  0 &amp; 0 &amp; 0 &amp; 0
\end{pmatrix} %]]&gt;&lt;/script&gt;

&lt;details&gt;
&lt;summary&gt;&lt;h3 class=&quot;inline&quot;&gt;CNOT and Quantum Computing&lt;/h3&gt;&lt;/summary&gt;
&lt;!-- ### Sidenote: CNOT --&gt;
As a final example, and lead in into quantum computing, I'll describe a binary gate that outputs two bits rather than one: the controlled NOT or &lt;code&gt;CNOT&lt;/code&gt; gate.

Its function is pretty simple: the gate acts as a NOT gate on the second bit, but only if the first bit is $1$. If this first bit, dubbed the &lt;b&gt;control bit&lt;/b&gt;, is $0$ then the second bit, dubbed the &lt;b&gt;target bit&lt;/b&gt;, won't be affected at all. The first, unchanged, bit and the second, possibly negated, bit are then outputted. Its matrix looks like this:

$$
  \text{CNOT} = \begin{pmatrix}
    1 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\
    0 &amp;amp; 1 &amp;amp; 0 &amp;amp; 0 \\
    0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1 \\
    0 &amp;amp; 0 &amp;amp; 1 &amp;amp; 0
  \end{pmatrix}
$$

$$\text{CNOT}|00\rangle=|00\rangle$$

$$\text{CNOT}|01\rangle=|01\rangle$$

$$\text{CNOT}|10\rangle=|11\rangle$$

$$\text{CNOT}|11\rangle=|10\rangle$$

While the gate may not seem particularly interesting (and indeed it isn't as far as classical computing is concerned) its real power shows when the control bit is in a superposition of both $|0\rangle$ and $|1\rangle$, ala quantum computing.
&lt;/details&gt;
</content>
 </entry>
 
 <entry>
   <title>Letter Case and Computing</title>
   <link href="http://localhost:4000/letter-case/"/>
   <updated>2018-07-05T00:00:00-04:00</updated>
   <id>http://localhost:4000/letter-case</id>
   <content type="html">&lt;h2 id=&quot;what-is-letter-case&quot;&gt;What is Letter Case?&lt;/h2&gt;
&lt;p&gt;Letter case refers to whether a letter is &lt;strong&gt;uppercase&lt;/strong&gt;/capitalized (&lt;em&gt;A&lt;/em&gt;) or &lt;strong&gt;lowercase&lt;/strong&gt; (&lt;em&gt;a&lt;/em&gt;). There are different styles of writing words and sentences with different conventions of where to capitalize letters and where not to.&lt;/p&gt;

&lt;p&gt;The topic of letter case is a detailed one, involving the history and linguistics of different languages, their grammars, and of printed word itself. This post will touch upon letter case styles in general writing, but will mostly focus on the use of case styles on the internet, in programming, and computing in general. Specifically in English.&lt;/p&gt;

&lt;!--more--&gt;

&lt;h2 id=&quot;writingtyping&quot;&gt;Writing/Typing&lt;/h2&gt;
&lt;!-- ### This is sentence case. --&gt;
&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;This is sentence case.&lt;/strong&gt;&lt;/summary&gt;
This is the standard letter case used in the English language. The beginning of sentences are capitalized along with proper nouns (i.e names, places, days of the week, etc.). Each sentence ends with some punctuation (ex &lt;i&gt;.&lt;/i&gt;, &lt;i&gt;?&lt;/i&gt;, &lt;i&gt;!&lt;/i&gt;).

Here's an example:
&lt;!-- &gt; The quick brown fox jumps over the lazy dog. --&gt;
&lt;blockquote&gt;The quick brown fox jumps over the lazy dog.&lt;/blockquote&gt;

You'd see this case when reading anything somewhat formal, like emails, a person's/company's website, text documents, academic papers, etc.

&lt;/details&gt;

&lt;!-- ### This is Title Case --&gt;
&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;This is Title Case&lt;/strong&gt;&lt;/summary&gt;
This is the case used when typing out titles. Title case has all words capitalized barring a limited subset that are deemed &lt;i&gt;common&lt;/i&gt; (unless they are the first word, in which case they are capitalized). That includes words like &lt;i&gt;or&lt;/i&gt;, &lt;i&gt;but&lt;/i&gt;, &lt;i&gt;if&lt;/i&gt;, &lt;i&gt;the&lt;/i&gt;, &lt;i&gt;in&lt;/i&gt;, etc.

Here's an example:
&lt;!-- &gt; The Quick Brown Fox Jumps over the Lazy Dog --&gt;
&lt;blockquote&gt;The Quick Brown Fox Jumps over the Lazy Dog&lt;/blockquote&gt;

Academic papers, emails (think subject line), music, videogames and just about any media or document has a title. Its important to note that there is no agreement on what exactly the subset of words not capitalized in title case is. That said, all titles more or less follow the same pattern of capitalization.
&lt;/details&gt;

&lt;!-- ### ALL CAPS --&gt;
&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;ALL CAPS&lt;/strong&gt;&lt;/summary&gt;
Writing in all uppercase letters, or &lt;b&gt;all caps&lt;/b&gt; as it's more commonly known, is taken to convey yelling when typed out, especially when texting or commenting on the internet. That said, all caps is also used in titles and headlines in order to make the text stand out more.

Here's what it looks like:

&lt;!-- &gt; THE QUICK BROWN FOX JUMPS OVER THE LAZY DOG! --&gt;
&lt;blockquote&gt;THE QUICK BROWN FOX JUMPS OVER THE LAZY DOG!&lt;/blockquote&gt;

Typing in all caps can be done by holding down the &lt;code&gt;SHIFT&lt;/code&gt; key while typing or, more commonly, by simply turning on the &lt;code&gt;CAPS LOCK&lt;/code&gt; key. Smartphone keyboards also have similar functionality with caps lock being activated by double tapping the shift key, for example. Acronyms are written in all caps (NATO, USA, ASAP) unless the acronym has had enough usage to become a fully fledged word like &lt;i&gt;radar&lt;/i&gt; (&lt;b&gt;ra&lt;/b&gt;dio &lt;b&gt;d&lt;/b&gt;etection &lt;b&gt;a&lt;/b&gt;nd &lt;b&gt;r&lt;/b&gt;anging) or &lt;i&gt;laser&lt;/i&gt; (&lt;b&gt;l&lt;/b&gt;ight &lt;b&gt;a&lt;/b&gt;mplification by &lt;b&gt;s&lt;/b&gt;timulated &lt;b&gt;e&lt;/b&gt;mission of &lt;b&gt;r&lt;/b&gt;adiation)
&lt;/details&gt;

&lt;!-- ### all lowercase --&gt;
&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;all lowercase&lt;/strong&gt;&lt;/summary&gt;
All lowercase is typically used when writing something informally and quickly, such as a text message or an internet comment (if the commenter isn't particularly concerned with the comment's presentation). This is because the message is not worth the extra effort of pressing the &lt;code&gt;SHIFT&lt;/code&gt; key (especially on smartphone keyboards). File extensions are also conventionally all lowercase.

Here's what it looks like:

&lt;!-- &gt; the quick brown fox jumps over the lazy dog --&gt;
&lt;blockquote&gt;the quick brown fox jumps over the lazy dog&lt;/blockquote&gt;

Since the use of all lowercase is one of convenience, text written in it is not usually punctuated unless necessary to convey intent. (you hate him. vs. you hate him?)

That said, this case may soon die out as some mobile keyboards, for example Google's GBoard for Android, automatically capitalize the first letter in sentences as well as in proper nouns. This essentially makes typing in sentence case the default.
&lt;/details&gt;

&lt;h2 id=&quot;computingprogramming&quot;&gt;Computing/Programming&lt;/h2&gt;
&lt;p&gt;For many languages, it is best practice to use different type cases to differentiate variables, methods, classes, functions, and other identifiers from each other, as well as provide useful metadata on said identifiers. Their use in languages is almost never mandatory and instead serve to promote readability and uniformity in code.&lt;/p&gt;

&lt;p&gt;Virtually all programming languages do not allow whitespace in identifier names. As such, the three main programming type cases do not make use of spaces and instead use other means of &lt;b&gt;delimiting&lt;/b&gt;, or telling apart, words. It is also important to note that these naming conventions don’t just utilize different type cases for different identifiers, but also uppercase and lowercase variations of those type cases.&lt;/p&gt;

&lt;!-- ### CamelCase --&gt;
&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;CamelCase&lt;/strong&gt;&lt;/summary&gt;
Camel case has the first letter of every word in the phrase/identifier capitalized. And so capitalization serves as camel case's delimiter. Lower camel case has the first letter of the first word in the identifier lowercased while upper camel case does not:

&lt;!-- &gt;UpperCamelCase --&gt;
&lt;blockquote&gt;UpperCamelCase&lt;/blockquote&gt;

&lt;!-- &gt;lowerCamelCase --&gt;
&lt;blockquote&gt;lowerCamelCase&lt;/blockquote&gt;

Used in
&lt;ul&gt;
  &lt;li&gt;Class names in Java (upper)&lt;/li&gt;
  &lt;li&gt;Class names in Python (upper)&lt;/li&gt;
  &lt;li&gt;Method names in Java (lower)&lt;/li&gt;
  &lt;li&gt;Variables names in Java (lower)&lt;/li&gt;
&lt;/ul&gt;
&lt;/details&gt;

&lt;!-- ### snake_case --&gt;
&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;snake_case&lt;/strong&gt;&lt;/summary&gt;
Snake case delimits words using underscores ( _ ). And like the others, There are uppercase and lowercase variants:
&lt;!-- &gt;UPPER_SNAKE_CASE --&gt;
&lt;blockquote&gt;UPPER_SNAKE_CASE&lt;/blockquote&gt;

&lt;!-- &gt;lower_snake_case --&gt;
&lt;blockquote&gt;lower_snake_case&lt;/blockquote&gt;

This case is commonly used for naming attributes on computers, like files or usernames, where space characters cannot be processed. Snake case is essentially the default type case used when spaces are unavailable.

Used in
&lt;ul&gt;
  &lt;li&gt;Constants in Java (upper)&lt;/li&gt;
  &lt;li&gt;Constants in Python (upper)&lt;/li&gt;
  &lt;li&gt;Variables in Python (lower)&lt;/li&gt;
  &lt;li&gt;Functions and classes in the C &amp;amp; C++ Standard Library (lower)&lt;/li&gt;
&lt;/ul&gt;
&lt;/details&gt;

&lt;!-- ### kebab-case &amp; Train-Case --&gt;
&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;kebab-case &amp;amp; Train-Case&lt;/strong&gt;&lt;/summary&gt;
Kebab case is similar to snake case, as well as used in similar situations, with its only difference being the use of hyphens (-) to delimit words rather than underscores:
&lt;!-- &gt;UPPER-KEBAB-CASE --&gt;
&lt;blockquote&gt;UPPER-KEBAB-CASE&lt;/blockquote&gt;

&lt;!-- &gt;lower-kebab-case --&gt;
&lt;blockquote&gt;lower-kebab-case&lt;/blockquote&gt;

There's also a third variation similar to camel case called train case where the first letter of each word capitalized:

&lt;!-- &gt;Train-Case --&gt;
&lt;blockquote&gt;Train-Case&lt;/blockquote&gt;

Used in
&lt;ul&gt;
  &lt;li&gt;CSS Classes (lower)&lt;/li&gt;
  &lt;li&gt;URLs (lower)&lt;/li&gt;
&lt;/ul&gt;
&lt;/details&gt;

&lt;h2 id=&quot;other&quot;&gt;Other&lt;/h2&gt;
&lt;!-- ### StUdLyCaPs --&gt;
&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;StUdLyCaPs&lt;/strong&gt;&lt;/summary&gt;
Studly caps is meant to be a joking letter case used to convey sarcasm. It has roots in mocking the seemingly random capitalization of products and services that was popular during the mid 1990s to early 2000s. A sort of in-joke amongst people on the internet at the time.
&lt;!-- &gt;tHeqUIckBrOwNFOxjuMpsOvERtHeLazYdOg --&gt;
&lt;blockquote&gt;tHeqUIckBrOwNFOxjuMpsOvERtHeLazYdOg&lt;/blockquote&gt;

A variation on Studly caps is one where spaces are included to delimit words. This makes the message more readable while still retaining its sarcastic tone:

&lt;!-- &gt;tHe qUIck BrOwN FOx juMps OvER tHe lAzYdOg --&gt;
&lt;blockquote&gt;tHe qUIck BrOwN FOx juMps OvER tHe lAzY dOg&lt;/blockquote&gt;

Indeed this is the same case used in the &lt;a href=&quot;http://knowyourmeme.com/memes/mocking-spongebob&quot;&gt;mocking Spongebob meme&lt;/a&gt; that was popular around 2017.
&lt;/details&gt;
</content>
 </entry>
 
 <entry>
   <title>On the Origin of "Hello, World!"</title>
   <link href="http://localhost:4000/hello-world/"/>
   <updated>2018-06-25T00:00:00-04:00</updated>
   <id>http://localhost:4000/hello-world</id>
   <content type="html">&lt;!-- - [What is &quot;Hello, World!&quot;?](#what-is-hello-world)
- [History](#history)
  - [A Tutorial Introduction to the Language B](#a-tutorial-introduction-to-the-language-b)
  - [Programming in C: A Tutorial](#programming-in-c-a-tutorial)
  - [The C Programming Language](#the-c-programming-language)
  - [Side note on BCPL](#side-note-on-bcpl)
- [Examples](#examples) --&gt;

&lt;h2 id=&quot;what-is-hello-world&quot;&gt;What is “Hello, World!”?&lt;/h2&gt;
&lt;p&gt;When learning to program for the first time, students are often instructed to write a program that outputs the string &lt;code class=&quot;highlighter-rouge&quot;&gt;&quot;Hello, World!&quot;&lt;/code&gt; in order to verify that their programming environment is set up correctly.&lt;/p&gt;

&lt;p&gt;Since the first “Hello, World!” program was written in 1972, it has become a tradition amongst computer science teachers and professors to introduce the topic of programming with this example. As such, “Hello, World!” is often the first program most people write.&lt;/p&gt;

&lt;!--more--&gt;

&lt;h2 id=&quot;history&quot;&gt;History&lt;/h2&gt;
&lt;p&gt;Why and how outputting the string “Hello, World!” became the quintessential beginner program is a matter of interest for both computer scientists and historians alike due to the prevalence of this practice in computer science education today.&lt;/p&gt;

&lt;p&gt;The program was created by Professor Brian Kernighan during his time at Bell Labs. He used them in his documentation of the B and C programming languages. Below, I detail the first 3 appearances of the example:&lt;/p&gt;

&lt;h3 id=&quot;a-tutorial-introduction-to-the-language-b&quot;&gt;A Tutorial Introduction to the Language B&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;B&lt;/strong&gt; is a programming language developed at Bell Labs around 1969 and is a simplified successor to &lt;strong&gt;BCPL&lt;/strong&gt;. It was in 1972 that Kernighan was tasked with writing a manual for using B that was to be used internally at Bell Labs. This memorandum, &lt;a href=&quot;https://www.bell-labs.com/usr/dmr/www/bintro.html&quot;&gt;&lt;em&gt;A Tutorial Introduction to the Language B&lt;/em&gt;&lt;/a&gt;, is the first documented instance of “Hello, World!” in all of programming cannon.&lt;/p&gt;

&lt;p&gt;The program appears in section &lt;em&gt;7 – External Variables&lt;/em&gt; in the following form to demonstrate the piecing together of several &lt;code class=&quot;highlighter-rouge&quot;&gt;char&lt;/code&gt; variables:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-b&quot;&gt;main( ) {
  extrn a, b, c;
  putchar(a); putchar(b); putchar(c); putchar(’!*n’);
}

a ’hell’;
b ’o, w’;
c ’orld’;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It appears again in the next section &lt;em&gt;8 – Functions&lt;/em&gt; to demonstrate the composition of functions:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-b&quot;&gt;main( ) {
  extrn a,b,c,d;
  put2char(a,b) ;
  put2char(c,d) ;
}

put2char(x,y) {
  putchar(x);
  putchar(y);
}

a ’hell’; b ’o, w’; c ’orld’; d ’!*n’;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;programming-in-c-a-tutorial&quot;&gt;Programming in C: A Tutorial&lt;/h3&gt;
&lt;p&gt;The next instance of “Hello, World!” appeared in another internal memorandum at Bell Labs titled &lt;a href=&quot;https://www.bell-labs.com/usr/dmr/www/ctut.pdf&quot;&gt;&lt;em&gt;Programming in C: A Tutorial&lt;/em&gt;&lt;/a&gt;. The document was again authored by Prof. Kernighan and was for the programming language &lt;strong&gt;C&lt;/strong&gt;, developed from 1969 to 1973 by Dennis Ritchie.&lt;/p&gt;

&lt;p&gt;The program is the first code seen in the document, appearing in section &lt;em&gt;2 – A Simple C Program&lt;/em&gt;:&lt;/p&gt;
&lt;div class=&quot;language-c highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;printf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;hello, world&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;the-c-programming-language&quot;&gt;The C Programming Language&lt;/h3&gt;
&lt;p&gt;While the above two instances of “Hello, World!” were the first, it wasn’t until Kernighan and Ritchie published their famous book &lt;em&gt;The C Programming Language&lt;/em&gt; in 1978 that the example program would catch on. It first appears in section &lt;em&gt;1.1 – Getting Started&lt;/em&gt;:&lt;/p&gt;
&lt;div class=&quot;language-c highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;cp&quot;&gt;#include &amp;lt;stdio.h&amp;gt;
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;printf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;hello, world&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;em&gt;The &lt;code class=&quot;highlighter-rouge&quot;&gt;include&lt;/code&gt; statement in the first line was added to the second edition of the book and is not present in the first.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;It then appears again in the same section but split up to demonstarte the use of the &lt;code class=&quot;highlighter-rouge&quot;&gt;\n&lt;/code&gt; escape sequence:&lt;/p&gt;

&lt;div class=&quot;language-c highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;cp&quot;&gt;#include &amp;lt;stdio.h&amp;gt;
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;printf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;hello, &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;printf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;world&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;printf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;em&gt;Again, the first edition doesn’t include the &lt;code class=&quot;highlighter-rouge&quot;&gt;include&lt;/code&gt; statement.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Indeed the book, often abbreviated K&amp;amp;R for its authors, remains a cornerstone of programming literature even today. It should be no surprise then, that the practice of using “Hello, World!” as a test program for beginners would originate from it.&lt;/p&gt;

&lt;h4 id=&quot;side-note-on-bcpl&quot;&gt;Side note on BCPL&lt;/h4&gt;
&lt;p&gt;Several sources on the internet claim that Prof. Kernighan first wrote the “Hello, World!” program in his documentation of the Basic Combined Programming Language (BCPL) at Bell Labs before he wrote it for the B tutorial. This however is untrue. Unsure myself of whether the first documented use of “Hello, World!” was in BCPL or B, I emailed Prof. Kernighan and he confirmed it:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;I have never written a line of BCPL, so I definitely never wrote a hello world example for BCPL documentation. As best I can recall, the original example was for the internal B manual that I wrote at Bell Labs…&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;examples&quot;&gt;Examples&lt;/h2&gt;
&lt;p&gt;Below are 4 examples of “Hello, World!” programs in Java, Python, C++, and Brainfuck respectively. You can find many more examples for different languages at the &lt;a href=&quot;http://helloworldcollection.de&quot;&gt;Hello World Collection&lt;/a&gt; website.&lt;/p&gt;

&lt;h4 id=&quot;java&quot;&gt;Java&lt;/h4&gt;
&lt;div class=&quot;language-java highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kd&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;HelloWorld&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;kd&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;System&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;println&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Hello, World!&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Java is a class based, object oriented language. Because of this we must first define a &lt;code class=&quot;highlighter-rouge&quot;&gt;HelloWorld&lt;/code&gt; object class with a &lt;code class=&quot;highlighter-rouge&quot;&gt;main()&lt;/code&gt; method. Here we can call the &lt;code class=&quot;highlighter-rouge&quot;&gt;println()&lt;/code&gt; method to output the string &lt;code class=&quot;highlighter-rouge&quot;&gt;&quot;Hello, World!&quot;&lt;/code&gt;.&lt;/p&gt;

&lt;h4 id=&quot;python&quot;&gt;Python&lt;/h4&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Hello, World!&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Python is a scripting language so programs are simply a set of commands run from top to bottom. As such, its “Hello, World!” program contains no boilerplate code and is simply a &lt;code class=&quot;highlighter-rouge&quot;&gt;print()&lt;/code&gt; statement.&lt;/p&gt;

&lt;h4 id=&quot;c&quot;&gt;C++&lt;/h4&gt;
&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;cp&quot;&gt;#include &amp;lt;iostream&amp;gt;
&lt;/span&gt;
&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cout&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Hello, World!&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;All C++ programs start by executing the &lt;code class=&quot;highlighter-rouge&quot;&gt;main()&lt;/code&gt; function. Here we can print out the desired string to the &lt;em&gt;standard character output device&lt;/em&gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;std::cout&lt;/code&gt;.&lt;/p&gt;

&lt;h4 id=&quot;brainfuck&quot;&gt;Brainfuck&lt;/h4&gt;
&lt;pre&gt;&lt;code class=&quot;language-brainfuck&quot;&gt;++++++++[&amp;gt;++++[&amp;gt;++&amp;gt;+++&amp;gt;+++&amp;gt;+&amp;lt;&amp;lt;&amp;lt;&amp;lt;-]&amp;gt;+&amp;gt;+&amp;gt;-&amp;gt;&amp;gt;+[&amp;lt;]&amp;lt;-]&amp;gt;&amp;gt;.&amp;gt;---.
+++++++..+++.&amp;gt;&amp;gt;.&amp;lt;-.&amp;lt;.+++.------.--------.&amp;gt;&amp;gt;+.&amp;gt;++.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;See &lt;a href=&quot;https://en.wikipedia.org/wiki/Brainfuck#Hello_World!&quot;&gt;here&lt;/a&gt; for a commented version of this program.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Brainfuck is an esoteric programming language whose sole purpose is to be obfuscated and difficult to program in. As such, writing a “Hello World!” program in it (and other esoteric languages) is a show of programmatical prowess rather than inexperience.&lt;/p&gt;

&lt;!-- #### C#
~~~ C#
using System;

internal static class HelloWorld {
    private static void Main() {
        Console.WriteLine(&quot;Hello, world!&quot;);
    }
}
~~~

#### Prolog
~~~ Prolog
main:- write('Hello, world!'),nl.
~~~ --&gt;
</content>
 </entry>
 
 <entry>
   <title>Natural Numbers</title>
   <link href="http://localhost:4000/natural-numbers/"/>
   <updated>2018-05-25T00:00:00-04:00</updated>
   <id>http://localhost:4000/natural-numbers</id>
   <content type="html">&lt;h2 id=&quot;what-are-natural-numbers&quot;&gt;What are Natural Numbers?&lt;/h2&gt;
&lt;p&gt;The natural numbers are considered the most &lt;em&gt;natural&lt;/em&gt; set of numbers humans can begin to think of, observe and calculate. They are the numbers we count with $0,1,2,3,4,5,\cdots$&lt;/p&gt;

&lt;p&gt;When used to denote the &lt;em&gt;order&lt;/em&gt; of objects, they are considered &lt;strong&gt;ordinal&lt;/strong&gt; numbers. When used to describe the quantity of a set of objects, they are considered &lt;strong&gt;cardinal&lt;/strong&gt; numbers. This distinction becomes important when the naturals are extended to include transfinite numbers.&lt;/p&gt;

&lt;h4 id=&quot;is-0-an-element-of-mathbbn&quot;&gt;Is $0$ an element of $\mathbb{N}$?&lt;/h4&gt;
&lt;p&gt;Some definitions of the natural numbers don’t include $0$ but, all things considered, including it makes defining the naturals as well as expressing a variety formulae easier. As such, whenever I reference the set of naturals it will always contain $0$. The &lt;a href=&quot;#notation&quot;&gt;notation&lt;/a&gt; for different variations is cleared up below.&lt;/p&gt;

&lt;!--more--&gt;

&lt;h2 id=&quot;definition&quot;&gt;Definition&lt;/h2&gt;
&lt;p&gt;The natural numbers are constructed by defining the empty set as zero, then defining the next number, or &lt;strong&gt;successor&lt;/strong&gt;, as the set of all previous numbers:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
0&amp;\equiv\emptyset\\
1&amp;\equiv\{0\}=\{\emptyset\}\\
2&amp;\equiv\{0,1\}=\{\emptyset,\{\emptyset\}\}\\
3&amp;\equiv\{0,1,2\}=\{\emptyset,\{\emptyset\},\{\emptyset,\{\emptyset\}\}\}\\
&amp;\ \ \vdots\\
n&amp;\equiv\{0,1,2,3,\cdots,n-1\}\\
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Defining the naturals in this way is consistent with the construction of the &lt;a href=&quot;http://mathworld.wolfram.com/OrdinalNumber.html&quot;&gt;von Neumann ordinals&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&quot;successor-function&quot;&gt;Successor Function&lt;/h4&gt;
&lt;p&gt;This definition of the naturals can be more succinctly written via the successor function $S(n)$ which returns the number that comes after $n$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;S(n)=n\cup\{n\}&lt;/script&gt;

&lt;p&gt;For example, we define the number $1$ to be the successor, or number that comes after, $0$. Since $0=\emptyset$ we can say:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
1&amp;\equiv S(0)\\
&amp;=0\cup\{0\}\\
&amp;=\{0\}\\
&amp;=\{\emptyset\}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;We can keep going with $2\equiv S(1)$, $3\equiv S(2)$ and in general, we can say $n+1\equiv S(n)$ for all natural numbers.&lt;/p&gt;

&lt;h4 id=&quot;existence-in-zfc&quot;&gt;Existence in ZFC&lt;/h4&gt;
&lt;p&gt;While it is possible to construct the successor of every natural number, using the axiom of pairing (to prove the existence of the singleton set) and the axiom of union, we can only prove that any &lt;em&gt;finite&lt;/em&gt; natural number exists. No matter how many successors we compute, there will always be another.&lt;/p&gt;

&lt;p&gt;So how do we prove the existence of the entire infinite set of natural numbers and not just an arbitrarily large, finite subset? Well as it turns out, the only way to do this is to simply declare that such a set exists. This is the &lt;strong&gt;axiom of infinity&lt;/strong&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\exists S\left(\emptyset\in S\land\forall x\in S\left(\left(x\cup\{x\}\right)\in S\right)\right)&lt;/script&gt;

&lt;p&gt;In English, the above statement asserts that there exists a set $S$ that contains the empty set $\emptyset$ as well as the successor (i.e $x\cup\{x\}$) to every one of its members. Notice that since we can always take the successor of another number, this set is &lt;em&gt;necessarily&lt;/em&gt; infinite. Indeed all infinite sets in ZFC originate from this set $S$ which, while we don’t know its exact composition, is by definition at least a superset of (if not equal to) the natural numbers: $\mathbb{N}\subseteq S$. Via the axiom of subset, we can thus guarantee the existence of the natural numbers.&lt;/p&gt;

&lt;h2 id=&quot;notation&quot;&gt;Notation&lt;/h2&gt;
&lt;p&gt;The set of natural numbers is denoted by the double struck capital letter $\mathbb{N}$. But there are variations on the set that do/don’t include $0$ as well as ones that include all natural numbers up to and including $n$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{alignat*}{2}
&amp;\mathbb{N}&amp;&amp;=\{0,1,2,3,4,5,\cdots\}\\
&amp;\mathbb{N}^* &amp;&amp;=\{1,2,3,4,5,\cdots\}\\
&amp;\mathbb{N}_n&amp;&amp;=\{0,1,2,3,4,\cdots,n\}\\
&amp;\mathbb{N}^* _ n&amp;&amp;=\{1,2,3,4,\cdots,n\}\\
\end{alignat*} %]]&gt;&lt;/script&gt;

&lt;p&gt;The last one in particular, the set of all non-zero natural numbers up to $n$, can be written as the integer interval $[1\ldotp\ldotp n]$ to avoid confusion.&lt;/p&gt;

&lt;h2 id=&quot;structure-on-the-naturals&quot;&gt;Structure on the Naturals&lt;/h2&gt;
&lt;details&gt;
&lt;summary&gt;&lt;h3 class=&quot;inline&quot;&gt;Addition&lt;/h3&gt;&lt;/summary&gt;
Using the successor function and two rules, we can &lt;i&gt;inductively&lt;/i&gt; define an operation $+$ such that it forms a commutative monoid $(\mathbb{N},+)$ with identity $0$ on the naturals:

$$a+0=0 \tag{1}$$
$$a+S(b)=S(a+b) \tag{2}$$

&lt;details&gt;&lt;summary&gt;&lt;strong&gt;Example&lt;/strong&gt;&lt;/summary&gt;

$$\begin{align}
2+2&amp;amp;=2+S(1) \tag{def of $2\equiv S(1)$}\\
&amp;amp;=S(2+1) \tag{addition rule #2}\\
&amp;amp;=S(2+S(0)) \tag{def of $1\equiv S(0)$}\\
&amp;amp;=S(S(2+0)) \tag{addition rule #2}\\
&amp;amp;=S(S(2)) \tag{addition rule #1}\\
&amp;amp;=S(3) \tag{def of $3\equiv S(2)$}\\
&amp;amp;=4 \tag{def of $4\equiv S(3)$}\\
\end{align}$$
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt;

This monoid is also &lt;b&gt;cancellative&lt;/b&gt;, meaning that an equation is still true even if we add or remove the same number on both sides:

$$a+c=b+c\implies a=b$$

It is from the addition operation $+$ (and successor function $S(n)$) that we can define all the other operations/relations on the naturals.
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;h3 class=&quot;inline&quot;&gt;Subtraction&lt;/h3&gt;&lt;/summary&gt;
We can define subtraction in terms of addition as so:

$$a-b=c\iff a=b+c$$

&lt;!-- The set that represents this binary operation would be constructed like this:

$$-\equiv\{(a,b,c)\in\mathbb{N}^3\mid a=b+c\}$$ --&gt;

But note that the difference between two natural numbers $a-b$ exists only if $a\ge b$. As such, subtraction is not defined for any two natural numbers and so is not closed. Moreover, subtraction isn't associative: $(a-b)-c\not=a-(b-c)$.

&lt;h4&gt;Truncated Subtraction&lt;/h4&gt;
However, it is possible to define a meaningful notion of subtraction on the naturals. It's called &lt;b&gt;truncated subtraction&lt;/b&gt;, denoted $\dot-$, and forms the magma $(\mathbb{N},\dot-)$.

Using the definition of subtraction above, we can define truncated subtraction as:

$$a\ \dot- \ b=
\begin{cases}
    0,&amp;amp; a&amp;lt;b\\
    a-b,&amp;amp; a\ge b
\end{cases}$$

If we want to define it purely in terms of natural arithmetic, we can &lt;i&gt;inductively&lt;/i&gt; define $\dot-$ similar to how we defined $+$:

$$a\ \dot- \ 0=a \tag{1}$$
$$0\ \dot- \ a=0 \tag{2}$$
$$S(a)\ \dot- \ S(b)=a\ \dot- \ b \tag{3}$$

&lt;details&gt;&lt;summary&gt;&lt;strong&gt;Examples&lt;/strong&gt;&lt;/summary&gt;
When $a&amp;gt;b$
$$\begin{align}
5\ \dot- \ 3&amp;amp;=S(4)\ \dot- \ S(2) \tag{def of $5$ &amp;amp; $3$}\\
&amp;amp;=4\ \dot- \ 2 \tag{subtraction rule #3}\\
&amp;amp;=S(3)\ \dot- \ S(1) \tag{def of $4$ &amp;amp; $2$}\\
&amp;amp;=3\ \dot- \ 1 \tag{subtraction rule #3}\\
&amp;amp;=S(2)\ \dot- \ S(0) \tag{def of $3$ &amp;amp; $1$}\\
&amp;amp;=2\ \dot- \ 0 \tag{subtraction rule #3}\\
&amp;amp;=2 \tag{subtraction rule #1}\\
\end{align}$$

When $a&amp;lt;b$
$$\begin{align}
3\ \dot- \ 5&amp;amp;=S(2)\ \dot- \ S(4) \tag{def of $3$ &amp;amp; $5$}\\
&amp;amp;=2\ \dot- \ 4 \tag{subtraction rule #3}\\
&amp;amp;=S(1)\ \dot- \ S(3) \tag{def of $2$ &amp;amp; $4$}\\
&amp;amp;=1\ \dot- \ 3 \tag{subtraction rule #3}\\
&amp;amp;=S(0)\ \dot- \ S(2) \tag{def of $1$ &amp;amp; $3$}\\
&amp;amp;=0\ \dot- \ 2 \tag{subtraction rule #3}\\
&amp;amp;=0 \tag{subtraction rule #2}\\
\end{align}$$
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt;

In conjunction with the commutative monoid on the naturals $(\mathbb{N},+)$, this operator is referred to as a &lt;b&gt;monus&lt;/b&gt; and forms a CMM, commutative monoid with monus: $(\mathbb{N},+,\dot-)$.
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;h3 class=&quot;inline&quot;&gt;Multiplication&lt;/h3&gt;&lt;/summary&gt;
We can define the multiplication of natural numbers in terms of repeated addition such that it forms a commutative monoid $(\mathbb{N},\times)$:

$$a\times b=c\iff \underbrace{a+a+\cdots+a}_{b\text{ copies}}=c$$

&lt;!-- The set that represents this binary operation would be constructed like this:

$$\times\equiv\{(a,b,c)\in\mathbb{N}^3\mid\underbrace{a+a+\cdots+a}_{b\text{ copies}}=c\}$$ --&gt;

We can also &lt;i&gt;inductively&lt;/i&gt; define $\times$ with 2 rules:

$$a\times 0=0 \tag{1}$$
$$a\times S(b)=(a\times b) + a \tag{2}$$

&lt;details&gt;&lt;summary&gt;&lt;strong&gt;Example&lt;/strong&gt;&lt;/summary&gt;
$$\begin{align}
2\times 3&amp;amp;=2\times S(2) \tag{def of $3\equiv S(2)$}\\
&amp;amp;=(2\times 2)+2 \tag{multiplication rule #2}\\
&amp;amp;=(2\times S(1))+2 \tag{def of $2\equiv S(1)$}\\
&amp;amp;=((2\times 1)+2)+2 \tag{multiplication rule #2}\\
&amp;amp;=((2\times S(0))+2)+2 \tag{def of $1\equiv S(0)$}\\
&amp;amp;=(((2\times 0)+2)+2)+2 \tag{multiplication rule #2}\\
&amp;amp;=((0+2)+2)+2 \tag{multiplication rule #1}\\
&amp;amp;=(2+2)+2\\
&amp;amp;=4+2\\
&amp;amp;=6\\
\end{align}$$
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt;

Like with addition, this monoid is almost &lt;i&gt;cancellative&lt;/i&gt;:

$$a\times c=b\times c\implies a=b$$

However it is not fully due to the fact that the above only holds true when $a,b,c\not=0$.

&lt;h4&gt;Commutative Semiring&lt;/h4&gt;
Even further, if we combine the $\times$ operator with the monoid  $(\mathbb{N},+)$ and the fact that it is distributive over $+$:

$$a\times(b+c)=(a\times b)+(a\times c)$$

we obtain the commutative semiring $(\mathbb{N},+,\times)$. It is &lt;i&gt;semi&lt;/i&gt; because the $+$ operator has no inverse operation, which is necessary for a full ring.
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;h3 class=&quot;inline&quot;&gt;Division&lt;/h3&gt;&lt;/summary&gt;
We can define the division of natural numbers in terms of multiplication:

$$a\div b=c\iff a=b\times c$$

&lt;!-- The set that represents this binary operation would be constructed like this:

$$\div\equiv\{(a,b,c)\in\mathbb{N}^3\mid a=b\times c\}$$ --&gt;

However, like subtraction, division isn't defined for all the naturals and so isn't closed (and certainly isn't associative).

&lt;h4&gt;Euclidean Division&lt;/h4&gt;
But, like truncated subtraction, it is possible to define a meaningful division operation on the naturals in terms of $+$ and $\times$. This operation is known as &lt;b&gt;Euclidean division&lt;/b&gt; (which I'll denote $\div_E$). It divides two natural numbers $a$ and $b$ and returns a &lt;b&gt;quotient&lt;/b&gt; $q$ and a &lt;b&gt;remainder&lt;/b&gt; $r$:

$$a\div_E b=(q,r)\iff (a=b\times q+r) \land (r&amp;lt;b)$$

For every pair of naturals there is a &lt;i&gt;unique&lt;/i&gt; pair of resultants. As such, you can consider $\div_E$ as a function that maps two naturals to two other naturals:

$$\div_E:\mathbb{N}^2\to\mathbb{N}^2$$

&lt;!-- The set that represents this binary operation would be constructed like this:

$$\div_E\equiv\{(a,b,q,r)\in\mathbb{N}^4\mid a=b\times q+r \land r&lt;b\}$$ --&gt;

&lt;!-- We can *inductively* define division with 2 rules: --&gt;
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;h3 class=&quot;inline&quot;&gt;Order&lt;/h3&gt;&lt;/summary&gt;
We can define a total order $\le$ on the natural numbers as so:

$$a\le b\iff(\exists c\in\mathbb{N})\ a+c=b$$

&lt;!-- The set that represents this binary relation would be constructed like this:

$$\le\equiv\{(a,b)\in\mathbb{N}^2\mid(\exists c\in\mathbb{N})\ a+c=b\}$$ --&gt;

The naturals along with this binary relation form a totally ordered set $(\mathbb{N},\le)$ that also happens to be well-ordered. Its order type is $\omega$, the first infinite ordinal.

&lt;h4&gt;Ordered Structures&lt;/h4&gt;
In conjunction with some of the algebraic structures mentioned above, the $\le$ relation forms an ordered algebraic structure. To be able to be ordered, the structure's operation must preserve order (which is similar to the cancellative property).

For addition we can create the ordered monoid $(\mathbb{N},+,\le)$ because:

$$a\le b\iff (\forall c\in\mathbb{N})\ a+c\le b+c$$

For multiplication, if we remove $0$ from the naturals, we can create the ordered monoid $(\mathbb{N}^* ,\times,\le)$ because:

$$a\le b\iff (\forall c\in\mathbb{N})\ a\times c\le b\times c$$

Combining these two ordered monoids, we can further create the ordered semiring $(\mathbb{N},+,\times,\le)$

Note that we cannot order the monoid $(\mathbb{N},\times)$ and the magma $(\mathbb{N},\dot-)$ because they do not preserve the ordering in all cases:

&lt;details&gt;&lt;summary&gt;&lt;strong&gt;Proof of $(\mathbb{N},\times)$&lt;/strong&gt;&lt;/summary&gt;
$$\begin{align}
10\le5&amp;amp;\iff \ 10\times 0\le 5\times 0\\
F&amp;amp;\iff \ 0\le 0\\
F&amp;amp;\iff \ T\\
\therefore\ &amp;amp;\hline{a\le b\nLeftrightarrow (\forall c\in\mathbb{N})\ a\times c\le b\times c}\\
\end{align}$$
&lt;/details&gt;

&lt;details&gt;&lt;summary&gt;&lt;strong&gt;Proof of $(\mathbb{N},\dot-)$&lt;/strong&gt;&lt;/summary&gt;
$$\begin{align}
10\le5&amp;amp;\iff \ 10\ \dot-\ 500\le 5\ \dot-\ 500\\
F&amp;amp;\iff \ 0\le 0\\
F&amp;amp;\iff \ T\\
\therefore\ &amp;amp;\hline{a\le b\nLeftrightarrow (\forall c\in\mathbb{N})\ a\ \dot-\ c\le b\ \dot-\ c}\\
\end{align}$$
&lt;/details&gt;

&lt;!-- $$a\le b\iff (\forall c\in\mathbb{N})\ a\ \dot-\ c\le b\ \dot-\ c$$

For truncated subtraction we can create the ordered magma $(\mathbb{N},\dot-,\le)$ and the ordered CMM $(\mathbb{N},+,\dot-,\le)$. --&gt;
&lt;/details&gt;
</content>
 </entry>
 
 <entry>
   <title>Parity Sequences</title>
   <link href="http://localhost:4000/parity-sequences/"/>
   <updated>2018-05-10T00:00:00-04:00</updated>
   <id>http://localhost:4000/parity-sequences</id>
   <content type="html">&lt;!--
New Date is formal reformulation of parity sequences

Thought about this when we had to find Taylor series' for certain sinusoidal functions and the terms had hard to capture patterns of negative/even terms. It is possible to capture this via more sinusoidal functions but I wanted a polynomial answer. I don't think there is a polynomial answer for the general case, and the 2 examples below are the extent of my findings.

first publish date: 2018-03-03--&gt;

&lt;p&gt;While toying around with infinite alternating series, I became interested in the idea of alternation beyond the simple $+,-,+,-$ like that of the power series of $\sin x$. This is where &lt;strong&gt;parity sequences&lt;/strong&gt; come in.&lt;/p&gt;

&lt;p&gt;A parity sequence is a pattern of alternating even and odd numbers. Only the parity of the numbers is important, not the actual values themselves. As such, we can represent the parity sequence of a sequence as a list of “$+$” for even and “$-$” for odd numbers. Below I formally define the notion of a parity sequence, and some of their properties (at least the ones I could think of off the top of my head).&lt;/p&gt;

&lt;h2 id=&quot;definition-and-equivalence&quot;&gt;Definition and Equivalence&lt;/h2&gt;
&lt;p&gt;Consider the set of all integer sequences (that is, all functions from the natural numbers to the integers):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbb{Z}^\mathbb{N}\equiv\{f\mid f:\mathbb{N}\to\mathbb{Z}\}&lt;/script&gt;

&lt;!--more--&gt;

&lt;p&gt;We can define an equivalence relation $\sim$ on $\mathbb{Z}^\mathbb{N}$ that relates sequences that have the same parity for each element:&lt;/p&gt;

&lt;!-- $$\sim\equiv\{\left(a_n,b_n\right)\in (\mathbb{Z}^\mathbb{N})^2\mid\forall n\in\mathbb{N},\ (-1)^{a_n}=(-1)^{b_n}\}$$ --&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sim\equiv\{\left(a,b\right) \in \mathbb{Z}^\mathbb{N} \times \mathbb{Z}^\mathbb{N} \mid \forall n\in\mathbb{N},\ (-1)^{a_n}=(-1)^{b_n}\}&lt;/script&gt;

&lt;p&gt;Or in English: if $-1$ to the $n$th element of sequence $a_n$ equals $-1$ to the $n$th element of sequence $b_n$, then the sequences are &lt;strong&gt;parity equivalent&lt;/strong&gt;. We can write this more succinctly as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
a\sim b&amp;\equiv (a,b)\in \sim\\
&amp;\equiv (-1)^{a_n}=(-1)^{b_n}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Using this notion of parity equivalence, we can say that a parity sequence $P$ of a given sequence $S$ is simply its equivalence class in $\sim$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P=[S]=\{x\in \mathbb{Z}^\mathbb{N}\mid S\sim x\}&lt;/script&gt;

&lt;h2 id=&quot;-and---notation&quot;&gt;$+$ and $-$ Notation&lt;/h2&gt;
&lt;p&gt;Because a parity sequence is uniquely defined by the parity of the elements of any one of its members, we can represent the parity sequence of a sequence by replacing all its even terms with a $+$ and its odd terms with a $-$. For example:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;n^2=\left(0,1,4,9,16,25,36,49,\cdots\right)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;[n^2]=\left(+,-,+,-,+,-,+,-,\cdots\right)&lt;/script&gt;

&lt;p&gt;&lt;em&gt;As a shorthand, I will refer to sequences, like the sequence $(n^2)_ {n\in\mathbb{N}}$ above, by simply their $n$th term starting at $n=0$. This should not introduce any ambiguities.&lt;/em&gt;&lt;/p&gt;

&lt;h4 id=&quot;what-are--and--&quot;&gt;What are $+$ and $-$?&lt;/h4&gt;
&lt;p&gt;If we want to formalize the $+$ $-$ notation we can define them as such:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;+\equiv0 \ \ \ \ \ \ \ \ -\equiv1&lt;/script&gt;

&lt;p&gt;Thus the parity sequence $[n]$ is simply:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\left(+,-,+,-,+,-,\cdots\right)=\left(0,1,0,1,0,1,\cdots\right)&lt;/script&gt;

&lt;p&gt;Another point to make is that $[n]\not=\left(+,-,+,-,+,-,\cdots\right)$ but in fact is an element of it:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\left(+,-,+,-,+,-,\cdots\right)=\left(0,1,0,1,0,1,\cdots\right)\in[n]&lt;/script&gt;

&lt;p&gt;We only write $[n]=\left(+,-,+,-,+,-,\cdots\right)$ as a shorthand. And again, we can do this because a parity sequence is uniquely defined by the parity of the elements (integers) of any one of its members (integer sequences), meaning there are no ambiguities.&lt;/p&gt;

&lt;h2 id=&quot;parity-swap&quot;&gt;Parity Swap&lt;/h2&gt;
&lt;p&gt;A useful property to take note of when constructing these sequences is that adding $1$ to a sequence flips the parity of each element in the sequence. After performing this &lt;strong&gt;parity swap&lt;/strong&gt; on a parity sequence $P$, we call the resulting parity sequence $\bar{P}$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P=[n^2]=\left(+,-,+,-,+,-,+,\cdots\right)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\bar{P}=[n^2+1]=\left(-,+,-,+,-,+,-,\cdots\right)&lt;/script&gt;

&lt;p&gt;This is an obvious consequence of the fact that an even number plus $1$ is an odd number and vice versa for odd numbers. This means that performing a parity swap on a sequence $2$ times or any even amount of times results in $P$ and doing it any odd number of times results in $\bar{P}$.&lt;/p&gt;

&lt;h2 id=&quot;eveningodding-out&quot;&gt;Evening/Odding Out&lt;/h2&gt;
&lt;p&gt;When an integer sequence is multiplied by $2$ it becomes parity equivalent to $2n$. This is called &lt;strong&gt;evening out&lt;/strong&gt; a sequence:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;[2S_n]=\left(+,+,+,+,+,\cdots\right)&lt;/script&gt;

&lt;p&gt;Similarly, when a sequence is multiplied by $2$ then increased by $1$, it becomes parity equivalent to $2n+1$. This is called &lt;strong&gt;odding out&lt;/strong&gt; a sequence:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;[2S_n+1]=\left(-,-,-,-,-,\cdots\right)&lt;/script&gt;

&lt;p&gt;&lt;em&gt;Note that odding out a sequence is equivalent to evening it out then parity swapping it. $2S_n$ is the evening and the $+1$ is the parity swap.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;uses&quot;&gt;Uses&lt;/h2&gt;
&lt;p&gt;Notice that when we raise $-1$ to the power of one of these sequences, it will evaluate to $+1$ for even values and $-1$ for odd values.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;This property is where the $+$ $-$ notation comes from.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;When constructing things like Taylor Series’, this is a useful property to have. Terms that alternate from negative to positive in different patterns can be dealt with by appending a factor of $(-1)^{S_n}$ to the series.&lt;/p&gt;

&lt;p&gt;Parity equivalence also allows us to simplify otherwise overly complicated sequences and expressions purely using algebra rather than writing the terms out and trying to spot the pattern. Here’s an example:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;(-1)^{\sin \frac{n\pi}{2}}\left(\frac{n^2-2n}{3}\right)^n&lt;/script&gt;

&lt;p&gt;At first this expression seems a little intimidating. However, notice that $\sin \frac{n\pi}{2}$ and $n$ are parity equivalent:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\left[\sin \frac{n\pi}{2}\right]=\left[n\right]=\left(+,-,+,-,+,-,+,-,\cdots\right)&lt;/script&gt;

&lt;p&gt;Using this knowledge, and the fact that $-1$ is being raised to this sequence, we can simplify the above expression as so:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
(-1)^{\sin \frac{n\pi}{2}}\left(\frac{n^2-2n}{3}\right)^n&amp;=(-1)^{n}\left(\frac{n^2-2n}{3}\right)^n\\
&amp;=\left(\frac{2n-n^2}{3}\right)^n
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Remember, parity equivalence is only defined on the integers (rational and real numbers don’t have parity) so the above result only holds for integer $n$.&lt;/p&gt;

&lt;h2 id=&quot;examples-of-parity-sequences&quot;&gt;Examples of Parity Sequences&lt;/h2&gt;
&lt;!-- All examples start indexing at 0 (the most rational way to index lists). --&gt;
&lt;h4 id=&quot;base-case-parity-sequences&quot;&gt;Base Case Parity Sequences&lt;/h4&gt;
&lt;p&gt;The two most simple parity sequences are simply the natural numbers:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;n=\left(0,1,2,3,4,5,6,7,\cdots\right)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;[n]=\left(+,-,+,-,+,-,+,-,\cdots\right)&lt;/script&gt;

&lt;p&gt;and the positive even numbers:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;2n=\left(0,2,4,6,8,10,12,14,\cdots\right)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;[2n]=\left(+,+,+,+,+,+,+,+,\cdots\right)&lt;/script&gt;

&lt;p&gt;These two sequences can be parity swapped to form the parity sequences $\left(-,+,-,+,-,+,\cdots\right)$ and $\left(-,-,-,-,-,-,\cdots\right)$ respectively.&lt;/p&gt;

&lt;h4 id=&quot;the-fibonacci-sequence&quot;&gt;The Fibonacci Sequence&lt;/h4&gt;
&lt;p&gt;The Fibonacci sequence can be considered a parity sequence with the pattern:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
F_n&amp;=F_{n-2}+F_{n-1}\\
&amp;=\left(0,1,1,2,3,5,8,13,\cdots\right)\\
\end{align} %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{where } F_0=0 \land F_1 = 1&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;[F_n]=\left(+,-,-,+,-,-,+,-,\cdots\right)&lt;/script&gt;

&lt;details&gt;
  &lt;summary&gt;Closed Form&lt;/summary&gt;
  &lt;p&gt;

  $$\begin{align}
  F_n=\frac{\phi^n-\psi^n}{\sqrt 5}&amp;amp;=\left(0,1,1,2,3,5,8,13,\cdots\right)
  \end{align}$$

  $$[F_n]=\left(+,-,-,+,-,-,+,-,\cdots\right)$$

  $$\begin{align*}
  \text{where } &amp;amp;\phi=\frac{1+\sqrt 5}{2} \text{ (the golden ratio)}\\
  &amp;amp;\psi=\frac{1-\sqrt 5}{2} \text{ (the conjugate golden ratio)}
  \end{align*}$$
  &lt;/p&gt;
&lt;/details&gt;

&lt;h4 id=&quot;other-examples&quot;&gt;Other Examples&lt;/h4&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{alignat*}{2}
  \frac{n(n-1)}{2}&amp; &amp;&amp;= \left(0,0,1,3,6,10,15,21,\cdots\right) &amp;&amp; \\
  \left[\frac{n(n-1)}{2}\right]&amp; &amp;&amp;=\left(+,+,-,-,+,+,-,-,\cdots\right) &amp;&amp; \\\\
  \frac{n^2(n-1)}{2}&amp; &amp;&amp;= \left(0,0,2,9,24,50,90,630,\cdots\right) &amp;&amp;\\
  \left[\frac{n^2(n-1)}{2}\right]&amp; &amp;&amp;=\left(+,+,+,-,+,+,+,-,\cdots\right)&amp;&amp;\\\\
  \cos \frac{n\pi}{2}&amp; &amp;&amp;= \left(1,0,-1,0,1,0,-1,0,\cdots\right) &amp;&amp;\\
  \left[\cos \frac{n\pi}{2}\right]&amp; &amp;&amp;=\left(-,+,-,+,-,+,-,+,\cdots\right)&amp;&amp;
\end{alignat*} %]]&gt;&lt;/script&gt;
</content>
 </entry>
 
 <entry>
   <title>Cartesian Product</title>
   <link href="http://localhost:4000/cartesian-product/"/>
   <updated>2018-04-30T00:00:00-04:00</updated>
   <id>http://localhost:4000/cartesian-product</id>
   <content type="html">&lt;h2 id=&quot;definition&quot;&gt;Definition&lt;/h2&gt;
&lt;p&gt;The Cartesian product $\times$ is an operation on two sets, call them $A$ and $B$, that returns the set of all &lt;a href=&quot;/n-tuples&quot;&gt;ordered pairs&lt;/a&gt; with their first element from $A$ and their second from $B$.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;A\times B=\{(a,b)\mid a\in A \land b\in B\}&lt;/script&gt;

&lt;h4 id=&quot;existence-in-zfc&quot;&gt;Existence in ZFC&lt;/h4&gt;
&lt;p&gt;We can show that, utilizing the &lt;a href=&quot;/n-tuples#definition&quot;&gt;Kuratowski definition&lt;/a&gt; of ordered pairs, the Cartesian product of two sets is merely a subset of the following:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;A\times B \subset \mathcal{P}\left(\mathcal{P}\left(A\cup B\right)\right)&lt;/script&gt;

&lt;details&gt;&lt;summary&gt;Proof&lt;/summary&gt;
First, note that $A\cup B$ is guaranteed by the axiom of union and that its power set is guaranteed by the axiom of power set.

Next notice that, among many other elements, the power set of $A\cup B$ contains:
&lt;!-- \left(\forall a\in A,\forall b\in B\right) --&gt;
$$\{a\},\{a,b\}\in\mathcal{P}\left(A\cup B\right)$$

where $a$ and $b$ are elements in $A$ and $B$ respectively. Now if we take the power set again we'll see that the result contains, among other things, the elements:

$$\{\{a\},\{a,b\}\}\in\mathcal{P}\left(\mathcal{P}\left(A\cup B\right)\right)$$

for all $a$ and $b$ in $A$ and $B$. Now invoke the axiom of subset to choose only those elements that fit the description and we are done!
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt;

&lt;!-- $A\cup B$ is guaranteed by the axiom of union, and its repeated power sets are guaranteed by the axiom of power set. This coupled with the fact that the Cartesian product is only a subset of the above expression implies that it must exist. --&gt;

&lt;!--more--&gt;

&lt;h2 id=&quot;n-ary-cartesian-product&quot;&gt;$n$-ary Cartesian Product&lt;/h2&gt;
&lt;p&gt;While the Cartesian product of two sets returns a set of ordered pairs, the Cartesian product of $n$ sets returns an $n$-tuple:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;S_1\times S_2\times \cdots \times S_n=\{\left(s_1,s_2,\cdots,s_n\right)\mid\forall k,\ s_k\in S_k)\}&lt;/script&gt;

&lt;details&gt;&lt;summary&gt;Details &amp;amp; Explanation&lt;/summary&gt;

Here's an example of a $3$-ary Cartesian product returning a set of $3$-tuples that uses the definition of an &lt;a href=&quot;/n-tuples#n-tuples&quot;&gt;$n$-tuple&lt;/a&gt;:

$$\begin{align}
A\times B\times C&amp;amp;=(A \times B)\times C\\
&amp;amp;=\{(a,b)\}\times C\\
&amp;amp;=\{((a,b),c)\}\\
&amp;amp;=\{(a,b,c)\}
\end{align}$$
*Where $a\in A$, $b\in B$, and $c\in C$*.
&lt;/details&gt;

&lt;h4 id=&quot;cartesian-powers&quot;&gt;Cartesian Powers&lt;/h4&gt;
&lt;p&gt;When referring to the repeated Cartesian product of the same set $S$, we can use an abbreviated notation:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\underbrace{S\times S \times \cdots \times S}_{n\text{ sets}}=S^n&lt;/script&gt;

&lt;h4 id=&quot;arity&quot;&gt;Arity&lt;/h4&gt;
&lt;p&gt;The arity of a Cartesian product is the number $n$ of sets that are being multiplied.&lt;/p&gt;

&lt;p&gt;When $n$ is infinite the product is &lt;strong&gt;infinitary&lt;/strong&gt; as opposed to &lt;strong&gt;finitary&lt;/strong&gt; for finite $n$.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\underbrace{\text{binary},\text{trinary},\cdots,n\text{-ary}}_{\text{finitary}}\cdots;\underbrace{\aleph_0\text{-ary},\aleph_1\text{-ary},\cdots}_{\text{infinitary}}&lt;/script&gt;

&lt;p&gt;All the definitions used above apply only to finite $n$. There &lt;em&gt;are&lt;/em&gt; valid notions of infinitary Cartesian products, but they are beyond the scope of this post.&lt;/p&gt;

&lt;h2 id=&quot;rules-of-replacement&quot;&gt;Rules of replacement&lt;/h2&gt;
&lt;h4 id=&quot;non-commutative&quot;&gt;Non-Commutative&lt;/h4&gt;
&lt;p&gt;The Cartesian product is a non-commutative operator:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;A\times B \not= B \times A&lt;/script&gt;

&lt;p&gt;This should be obvious as the ordered tuples $(a,b)\not=(b,a)$ where $a\in A$ and $b\in B$ via the definition of &lt;a href=&quot;/n-tuples#equality&quot;&gt;tuple equality&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&quot;non-associative&quot;&gt;Non-Associative&lt;/h4&gt;
&lt;p&gt;The Cartesian product is a non-associative operator:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\left(A\times B\right) \times C \not= A\times \left(B \times C\right)&lt;/script&gt;

&lt;p&gt;Similar to its non-commutativity, we can illustrate this by noting that $((a,b),c)\not= (a,(b,c))$ where $a\in A$, $b\in B$, and $c\in C$.&lt;/p&gt;

&lt;h4 id=&quot;left-associative&quot;&gt;Left-Associative&lt;/h4&gt;
&lt;p&gt;When faced with an $n$-ary Cartesian product, it is conventionally evaluated from left to right. That is to say the Cartesian product is left-associative:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;A\times B \times C=(A \times B)\times C&lt;/script&gt;

&lt;details&gt;
&lt;summary&gt;&lt;h2 class=&quot;inline&quot;&gt;Distributive Properties&lt;/h2&gt;&lt;/summary&gt;

&lt;h4&gt;Union&lt;/h4&gt;
$$A\times\left(B\cup C\right)=\left(A\times B\right)\cup \left(A\times C\right)$$

&lt;h4&gt;Intersection&lt;/h4&gt;
$$A\times\left(B\cap C\right)=\left(A\times B\right)\cap \left(A\times C\right)$$

$$\left(A\times B\right)\cap \left(C\times D\right)=\left(A\cap B\right)\times \left(C\cap D\right)$$

&lt;h4&gt;Set Difference&lt;/h4&gt;
$$A\times\left(B\setminus C\right)=\left(A\times B\right)\setminus \left(A\times C\right)$$

&lt;h4&gt;Subsets&lt;/h4&gt;
$$B\subseteq C\iff\left(A\times B\right)\subseteq \left(A\times C\right)$$

$$\left(A\times B\right)\subseteq \left(C\times D\right)\iff\left(A\subseteq B\right)\land \left(C\subseteq D\right)$$

&lt;h4&gt;Complements&lt;/h4&gt;
If $A$ and $B$ are memebers of some universal set $U$, then their absolute complement is denoted $A^C$ and $B^C$ respectively.

$$\left(A\times B\right)^C=\left(A^C\times B^C\right)\cup\left(A^C\times B\right)\cup\left(A\times B^C\right)$$
&lt;/details&gt;

&lt;h2 id=&quot;examples&quot;&gt;Examples&lt;/h2&gt;
&lt;h4 id=&quot;functions-and-relations&quot;&gt;Functions and Relations&lt;/h4&gt;
&lt;p&gt;&lt;a href=&quot;\relations&quot;&gt;Relations&lt;/a&gt;, and thus functions and operations, from a set $A$ to a set $B$ are defined as subsets of the Cartesian product $A\times B$. For example, the less than or equal to relation $\le$ on the integers can be defined as so:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\le\equiv\{\cdots,\left(1,2\right),\left(-14,0\right),\left(5,5\right),\left(12,13\right),\cdots\}&lt;/script&gt;

&lt;p&gt;&lt;em&gt;We can formalize this definition to set theory by comparing the cardinality of the sets represented by the naturals and further the integers.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;And so, because $10$ is less than $12$, the ordered pair $(10,12)\in\le$. We can write this more conventionally as $10\le12$.&lt;/p&gt;

&lt;h4 id=&quot;cardinal-multiplication&quot;&gt;Cardinal Multiplication&lt;/h4&gt;
&lt;p&gt;Cartesian products are used to define the product of two cardinal numbers:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\left|X\right|\cdot\left|Y\right|=\left|X\times Y\right|&lt;/script&gt;

&lt;h4 id=&quot;coordinates&quot;&gt;Coordinates&lt;/h4&gt;
&lt;p&gt;Every point in $2$-space represents an element of the Cartesian product of the Reals with themselves (i.e $\mathbb{R}^2$). This can be generalized for $n$-space with every point being an element of $\mathbb{R}^n$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbb{R}^n=\{(x_1,x_2,\cdots,x_n)\mid \forall k\in\mathbb{N}_n^* :   x_k\in\mathbb{R}\}&lt;/script&gt;

&lt;p&gt;&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/thumb/0/0e/Cartesian-coordinate-system.svg/354px-Cartesian-coordinate-system.svg.png?style=centerme&quot; alt=&quot;plane&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Indeed, the graphical representation above was dubbed the Cartesian plane after the same &lt;a href=&quot;https://en.wikipedia.org/wiki/René_Descartes&quot;&gt;Descartes&lt;/a&gt; that used it to represent the Cartesian product $\mathbb{R}\times\mathbb{R}$.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Points in $3$-space are necessary in describing the &lt;a href=&quot;/position&quot;&gt;position&lt;/a&gt; of objects and particles in space and thus set up the study of &lt;a href=&quot;\kinematics&quot;&gt;motion&lt;/a&gt;, the causes of that motion and, ultimately, the rest of physics.&lt;/p&gt;

&lt;p&gt;Moreover, $2$-space (and less frequently $3$-space) is used in plotting and making inferences from data as well as visualizing functions over an interval of numbers.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>On Culpability</title>
   <link href="http://localhost:4000/on-culpability/"/>
   <updated>2018-04-25T00:00:00-04:00</updated>
   <id>http://localhost:4000/on-culpability</id>
   <content type="html">&lt;h3 id=&quot;scenario-1&quot;&gt;Scenario 1&lt;/h3&gt;
&lt;p&gt;Consider a man driving home late after having a few too many drinks. While drunk he decides to run a red light. Nothing comes of it and he manages to arrive home safe and sound.&lt;/p&gt;

&lt;h3 id=&quot;scenario-2&quot;&gt;Scenario 2&lt;/h3&gt;
&lt;p&gt;Now imagine this same man, under the same circumstances; a man who had just as many drinks, a man who decided to drive while inebriated at the same late hour.&lt;/p&gt;

&lt;p&gt;This time he still decides to run the red light but, unlike in the other situation, he blindsides a little girl walking across the street. He puts on the brakes as soon as he sees her but, his reaction time being delayed, runs over and kills her.&lt;/p&gt;

&lt;!--more--&gt;

&lt;h3 id=&quot;result&quot;&gt;Result&lt;/h3&gt;
&lt;p&gt;Gruesome to be sure, but consider the repercussions of his actions in both cases. In one instance the man got home scot-free, with nothing more than a traffic ticket looming over his head.&lt;/p&gt;

&lt;p&gt;In the other instance however, he was charged and convicted of manslaughter.&lt;/p&gt;

&lt;p&gt;Both instances of the man made the same, poor decisions up to the point of the accident. And neither wanted to kill anybody that night. What then delineates these two situations? What makes the man a murderer in one situation and in the other just a drunk driver?&lt;/p&gt;

&lt;p&gt;Luck.&lt;/p&gt;

&lt;h3 id=&quot;why&quot;&gt;Why?&lt;/h3&gt;
&lt;p&gt;If you were to ask someone whether the one who ran the red-light with no consequence or the one who ran over a pedestrian was more guilty, you would have good reason to believe they would say the pedestrian killer. And even if we were to concede that, at least by intent, the two drunk men were equally culpable, it still just doesn’t sit right with us. How could it be that a murderer is just as evil as a reckless driver? Does it seem wrong that one be locked up for life and the other lose a star on their driver’s license?&lt;/p&gt;

&lt;p&gt;Well… I don’t know. Why is it that a 17 year old, a day before his birthday, isn’t fully responsible for his actions but once he is 18 is considered independent by the law? How can anybody be culpable if they are a product of their parents, environment, genetics, etc? What is culpability?&lt;/p&gt;

&lt;!-- HOW CAN CULPABILITY EXITTS WITHOUT FREE WILL --&gt;

&lt;p&gt;Well someone has to be culpable. You have to draw the line in the sand somewhere. Why? Well it seems that punishment may not be dealt out to punish those who have evil intent but, instead, to make an affirmation that the effects their intent caused are unwanted.&lt;/p&gt;

&lt;p&gt;When a court decides that the pedestrian killer should serve 15 years in jail and the red light offender shouldn’t, they may very well be considering his intent, but they also consider the effects his intent has caused, whether it be in his control or not. A judiciary in a government, I’d say, serves to remove and make examples of those who do not follow the norms of society and who may pose danger to it. Whether or not they are more culpable then the next driver they still killed someone and so we make sure to discourage this behavior not because we care for their intent but because we care for their effects.&lt;/p&gt;

&lt;p&gt;“But wait”, many have objected, “punishing people for their actions in this way rather than offering support to correct their behavior is a misguided tactic in reducing these unwanted effects (stealing, murder, crime, etc.).” And indeed, these objectors may very well be right. So then is there another reason that such methods are employed if more effective ones exist? Is there something fundamental about punishing others for actions a society does not like? I think so.&lt;/p&gt;

&lt;p&gt;It boils down to our own feelings and behaviors. &lt;em&gt;‘Our’&lt;/em&gt; meaning human. Of course it doesn’t feel right to punish the two men in the same way, whether we lock them both up or simply take a star from both of their licenses. We &lt;em&gt;FEEL&lt;/em&gt; like they must be dealt with differently. It is ultimately our own instincts that drive our behavior as a society. The system of justice we have today is not the product not careful reasoning, but an ad-hoc solution. One begot from evolutionary processes. Processes that took time immemorial just to get a working solution, much less an optimal one…&lt;/p&gt;

&lt;!-- MORALITY TOO IS NOTHING BUT A FARCE TO COVER UP OUR INSTINCTS --&gt;

&lt;!-- EVOLTION IS A HIGYH LEVEL LOCAL OPTIMIZATION PROBLEM --&gt;
</content>
 </entry>
 
 <entry>
   <title>n-Tuples</title>
   <link href="http://localhost:4000/n-tuples/"/>
   <updated>2018-04-23T00:00:00-04:00</updated>
   <id>http://localhost:4000/n-tuples</id>
   <content type="html">&lt;p&gt;An $n$-tuple is an ordered list of $n$ elements. It is dissimilar to a set in that the order of its elements matter, it must be finite, and it can contain multiples of the same element.&lt;/p&gt;

&lt;p&gt;Here are some common names for tuples of different size:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;$n$-tuple&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Name(s)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;$0$&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;null-tuple, $0$-tuple&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;$1$&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;singleton, $1$-tuple&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;$2$&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;ordered pair, $2$-tuple&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;$3$&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;ordered triplet, $3$-tuple&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;$n$&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;$n$-tuple&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;em&gt;My &lt;a href=&quot;https://medium.com/@ozanerhansha/the-ordered-pair-and-set-theory-69aa6e2b8a32&quot;&gt;medium article&lt;/a&gt; on ordered pairs and their uses.&lt;/em&gt;&lt;/p&gt;

&lt;!--more--&gt;

&lt;h2 id=&quot;definition&quot;&gt;Definition&lt;/h2&gt;
&lt;h4 id=&quot;2-tuples&quot;&gt;$2$-tuples&lt;/h4&gt;
&lt;p&gt;We define ordered pairs, or $2$-tuples, as &lt;a href=&quot;https://en.wikipedia.org/wiki/Kazimierz_Kuratowski&quot;&gt;Kuratowski&lt;/a&gt; did:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;(x_1,x_2)=\{\{x_1\},\{x_1,x_2\}\}&lt;/script&gt;

&lt;p&gt;The above definition distinguishes $a$ from $b$ in that both sets in the right set contain $a$ but only one contains $b$. This is what gives the pair order.&lt;/p&gt;

&lt;h4 id=&quot;n-tuples&quot;&gt;$n$-tuples&lt;/h4&gt;
&lt;p&gt;Using the $2$-tuple, we can define all $n$-tuples recursively as an ordered pair of an $(n-1)$-tuple and another element. The $3$-tuple, for example, would be:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
(x_1,x_2,x_3)&amp;=((x_1,x_2),x_3)\\
&amp;=\{\{(x_1,x_2)\},\{(x_1,x_2),x_3\}\}\\
&amp;=\{\{\{\{x_1\},\{x_1,x_2\}\}\},\{\{\{x_1\},\{x_1,x_2\}\},x_3\}\}\\
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;And in general, an $n$-tuple for $n&amp;gt;2$ is defined as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;(x_1,x_2,\cdots,x_{n-1},x_n)=((x_1,x_2,\cdots,x_{n-1}),x_n)&lt;/script&gt;

&lt;p&gt;&lt;em&gt;While the above definition is standard, it lacks a notion of a $0$ and $1$-tuple. See &lt;a href=&quot;https://en.wikipedia.org/wiki/Tuple#Tuples_as_nested_sets&quot;&gt;this&lt;/a&gt; for a different definition of $n$-tuples that uses the null-tuple as a base case rather than ordered pairs. I can’t find a use for them that justifies their complication to the definition of $n$-tuple, so I excluded them.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Alternatively, $n$-tuples can also be defined as functions with domains over some finite interval of the positive integers. In this sense, they would be equivalent to finite sequences.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Whatever definition is used, all that matters is that two $n$-tuples are equivalent &lt;em&gt;only&lt;/em&gt; when the elements at each of their indices are equivalent. This is their defining property.&lt;/p&gt;

&lt;h4 id=&quot;existence-in-zfc&quot;&gt;Existence in ZFC&lt;/h4&gt;
&lt;p&gt;The existence of an ordered pair of elements in ZFC can be proved using the axiom of pairing twice over. Once to prove that $ \{ a \} $ and $ \{ a,b \} $ exist and again to pair them with each other.&lt;/p&gt;

&lt;p&gt;And since, $n$-tuples are simply nested ordered pairs, they too must exist.&lt;/p&gt;

&lt;h2 id=&quot;equality&quot;&gt;Equality&lt;/h2&gt;
&lt;p&gt;Two ordered pairs are equivalent if and only if the elements in each of their respective indices are equal. That is to say, for two $n$-tuples $X$ and $Y$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
X&amp;=(x_1,x_2,x_3,\cdots,x_n)\\
Y&amp;=(y_1,y_2,y_3,\cdots,y_n)
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;$X$ and $Y$ are only equal if:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
X=Y&amp;\equiv\forall k\in\mathbb{N}_n^ * \ (x_k=y_k)\\
&amp;\equiv x_1=y_1 \land x_2=y_2 \land \cdots \land x_n=y_n\\
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;&lt;em&gt;Where &lt;a href=&quot;/natural-numbers#notation&quot;&gt;$\mathbb{N}_n^ * $&lt;/a&gt; is the set of all positive integers from and including $1$ to $n$.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Note that for this definition to work, the two tuples have to be the same size. As such, tuples of different sizes will never be equal.&lt;/p&gt;

&lt;h2 id=&quot;extraction&quot;&gt;Extraction&lt;/h2&gt;
&lt;h4 id=&quot;first-element&quot;&gt;First Element&lt;/h4&gt;
&lt;p&gt;To extract the first element $\pi_1(P)$ of an ordered pair $P=(a,b)$ we can use the following construction:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\pi_1(P)=\bigcup\bigcap P=a&lt;/script&gt;

&lt;details&gt;&lt;summary&gt;Proof&lt;/summary&gt;
  &lt;b&gt;Lemma 1&lt;/b&gt;
  &lt;p&gt;To make proving the above statement easier, it would help to prove that the arbitrary union of a set of an element $\{x\}$ is that element $x$:

  $$\bigcup \{x\}=x$$

  First let's start with the definition of the arbitrary union of a set $S$:

  $$\bigcup S=\{a\mid \left(\exists b\in S\right)a\in b\}$$

  In in English this means, all elements $a$ that are contained in at least one set $b$ that are contained in $S$. (i.e the union of all the elements in $S$). Plugging $\{x\}$ in for $S$ we see:

  $$\bigcup \{x\}=\{a\mid \left(\exists b\in \{x\}\right)a\in b\}$$

  Since there is only one element in $\{x\}$, namely $x$, there is only one set $b$ could be: $x$. So, we can say the following:

  $$\bigcup \{x\}=\{a\mid a\in x\}$$

  And since the set of all elements in $x$ is simply that same set:

  $$\boxed{\bigcup \{x\}=x}$$
  &lt;/p&gt;
  &lt;b&gt;The Proof&lt;/b&gt;
  &lt;p&gt;

  $$\begin{align}
  \pi_1(P)&amp;amp;=\bigcup\bigcap P\\
  &amp;amp;=\bigcup\bigcap \{\{a\},\{a,b\}\}\\
  &amp;amp;=\bigcup \left(\{a\}\cap\{a,b\}\right)\\
  \end{align}$$

  Of course the only element in common between $\{a\}$ and $\{a,b\}$ is $a$ so:

  $$\pi_1(P)=\bigcup \{a\}$$

  And from Lemma 1, we know this equals:

  $$\pi_1(P)=\bigcup \{a\}=a$$

  And indeed, $a$ is the first element of the ordered pair $P$.
  &lt;/p&gt;
&lt;/details&gt;

&lt;h4 id=&quot;second-element&quot;&gt;Second Element&lt;/h4&gt;
&lt;p&gt;The second element $\pi_2(P)$ of an ordered pair $P=(a,b)$ can be found like so:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\pi_2(P)=\bigcup\{x\in\bigcup P\mid\bigcup P\not=\bigcap P\implies x\not\in\bigcap P\}=b&lt;/script&gt;

&lt;details&gt;&lt;summary&gt;Proof&lt;/summary&gt;
I'll do it later...
&lt;/details&gt;

&lt;h4 id=&quot;extracting-elements-of-n-tuples&quot;&gt;Extracting Elements of $n$-Tuples&lt;/h4&gt;
&lt;p&gt;You may have noticed that the above definitions only apply to $2$-tuples. What about $n$-tuples? Well, as it turns out, we can extract the elements of an $n$-tuple of any size by recursively using the $\pi_1$ and $\pi_2$ functions we defined above.&lt;/p&gt;

&lt;p&gt;The notation we’ll use is as follows: $\pi^n_a(P)$ is the $a$th element of the $n$-tuple $P=\left(x_1,x_2,x_3,\cdots,x_n\right)$.&lt;/p&gt;

&lt;p&gt;The first element of $P$ can be found like so:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\pi^n_1(P)=\underbrace{\pi_1\circ\cdots\circ\pi_1}_{n-1\text{ iterations}}(P)=x_1&lt;/script&gt;

&lt;p&gt;For any element &lt;em&gt;other&lt;/em&gt; than the first, we can use the following formula:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\pi^n_a(P)=\pi_2\circ\underbrace{\pi_1\circ\cdots\circ\pi_1}_{n-a\text{ iterations}}(P)=x_a&lt;/script&gt;

&lt;details&gt;&lt;summary&gt;&quot;Proof&quot; &amp;amp; Intuition&lt;/summary&gt;
Not really a proof, I just wrote down how to find the elements of $2,3,4,5$-tuples and found the pattern:

$$\begin{align}
  &amp;amp;2\text{-tuple}\left\{
    \begin{array}{l}
      \pi^2_1=\pi_1(P)\\
      \pi^2_2=\pi_2(P)\\
    \end{array}
  \right.\\
  &amp;amp;3\text{-tuple}\left\{
    \begin{array}{l}
      \pi^3_1=\pi_1\left(\pi_1\left(P\right)\right)\\
      \pi^3_2=\pi_2\left(\pi_1\left(P\right)\right)\\
      \pi^3_3=\pi_2\left(P\right)\\
    \end{array}
  \right.\\
  &amp;amp;4\text{-tuple}\left\{
    \begin{array}{l}
      \pi^4_1=\pi_1\left(\pi_1\left(\pi_1\left(P\right)\right)\right)\\
      \pi^4_2=\pi_2\left(\pi_1\left(\pi_1\left(P\right)\right)\right)\\
      \pi^4_3=\pi_2\left(\pi_1\left(P\right)\right)\\
      \pi^4_4=\pi_2\left(P\right)\\
    \end{array}
  \right.\\
  &amp;amp;5\text{-tuple}\left\{
    \begin{array}{l}
      \pi^5_1=\pi_1\left(\pi_1\left(\pi_1\left(\pi_1\left(P\right)\right)\right)\right)\\
      \pi^5_2=\pi_2\left(\pi_1\left(\pi_1\left(\pi_1\left(P\right)\right)\right)\right)\\
      \pi^5_3=\pi_2\left(\pi_1\left(\pi_1\left(P\right)\right)\right)\\
      \pi^5_4=\pi_2\left(\pi_1\left(P\right)\right)\\
      \pi^5_5=\pi_2\left(P\right)\\
    \end{array}
  \right.\\
\end{align}$$

Why are is there a conditional definition of the $a$th element of an $n$-tuple? What causes this asymmetry? Well it must be the fact that our base case in defining $n$-tuples was the ordered pair rather than some sort of $1$-tuple. Although it's possible that starting with a $1$-tuple wouldn't change this conditional...
&lt;/details&gt;

&lt;h2 id=&quot;some-uses&quot;&gt;Some Uses&lt;/h2&gt;
&lt;h4 id=&quot;cartesian-product&quot;&gt;&lt;a href=&quot;/cartesian-product&quot;&gt;Cartesian Product&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;Ordered pairs are necessary in defining the Cartesian product, which in turn are used to define relations, functions, coordinates, etc.&lt;/p&gt;

&lt;h4 id=&quot;mathematical-structures&quot;&gt;Mathematical Structures&lt;/h4&gt;
&lt;p&gt;Tuples are often used to encapsulate sets along with some operator or relation into a complete mathematical structure. One example is a graph which is defined as an ordered pair $G=(V,E)$ where $V$ is a set of vertices and $E$ a set of edges connecting those vertices. Another example is a group which is defined as an ordered pair $G=(S,\cdot)$ where $\cdot$ is some binary operation on the elements of $S$. Tuples are also used to encapsulate rings, fields, vector spaces, topological spaces, ordered sets, and so on.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Superfactorial and Hyperfactorial</title>
   <link href="http://localhost:4000/superfactorial-and-hyperfactorial/"/>
   <updated>2018-04-18T00:00:00-04:00</updated>
   <id>http://localhost:4000/superfactorial-and-hyperfactorial</id>
   <content type="html">&lt;h2 id=&quot;factorial&quot;&gt;Factorial&lt;/h2&gt;
&lt;p&gt;The factorial function $n!$ is the product of the first $n$ &lt;a href=&quot;/natural-numbers&quot;&gt;natural numbers&lt;/a&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;n!=\prod_{k=1}^{n}k=1\times2\times3\times\cdots\times n&lt;/script&gt;

&lt;p&gt;&lt;em&gt;The factorials are sequence &lt;a href=&quot;https://oeis.org/A000142&quot;&gt;A000142&lt;/a&gt; in the OEIS.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;It’s important to note that $0!$ is defined as $1$ because it is an &lt;a href=&quot;https://en.wikipedia.org/wiki/Empty_product&quot;&gt;empty product&lt;/a&gt;. This is not an arbitrary definition and in fact simplifies many formulas involving factorials.&lt;/p&gt;

&lt;p&gt;The function is a mapping from the natural numbers to itself:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;!:\mathbb{N}\rightarrow\mathbb{N}&lt;/script&gt;

&lt;p&gt;&lt;em&gt;Using &lt;a href=&quot;https://en.wikipedia.org/wiki/Analytic_continuation&quot;&gt;analytic continuation&lt;/a&gt;, the factorial function can be generalized to complex numbers. The resulting function is dubbed the $\Gamma(z)$ &lt;a href=&quot;https://en.wikipedia.org/wiki/Gamma_function&quot;&gt;gamma function&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;While there is much to discuss about the factorial function, this post concerns itself with two particular variations of the factorial. Namely the superfactorial and the hyperfactorial.&lt;/p&gt;

&lt;!--more--&gt;

&lt;h2 id=&quot;superfactorial&quot;&gt;Superfactorial&lt;/h2&gt;
&lt;p&gt;We denote the superfactorial of $n$ as $n{$}$. It is defined as the product of the first $n$ factorials:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;n$=\prod_{k=1}^{n}k!=1!\times2!\times3!\times\cdots\times n!&lt;/script&gt;

&lt;p&gt;&lt;em&gt;The superfactorials are sequence &lt;a href=&quot;https://oeis.org/A000178&quot;&gt;A000178&lt;/a&gt; in the OEIS.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;The above definition of the superfactorial was created by Neil J.A. Sloane and Simon Plouffe (co-authors of the OEIS). Another definition of superfactorials exists: $n{$}=n!\uparrow\uparrow n!$ where the double arrows denote &lt;a href=&quot;https://en.wikipedia.org/wiki/Tetration&quot;&gt;tetration&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;And, like the factorial function, it is a map from the naturals onto itself:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;$:\mathbb{N}\rightarrow\mathbb{N}&lt;/script&gt;

&lt;p&gt;&lt;em&gt;Like factorial, the superfactorial function can be generalized to the complex numbers, resulting in $G(z)$ the &lt;a href=&quot;https://en.wikipedia.org/wiki/Barnes_G-function&quot;&gt;Barnes G-function&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;hyperfactorial&quot;&gt;Hyperfactorial&lt;/h2&gt;
&lt;p&gt;The hyperfactorial of $n$ is denoted $H(n)$ and is defined as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;H(n)=\prod_{k=1}^{n}k^k=1^1\times2^2\times3^3\times\cdots\times n^n&lt;/script&gt;

&lt;p&gt;&lt;em&gt;The hyperfactorials are sequence &lt;a href=&quot;https://oeis.org/A002109&quot;&gt;A002109&lt;/a&gt; in the OEIS.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Again, we can consider this function to be a map from the naturals onto itself:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;H:\mathbb{N}\rightarrow\mathbb{N}&lt;/script&gt;

&lt;p&gt;&lt;em&gt;The hyperfactorial too can be generalized to the complex numbers. The resulting function is known as $K(z)$ the &lt;a href=&quot;https://en.wikipedia.org/wiki/K-function&quot;&gt;K-function&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;factorial-identity&quot;&gt;Factorial Identity&lt;/h2&gt;
&lt;p&gt;It is possible to relate all three of these factorial variants (factorial, superfactorial and hyperfactorial):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;n{$}\cdot H(n)=n!^{n+1}&lt;/script&gt;

&lt;details&gt;
&lt;summary&gt;&lt;h4 class=&quot;inline&quot;&gt;Proof&lt;/h4&gt;&lt;/summary&gt;

We can prove the above statement, which we'll call $P(n)$, by induction:

$$P(n)\equiv n{$}\cdot H(n)=n!^{n+1}$$

First we multiply both sides of the equation by $(n+1)!(n+1)^{n+1}$:

$$\begin{align}n{$}\cdot H(n)&amp;amp;=n!^{n+1}\\
(n+1)!(n+1)^{n+1}&amp;amp; \ \ \ \ \ (n+1)!(n+1)^{n+1}
\end{align}$$

Now let's simplify the left hand side first. Notice that $n{\$}\cdot(n+1)!=(n+1){\$}$ and that $H(n)\cdot(n+1)^{n+1}=H(n+1)$. From this the left hand side simply becomes:

$$(n+1){$}\cdot H(n+1)$$

Now let's deal with right hand side. Notice that the expression can be rewritten as:

$$\begin{align}
n!^{n+1} \color{green}{(n+1)!}(n+1)^{n+1}&amp;amp;=n!^{n+1}\color{green}{n!(n+1)}(n+1)^{n+1}\\
&amp;amp;=n!^{n+2}(n+1)^{n+2}\\
&amp;amp;=(n+1)!^{n+2}\\
\end{align}$$

Putting the right and left hand sides back together we can see that we just proved $P(n+1)$:

$$P(n+1)\equiv(n+1){$}\cdot H(n+1)=(n+1)!^{n+2}$$

However $P(n+1)$ was proved under the assumption that $P(n)$ was true. Thus:

$$P(n)\implies P(n+1)$$

But, notice that $P(1)$ is true:

$$\begin{align}
P(1)&amp;amp;\equiv 1{$}\cdot H(1)=(1!)^{1+1}\\
&amp;amp;\equiv1\cdot1=1\\
&amp;amp;\equiv T
\end{align}$$

Because $P(1)$ is true, this means that $P(1+1)=P(2)$ is true. This implies that $P(2+1)=P(3)$ is true and so on and so forth for all integers above $1$. Thus by induction:

$$\begin{align}
&amp;amp;P(n)\implies P(n+1)\\
&amp;amp;P(1)\\
\therefore\ &amp;amp;\hline{\forall n\in \mathbb{N},\ P(n)}\\
\end{align}$$
&lt;/details&gt;
</content>
 </entry>
 
 <entry>
   <title>The Dottie Number</title>
   <link href="http://localhost:4000/dottie-number/"/>
   <updated>2018-04-16T00:00:00-04:00</updated>
   <id>http://localhost:4000/dottie-number</id>
   <content type="html">&lt;p&gt;The Dottie number, which I will denote as $\textbf{d}$, is the only real solution to the following equation:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\cos x=x&lt;/script&gt;

&lt;p&gt;&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/thumb/9/98/Dottie_number.svg/800px-Dottie_number.svg.png?style=centerme&quot; alt=&quot;graph&quot; width=&quot;440px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The above is what’s known as a transcendental equation. Equations like these &lt;em&gt;usually&lt;/em&gt; return transcendental numbers and indeed $\textbf{d}$ is transcendental. It’s decimal expansion is as follows:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\textbf{d}=0.7390851332151606...&lt;/script&gt;

&lt;p&gt;&lt;em&gt;The Dottie number is sequence &lt;a href=&quot;https://oeis.org/A003957&quot;&gt;A003957&lt;/a&gt; in the OEIS.&lt;/em&gt;&lt;/p&gt;

&lt;!--more--&gt;

&lt;h3 id=&quot;fixed-point&quot;&gt;Fixed Point&lt;/h3&gt;
&lt;p&gt;$\textbf{d}$ is what’s called a fixed point of $\cos x$, because the cosine function maps $\textbf{d}$ to itself. As a result, repeatedly taking the cosine of $\textbf{d}$ returns the same result:&lt;/p&gt;

&lt;!-- $$\cos \textbf{d} = \textbf{d} \implies (\forall n\in\mathbb{N})\  \underbrace{\cos\circ\cos\circ\cdots\circ\cos}_{n}\ \textbf{d}=\textbf{d}$$ --&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\cos \textbf{d} = \textbf{d} \implies \forall n\in\mathbb{N}\  \left(\underbrace{\cos\circ\cos\circ\cdots\circ\cos}_{n}\ \textbf{d}=\textbf{d}\right)&lt;/script&gt;

&lt;p&gt;$\textbf{d}$ is the $\cos$ function’s only &lt;em&gt;real&lt;/em&gt; fixed point, but there exists infinitely many solutions to $\cos z=z$ for the complex numbers. Those solutions, however, are not attractors.&lt;/p&gt;

&lt;h3 id=&quot;universal-attractor&quot;&gt;Universal Attractor&lt;/h3&gt;
&lt;p&gt;What’s interesting about $\textbf{d}$ is that it’s not just the real fixed point of $\cos$ but also its &lt;strong&gt;universal fixed point attractor&lt;/strong&gt;. That is to say, if you take the cosine of any real number and repeatedly take the cosine of the result, you will always approach $\textbf{d}$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\forall x\in\mathbb{R} \left(\lim_{n\to\infty} \underbrace{\cos\circ\cos\circ\cdots\circ\cos}_{n}\ x=\textbf{d}\right)&lt;/script&gt;

&lt;p&gt;In fact, the above is true for a certain range of the complex numbers as well. This range forms the &lt;a href=&quot;https://en.wikipedia.org/wiki/Julia_set&quot;&gt;&lt;em&gt;Julia Set&lt;/em&gt;&lt;/a&gt; of $\cos z$.&lt;/p&gt;

&lt;details&gt;
&lt;summary&gt;&lt;h3 class=&quot;inline&quot;&gt;Proof of Transcendence &lt;/h3&gt;&lt;/summary&gt;

&lt;h4&gt;LWT&lt;/h4&gt;&lt;p&gt;
To prove $\textbf{d}$'s transcendence, we'll need to make use of the &lt;b&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Lindemann%E2%80%93Weierstrass_theorem&quot;&gt;Lindemann–Weierstrass theorem&lt;/a&gt;&lt;/b&gt; (LWT) which states:

$$\forall x\in \mathbb{A}\left(x\not= 0\implies e^x\notin\mathbb{A} \right)$$

Or in English: $e$ to the power of any non-zero algebraic number is not algebraic (i.e transcendental).
&lt;/p&gt;

&lt;h4&gt;Lemma 1&lt;/h4&gt;&lt;p&gt;
The proof will be easier if we first establish the following identity:

$$\begin{align*}
\sin^2 \textbf{d}+\cos^2 \textbf{d} = 1 \tag{Pythagorean theorem}\\
\sin^2 \textbf{d}+ \textbf{d}^2 = 1 \tag{\(\textbf{d}\) is a fixed point}\\
\sin \textbf{d} = \sqrt{1-\textbf{d}^2}
\end{align*}$$
&lt;/p&gt;

&lt;h4&gt;The Proof&lt;/h4&gt;&lt;p&gt;
Now we can prove $\textbf{d}$'s transcendence using Lemma 1 and LWT:

$$\begin{align*}
e^{i\textbf{d}}&amp;amp;=\cos \textbf{d} + i \sin \textbf{d} \tag{Euler's formula}\\
&amp;amp;=\textbf{d}+i\sin \textbf{d} \tag{\(\textbf{d}\) is a fixed point}\\
&amp;amp;=\textbf{d}+i\sqrt{1-\textbf{d}^2} \tag{Lemma 1}
\end{align*}
$$

$$\boxed{e^{i\textbf{d}}=\textbf{d}+i\sqrt{1-\textbf{d}^2}}$$

Let us assume that $\textbf{d}$ is algebraic. If this is the case then:

$$\left(\textbf{d}+i\sqrt{1-\textbf{d}^2}\right) \in \mathbb{A}$$

This is because it consists solely of algebraic numbers $\left(\textbf{d},i,1\right)$ and basic algebraic operations $\left(+,-,\times,x^2,\sqrt{x}\right)$ and thus must be root of a polynomial with rational coefficients (i.e algebraic).&lt;p&gt;&lt;/p&gt;

However, also assuming $\textbf{d}$ is algebraic, LWT tells us:

$$e^{i\textbf{d}}\notin \mathbb{A}$$

Since the right side of the boxed equation is algebraic yet LWT guarantees that the left side is transcendental (because $i\textbf{d}$ is algebraic), we are left with a contradiction. Meaning our initial assumption, that $\textbf{d}$ is algebraic, was false. Via &lt;i&gt;reductio ad absurdum&lt;/i&gt; we can conclude:

$$\begin{align}
&amp;amp;e^{i\textbf{d}}=\textbf{d}+i\sqrt{1-\textbf{d}^2} \tag{Euler's formula}\\
&amp;amp;e^{i\textbf{d}}\notin \mathbb{A} \tag{LWT}\\
&amp;amp;\left(\textbf{d}+i\sqrt{1-\textbf{d}^2}\right) \in \mathbb{A} \tag{def. of algebraic number}\\
\therefore\ &amp;amp;\hline{\textbf{d}\notin \mathbb{A}} \tag{q.e.d}\\
\end{align}$$

&lt;/p&gt;&lt;/details&gt;

&lt;h3 id=&quot;kaplans-series&quot;&gt;Kaplan’s Series&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://www.maa.org/sites/default/files/Kaplan2007-131105.pdf&quot;&gt;Kaplan&lt;/a&gt; proved that $\textbf{d}$ is equivalent to the following series:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\textbf{d} = \sum_{n=0}^{\infty}\ g^{\left(n\right)}\left(\frac{\pi}{2}\right)\frac{\left(-\pi\right)^n}{2^nn!}&lt;/script&gt;

&lt;p&gt;Where the $g^{\left(n\right)}$ is the $n$th derivative of $f^{-1}$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;g^{\left(n\right)}\left(x\right)=\frac{d^n}{dx^n}f^{-1}\left(x\right)&lt;/script&gt;

&lt;p&gt;and $f^{-1}$ is the inverse of the function $f$ which is defined as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f\left(x\right)=x-\cos x&lt;/script&gt;

&lt;p&gt;&lt;em&gt;We define it in this roundabout way because there is no explicit definition of $f^{-1}\left(x\right)$. This makes the construction of Kaplan’s series all the more interesting.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Written out, the series looks something like this:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\textbf{d} = \frac{\pi}{4} - \frac{\pi^3}{768} - \frac{\pi^5}{61440} - \frac{43\pi^7}{165150720} - \cdots&lt;/script&gt;

&lt;p&gt;We can write this more succinctly (and as Kaplan originally proved) as so:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\textbf{d} = \sum_{n=0}^{\infty}a_n\pi^{2n+1}&lt;/script&gt;

&lt;p&gt;Where $a_n$ is a sequence of rational numbers found by solving the above equation.&lt;/p&gt;

&lt;!-- Proof of Kaplan's Series --&gt;
&lt;details&gt;
  &lt;summary&gt;&lt;h3 class=&quot;inline&quot;&gt;Proof of Kaplan's Series&lt;/h3&gt;&lt;/summary&gt;
  &lt;h4&gt;Taylor Series of $f^{-1}$&lt;/h4&gt;
  &lt;p&gt;
    To start off with, consider a function $f$ that is defined as such:

    $$f\left(x\right)=x-\cos x$$

    Kaplan was able to construct his series by noticing a few interesting properties of this function and it's inverse $f^{-1}$ (which has no explicit definition). The first of which was it's zero:

    $$\begin{align}
    f\left(\textbf{d}\right)&amp;amp;=\textbf{d}-\cos \textbf{d}\\
    &amp;amp;= \textbf{d}-\textbf{d}\\
    &amp;amp;= 0
    \end{align}$$

    This implies the following about $f^{-1}$ it's inverse:

    $$f^{-1}\left(0\right) = \textbf{d}$$

    And before we move on let's rename $f^{-1}$ to $g$ to make things less cluttered:

    $$f^{-1}\left(x\right) = g\left(x\right)$$

    We now have an expression for $\textbf{d}$. It is simply the value of $g\left(0\right)$. We currently do not have an explicit definition for $g\left(x\right)$ but we can create one via a Taylor series:

    $$g\left(x\right)=\sum_{n=0}^{\infty}g^{\left(n\right)}\left(c\right)\frac{\left(x-c\right)^n}{n!}$$

    Where $g^{\left(n\right)}\left(x\right)$ is the $n$th derivative of $g\left(x\right)$ and $c$ is the point we are constructing the Taylor series about. Since we are letting $n\to\infty$ the choice of constant won't affect the outcome.
  &lt;/p&gt;

  &lt;!-- Fixed Point --&gt;
  &lt;h4&gt;Fixed point of $f$ and $g$&lt;/h4&gt;
  &lt;p&gt;
    So now let us choose a value of $c$ that will be easy to compute. Notice that:

    $$\begin{align}
    f\left(\frac{\pi}{2}\right)&amp;amp;=\frac{\pi}{2}-\cos \frac{\pi}{2}\\
    &amp;amp;= \frac{\pi}{2}-0\\
    &amp;amp;= \frac{\pi}{2}
    \end{align}$$

    This means that $\frac{\pi}{2}$ is a fixed point of $f$ and that the following is also true of its inverse $g$:

    $$g\left(\frac{\pi}{2}\right)=\frac{\pi}{2}$$
  &lt;/p&gt;

  &lt;!-- nth Derivative of f --&gt;
  &lt;h4&gt;$n$th derivative of $f$&lt;/h4&gt;
  &lt;p&gt;
    Also notice that finding the $n$th derivative of $f$ at $\frac{\pi}{2}$ is simple:

    $$\begin{align}
    f\left(x\right)&amp;amp;=x-\cos x\\
    f'\left(x\right)&amp;amp;=1+\sin x\\
    f''\left(x\right)&amp;amp;=\cos x\\
    &amp;amp;\vdots\\
    \left(\forall n&amp;gt;1\right)\ f^{(n)}\left(x\right)&amp;amp;=\frac{d^{n-2}}{dx^{n-2}}\cos x
    \end{align}$$

    Because the derivatives of $\cos x$ are cyclical, we only need to evaluate the next 3 derivatives after $f''\left(\frac{\pi}{2}\right)$. Doing this we can see the pattern:

    $$f^{(n)}\left(\frac{\pi}{2}\right)=\{\frac{\pi}{2}, 2,0,-1,0,1,0,-1,\cdots\}$$
  &lt;/p&gt;

  &lt;!-- nth Derivative of g --&gt;
  &lt;h4&gt;$n$th derivative of $g$&lt;/h4&gt;
  &lt;p&gt;
    Now knowing the $n$th derivative of $f$ at $\frac{\pi}{2}$, we can calculate the $n$th derivative of $g$ at $\frac{\pi}{2}$:

    $$\begin{align}
    f\left(g\left(x\right)\right)=x \tag{inverse func.}\\
    f'\left(g\left(x\right)\right)g'\left(x\right)=1 \tag{chain rule}\\
    g'\left(x\right)=\frac{1}{f'\left(g\left(x\right)\right)}
    \end{align}$$

    We can use the chain and product rules repeatedly to find the $n$th derivative of $g$. The second derivative, for example, can be computed by differentiating both sides of the above equation:

    $$\begin{align}
    f'\left(g\left(x\right)\right)g'\left(x\right)=1\\
    f'(g(x))g''(x) + f''(g(x))g'(x)^2 = 0\\
    f'(g(x))g''(x) = - f''(g(x))g'(x)^2\\
    g''(x) = \frac{-f''(g(x))g'(x)^2}{f'(g(x))}
    \end{align}$$

    &lt;i&gt;Repeated use of the chain rule can be generalized via &lt;a href=&quot;https://en.wikipedia.org/wiki/Faà_di_Bruno%27s_formula&quot;&gt;Faà di Bruno's formula.&lt;/a&gt;&lt;/i&gt;
  &lt;/p&gt;

  &lt;h4&gt;Solving the Taylor Series&lt;/h4&gt;
  &lt;p&gt;
    Using $\frac{\pi}{2}$ as our value of $c$, because $g^{(n)}(\frac{\pi}{2})$ is easy to compute, we can rewrite the Taylor series for $g$ as so:

    $$g\left(x\right)=\sum_{n=0}^{\infty}g^{\left(n\right)}\left(\frac{\pi}{2}\right)\frac{\left(x-\frac{\pi}{2}\right)^n}{n!}$$

    Since we are solving for $g(0)$ which equals $\textbf{d}$ we can plug it into the above series to arrive at:

    $$\textbf{d}=\sum_{n=0}^{\infty}g^{\left(n\right)}\left(\frac{\pi}{2}\right)\frac{\left(-\pi\right)^n}{2^nn!}$$

    Now we just have to solve for each of the terms in this sequence.

    &lt;details&gt;
      &lt;summary&gt;The zeroth term is equal to:&lt;/summary&gt;
      $$g\left(\frac{\pi}{2}\right)\frac{\left(-\pi\right)^0}{2^00!}=\frac{\pi}{2}$$
    &lt;/details&gt;

    &lt;details&gt;
      &lt;summary&gt;The first term is equal to:&lt;/summary&gt;
      $$g'\left(\frac{\pi}{2}\right)\frac{\left(-\pi\right)^1}{2^11!}=\frac{-\pi}{4}$$

      Because $g'\left(\frac{\pi}{2}\right)$ can be found by plugging $\frac{\pi}{2}$ into the equation we solved earlier:

      $$\begin{align}
      g'\left(\frac{\pi}{2}\right)&amp;amp;=\frac{1}{f'\left(g\left(\frac{\pi}{2}\right)\right)}\\
      &amp;amp;=\frac{1}{f'\left(\frac{\pi}{2}\right)}\\
      &amp;amp;=\frac{1}{2}\\
      \end{align}$$
    &lt;/details&gt;

    &lt;details&gt;
      &lt;summary&gt;The second term equals:&lt;/summary&gt;

      $$g''\left(\frac{\pi}{2}\right)\frac{\left(-\pi\right)^2}{2^22!}=0$$

      Because $g''\left(\frac{\pi}{2}\right)$ can be found as such:

      $$\begin{align}
      g''(x) &amp;amp;= \frac{-f''(g(x))g'(x)^2}{f'(g(x))}\\
      &amp;amp;=\frac{-f''(\frac{\pi}{2})g'(\frac{\pi}{2})^2}{f'(\frac{\pi}{2})}\\
      &amp;amp;=-\frac{0 (\frac{1}{2})}{2}\\
      &amp;amp;=0
      \end{align}$$
    &lt;/details&gt;

    Putting these terms together we find the following sequence:

    $$\textbf{d} = \frac{\pi}{2} - \frac{\pi}{4} + 0 - \frac{\pi^3}{768} + 0 - \frac{\pi^5}{61440} - \cdots$$

    One thing to note here are that all the even derivatives of $g(\frac{\pi}{2})$ are always $0$ meaning we can ignore all the even terms of the sequence.
    &lt;p&gt;&lt;/p&gt;
    Another thing to note is that we can simplify the first two terms in the series:

    $$\frac{\pi}{2}-\frac{\pi}{4}=\frac{\pi}{4}$$

    This allows us to rewrite the series as such:

    $$\textbf{d} = \frac{\pi}{4} - \frac{\pi^3}{768} - \frac{\pi^5}{61440} - \cdots$$

    This is what allows us (and Kaplan) to state the following:

    $$\textbf{d} = \sum_{n=0}^{\infty}a_n\pi^{2n+1}$$

    Where $a_n$ is a sequence of rational numbers.
    &lt;p&gt;&lt;/p&gt;

    &lt;i&gt;As a side note, to prove this we assumed that $g$ is infinitely differentiable and that $0$ fell in its interval of convergence, two things necessary to create a Taylor series for it and plug $0$ into it. These two facts are indeed true, they just weren't proved above.&lt;/i&gt;
  &lt;/p&gt;
&lt;/details&gt;

&lt;h3 id=&quot;approximating-the-dottie-number&quot;&gt;Approximating the Dottie Number&lt;/h3&gt;
&lt;h4 id=&quot;solve-kaplans-series&quot;&gt;Solve Kaplan’s series&lt;/h4&gt;
&lt;p&gt;One way is to simply calculate a specified number of terms in Kaplan’s series and sum them.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\textbf{d} \approx \frac{\pi}{4} &amp;= 0.7\color{red}{854\cdots}\\
\textbf{d} \approx \frac{\pi}{4} - \frac{\pi^3}{768} &amp;= 0.7\color{red}{450\cdots}\\
\textbf{d} \approx \frac{\pi}{4} - \frac{\pi^3}{768} - \frac{\pi^5}{61440} &amp;= 0.7\color{red}{400\cdots}\\
\textbf{d} \approx \frac{\pi}{4} - \frac{\pi^3}{768} - \frac{\pi^5}{61440} - \frac{43\pi^7}{165150720} &amp;= 0.739\color{red}{2\cdots}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;While this can provide a decent approximation, it is a very time consuming process and doesn’t allow the approximator to skip ahead to a desired accuracy (i.e to within .01%). Moreover while this series converges to $\textbf{d}$, it does so relatively slowly. To get just 17 decimal places of accuracy, one would need to solve for 25 terms of the series:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\textbf{d}\approx 0.73908 51332 15160 64\color{red}{570 711495 ...}&lt;/script&gt;

&lt;h4 id=&quot;taylor-series-of-cosine&quot;&gt;Taylor Series of Cosine&lt;/h4&gt;
&lt;p&gt;Another way to approximate $\textbf{d}$ is to simply substitute Taylor polynomials of $\cos x$ for $\cos x = x$ and solve for the zero of the resulting polynomial:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\cos x = 1 - \frac{x^2}{2!} + \frac{x^4}{4!}- \frac{x^6}{6!}+\cdots&lt;/script&gt;

&lt;p&gt;For the &lt;strong&gt;second&lt;/strong&gt; degree Taylor polynomial:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
1-\frac{x^2}{2!}=x\\
-\frac{x^2}{2}-x+1=0
\end{align}&lt;/script&gt;

&lt;p&gt;Using the quadratic formula we find:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x = \sqrt{3}-1 = 0.73\color{red}{205\cdots}&lt;/script&gt;

&lt;p&gt;For the &lt;strong&gt;fourth&lt;/strong&gt; degree Taylor polynomial:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
1-\frac{x^2}{2!}+\frac{x^4}{4!}=x \\
\frac{x^4}{24}-\frac{x^2}{2}-x+1=0
\end{align}&lt;/script&gt;

&lt;p&gt;Using the &lt;a href=&quot;https://en.wikipedia.org/wiki/Quartic_function#General_formula_for_roots&quot;&gt;quartic formula&lt;/a&gt; we find:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x = 0.73\color{red}{557\cdots}&lt;/script&gt;

&lt;p&gt;However, after the 3rd term, we run into a problem. &lt;a href=&quot;https://en.wikipedia.org/wiki/Abel–Ruffini_theorem&quot;&gt;Abel’s impossibility theorem&lt;/a&gt; states that there is no generic solution to polynomial equations above degree 4. For these polynomials a root finding algorithm has to be applied to approximate the zeros of the function. But if we have to use an approximation (root finder) just to calculate our approximation (Taylor polynomial) we might as well use the root finder on the original function: $\cos x - x = 0$.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;You may have noticed I didn’t include an exact representation of the quartic equation above. This is because, even though there exists a generic solution to quartic polynomials, it is crazily complex and not worth using practically. Click the &lt;a href=&quot;https://upload.wikimedia.org/wikipedia/commons/9/95/Quartic_Formula.jpg&quot;&gt;quartic&lt;/a&gt; link to see the equation in its entirety&lt;/em&gt;&lt;/p&gt;

&lt;h4 id=&quot;general-root-finding-algorithm&quot;&gt;General Root Finding Algorithm&lt;/h4&gt;
&lt;p&gt;The only choice we have left is to use a general root, or zero, finding algorithm like &lt;a href=&quot;https://en.wikipedia.org/wiki/Newton%27s_method&quot;&gt;Newton’s method&lt;/a&gt; or the &lt;a href=&quot;https://en.wikipedia.org/wiki/Bisection_method&quot;&gt;bisection method&lt;/a&gt; to calculate $\textbf{d}$ to a given accuracy.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>On the Grounding of Ideas</title>
   <link href="http://localhost:4000/metaphysical-nightmare/"/>
   <updated>2018-04-09T00:00:00-04:00</updated>
   <id>http://localhost:4000/metaphysical-nightmare</id>
   <content type="html">&lt;h2 id=&quot;poorly-defined-concepts&quot;&gt;Poorly Defined Concepts&lt;/h2&gt;
&lt;p&gt;What is jealousy? What is a cat? What does it mean to be free? What &lt;em&gt;are&lt;/em&gt; you?&lt;/p&gt;

&lt;p&gt;These aren’t foreign questions to us. Jealousy is an emotion we feel when we envy someone for what they have, a cat is an animal with a tail and four paws, freedom is the state of being able to do what you please, and I am a human being with thoughts and emotions.&lt;/p&gt;

&lt;p&gt;But what is jealousy &lt;em&gt;really&lt;/em&gt;? What’s a cat &lt;em&gt;really&lt;/em&gt;? What does it &lt;em&gt;really&lt;/em&gt; mean to be human?&lt;/p&gt;

&lt;p&gt;There is no formal definition of these concepts. We just know them when we see them. I know what jealousy feels like, I know what being human feels like (I hope), and I know a cat when I see it.&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;“Alright,” you might say, “maybe humanity and jealousy are too lofty to state objectively, but can’t cats be formally defined? Surely a cat has a particular genome that defines it regardless of its outward appearance, right?”&lt;/p&gt;

&lt;p&gt;But in a world where cats evolved from another species via gradual mutations to individual base pairs in their DNA (not big leaps), where do we draw the line between cat and cave-cat, so to speak.&lt;/p&gt;

&lt;p&gt;Moreover is a picture of a cat a cat? It certainly represents the notion of a cat, but it doesn’t exactly have genetic material. Plus, there is no formal way to tell whether a given picture is of a cat or not. No formula, no physical law of cats. We can tell a cat, real or drawn, from another animal using our &lt;em&gt;intuition&lt;/em&gt;, whatever that is.&lt;/p&gt;

&lt;p&gt;“But it’s easy” you might say, “just look for a tail, 4 legs, pointy ears and some fur.” What about hairless cats? a cat drawn with no tail? a cat who’s ears are down, etc. There are so many exceptions that it’s a miracle we are even able to tell cat from houseplant!&lt;/p&gt;

&lt;h4 id=&quot;whats-wrong-with-being-poorly-defined&quot;&gt;What’s Wrong with being Poorly Defined?&lt;/h4&gt;
&lt;p&gt;Why is this a problem? Well, without a concrete definition, the statements we can make about a concept are limited and just as hazy as the definition.&lt;/p&gt;

&lt;p&gt;Take morality. Unlike a well defined concept, you can’t point to some mathematical construction or set of rules that it follows. I can’t tell you without a doubt whether a given action was ‘moral’ or not, nor can I even define morality.&lt;/p&gt;

&lt;p&gt;Killing is certainly immoral… right? Well, except when it’s in the defense of yourself or others. What about hitting a kid as punishment? We certainly wouldn’t find that acceptable today, but it was commonplace not too long ago. How can there be an objective definition of morality if it is riddled with exceptions and even evolves over time?&lt;/p&gt;

&lt;p&gt;A chemist can give you certainty that a particular solution will erode a metal because chemistry is a formal science with mathematical definitions of all its concepts. A psychologist, however, may only have a hazy idea of why you may feel a certain emotion (whatever an emotion is) and what could have caused it.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;That said, it’s not too far-fetched to imagine an operational definition of emotions based on the amount of certain neurotransmitters in one’s brain. But of course, this is a bit of an oversimplification.&lt;/em&gt;&lt;/p&gt;

&lt;h4 id=&quot;computation&quot;&gt;Computation&lt;/h4&gt;
&lt;p&gt;Computers are a prime example of this, as they can only deal with problems that are well defined (i.e programmable via a set of steps or &lt;strong&gt;algorithm&lt;/strong&gt;). Ask it to solve the quadratic function, model an atom, or even play music and it’ll do just fine. But ask it to judge how strong an argumentative essay is and it might run into trouble. Digital music is just a sound wave sampled at a high rate, easy. But what is a good essay, formally speaking?&lt;/p&gt;

&lt;p&gt;&lt;em&gt;This problem of doing shakily defined tasks on computers can be partially dealt with via &lt;strong&gt;machine learning&lt;/strong&gt;. And the underlying philosophy behind it’s effectiveness in the real world is called the &lt;a href=&quot;http://www.deeplearningbook.org/version-2015-10-03/contents/manifolds.html&quot;&gt;manifold hypothesis&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;well-defined-concepts&quot;&gt;Well Defined Concepts&lt;/h2&gt;
&lt;p&gt;To formally deal with any idea or concept, it has to be well defined. By making our definitions more concrete we can make more precise statements about these ideas. This is how math and science are born.&lt;/p&gt;

&lt;p&gt;Take the atom. An atom is a collection of protons, neutrons, and electrons which are further composed of elementary particles, all of which are governed by an exact set of equations (i.e quantum mechanics).&lt;/p&gt;

&lt;p&gt;A Cesium atom, for example, is thus completely mathematically defined. Everything about it is encapsulated in it’s wave function, no need to look at the natural world to understand it.&lt;/p&gt;

&lt;p&gt;This applies to all atoms, and thus all molecules, and further anything in the universe made of matter, like humans. This shouldn’t come as a surprise. After all, we know that math is the bedrock of science and thus of formal knowledge as a whole.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Of course, modeling even molecules (much less humans) as quantum wave functions is pretty much out of our reach computationally. As such, the claim that quantum wave functions can model all aspects of our lives is solely a theoretical one.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/philosophy/metaphysical-nightmare/chain_of_abstraction.png?style=centerme&quot; alt=&quot;chain of abstraction&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Take psychology. It is really just applied biology, which is just applied chemistry, which is just applied physics, which is just (as we’ve seen) applied mathematics.&lt;/p&gt;

&lt;p&gt;This &lt;em&gt;chain of abstraction&lt;/em&gt; applies to all fields of human knowledge, but its relation to psychology and the human mind will be particularly useful to us…&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://imgs.xkcd.com/comics/purity.png?style=centerme&quot; alt=&quot;xkcd&quot; /&gt;&lt;/p&gt;
&lt;center&gt;&lt;i&gt;Relevant xckd&lt;/i&gt;&lt;/center&gt;

&lt;h2 id=&quot;bridging-the-gap&quot;&gt;Bridging the Gap&lt;/h2&gt;
&lt;p&gt;Now consider the concept of love. It seems impossible to define it objectively right? Sure it’s an evolutionary aid to sexual reproduction, but that’s far from concrete and certainly ignores our own experience of it.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;We must first ask ourselves “what is love?”
    &lt;ul&gt;
      &lt;li&gt;Well it’s an emotion, right?&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Sure, but what’s an emotion?
    &lt;ul&gt;
      &lt;li&gt;A state of mind.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;But what is &lt;em&gt;“the mind”&lt;/em&gt;?
    &lt;ul&gt;
      &lt;li&gt;Well, the mind arises from the brain processing the information blasted into it via our senses.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;What is the brain made up of, and what comprises its state?
    &lt;ul&gt;
      &lt;li&gt;The brain is comprised of trillions of specialized cells called neurons. These neurons connect to form large &lt;em&gt;neural networks&lt;/em&gt; and communicate via electrical activity and chemicals known as neurotransmitters.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;And further, neurons are comprised of different organelle and so on until we reach atoms and ultimately the fundamental particles of the universe.&lt;/p&gt;

&lt;p&gt;This relation between the most fundamental (and very well defined) concepts in our universe, like elementary particles, to the most hazy and abstract ones, like emotions, is what will allow us the bridge the between poorly defined and well defined in a somewhat, admittedly, crude way.&lt;/p&gt;

&lt;h4 id=&quot;brain-graphs&quot;&gt;Brain Graphs&lt;/h4&gt;
&lt;p&gt;Consider the human brain. It is simply a collection of interconnected neurons, no? We can represent that neural network as graph. This graph will contain a bunch (billions) of nodes, each representing a neuron, and even more (trillions) of edges between those nodes, representing connections.&lt;/p&gt;

&lt;p&gt;Making our definition a bit more concrete, we can define any brain, in a single instant, as a graph of it’s neurons:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;B=\left(N,C\right)&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;$B$ is a graph of a brain.&lt;/li&gt;
  &lt;li&gt;$N$ is the set of all nodes in $B$ (neurons).&lt;/li&gt;
  &lt;li&gt;$C$ is the set of all connections between members of $N$ (neurons).&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;functions-of-time&quot;&gt;Functions of Time&lt;/h4&gt;
&lt;p&gt;Of course, a thought doesn’t occur in an instant, it happens over a small interval of time. So these brain graphs would actually be brain graph functions over time. Plugging in a time into the function would output the state of the brain graph at that moment in time.&lt;/p&gt;

&lt;p&gt;We can formalize this via:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;B(t)=\left(N,C(t)\right)&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;$B(t)$ is a function of the graph of a brain over time.&lt;/li&gt;
  &lt;li&gt;$N$ is the same as before (assuming the neurons in the brain stay constant during this thought)&lt;/li&gt;
  &lt;li&gt;$C$ is the function of the set of all connections between members of $N$ (neurons) over time.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;set-of-all-of-brain-graph-functions&quot;&gt;Set of all of Brain Graph Functions&lt;/h4&gt;
&lt;p&gt;Finally we can define an idea or concept as the set $S$ of all the possible brain graph functions that (we would consider) are thinking of that idea:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;S=\left\{B \in T\mid B \text{ is a brain that is thinking of S}\right\}&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;$S$ is a particular idea (represented by a set).&lt;/li&gt;
  &lt;li&gt;$B$ is a brain graph function. (The $(t)$ is omitted because we are just talking about the function itself).&lt;/li&gt;
  &lt;li&gt;$T$ is the set of all possible brain graph functions.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Notice that in this definition we are quantifying over all &lt;em&gt;possible&lt;/em&gt; brain graphs not just the ones that have existed or will exist. Thus, for example a cat:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\text{Cat}&amp;=\left\{B \in T\mid B \text{ is a brain that is thinking of cat}\right\}\\
\text{Cat}&amp;=\left\{B_1,B_2,B_3,B_4,\cdots\right\}
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;In the example above $B_1, B_2, B_3$ and so on are all different possible human brains that are thinking about cats.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Labeling the graphs with subscript $1,2,3,\cdots$ is misleading because such a set is most likely not countable.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;how-does-this-definition-even-help&quot;&gt;How does this Definition even Help?&lt;/h2&gt;
&lt;p&gt;Remember when I said formally defining concepts allows us to make formal statements about them? Well that was only partially true. While this ‘set of all possible particle configurations’ definition is more formal than your standard notion of jealousy, it’s not exactly helpful in trying to understand the nature of emotion of people’s interactions with each other.&lt;/p&gt;

&lt;p&gt;So then what’s the real point? The real point is purely a semantic one. A scientist/philosopher might like to associate some physical meaning to ideas like emotion or creativity. Thus, we can assure ourselves that everything imaginable can be considered physical (and further mathematical) in some sense. Even if that physicality is purely expressed as your brain thinking about said concept, it’s still just particles following the same physical laws.&lt;/p&gt;

&lt;h4 id=&quot;an-aside&quot;&gt;An Aside&lt;/h4&gt;
&lt;p&gt;The idea I presented above is, of course, ridiculous and riddled with problems. It is just meant to serve as a rough idea of how one might formalize the notion of an “idea” or “concept” when, at first, it may seem that they can only exist in our minds. Indeed these concepts do only exist in our minds, but our minds are purely physical (unless you’re a &lt;a href=&quot;https://plato.stanford.edu/entries/dualism/&quot;&gt;duelist&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;One such problem I would like to somewhat address is the apparent circularity of saying “the notion of cat is encompassed by the set of all brains that are thinking of the concept of cat.” This isn’t as bad as it sounds because it should really be phrased as “the notion of cat is encompassed by the set of all brains that are thinking of &lt;strong&gt;what we consider&lt;/strong&gt; the concept of cat.” This definition draws on our own subjective ideas of cats to objectively define them. (In this case &lt;em&gt;our&lt;/em&gt; might represent an average of all human opinions or something of the sort.)&lt;/p&gt;

&lt;p&gt;Another, more obvious, problem is that a certain concept or idea that occurs in a human mind may not be isolated in their brains. The human mind is a complex mechanism whose function is a product of one’s body, senses, and general environment. This problem can be solved by simply replacing the idea of brain graph with the set of cells that represent the human body or even set of particles/wavefunction that comprise that human body if necessary. The brain graph formulation I used above is simply for convenience.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Drag Force</title>
   <link href="http://localhost:4000/drag-force/"/>
   <updated>2018-03-27T00:00:00-04:00</updated>
   <id>http://localhost:4000/drag-force</id>
   <content type="html">&lt;h3 id=&quot;general-info&quot;&gt;General Info&lt;/h3&gt;
&lt;p&gt;The drag force, or fluid friction, is a force that opposes the motion of an object through a fluid/gas, similar to dry &lt;a href=&quot;\friction&quot;&gt;friction&lt;/a&gt;. It can also be used to describe the resistance between a fluid in another fluid.&lt;/p&gt;

&lt;p&gt;When the fluid is air, we refer to the force as &lt;em&gt;aerodynamic drag&lt;/em&gt; or &lt;strong&gt;air resistance&lt;/strong&gt;. When it’s water we call it &lt;em&gt;hydrodynamic drag&lt;/em&gt;.&lt;/p&gt;

&lt;h4 id=&quot;non-conservative-force&quot;&gt;Non-Conservative Force&lt;/h4&gt;
&lt;p&gt;Like friction, the drag force is a non-conservative force meaning the work done by it is path dependent. And, also like friction, drag converts some energy into heat, thus it does not conserve mechanical energy.
$\renewcommand{\vec}[1]{\mathbf{#1}}$&lt;/p&gt;

&lt;h3 id=&quot;drag-equation&quot;&gt;Drag Equation&lt;/h3&gt;
&lt;p&gt;The magnitude of the drag force is given by:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;F_D=\frac{\rho C_D A v^2}{2}&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;$F_D$ is the magnitude of the drag force.&lt;/li&gt;
  &lt;li&gt;$\rho$ is density of the fluid.&lt;/li&gt;
  &lt;li&gt;$C_D$ is the drag coefficient of the fluid.&lt;/li&gt;
  &lt;li&gt;$A$ is the cross-sectional area of the object (or more accurately the area of a projection of the object onto a plane perpendicular to the velocity).&lt;/li&gt;
  &lt;li&gt;$v$ is the magnitude of the velocity.&lt;/li&gt;
&lt;/ul&gt;

&lt;!--more--&gt;

&lt;p&gt;The direction of drag is opposite to that of the object’s velocity:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{\vec{F_D}}=-\hat{\vec{v}}&lt;/script&gt;

&lt;p&gt;Combining these two equations, we can write the drag force as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\vec{F_D}=\frac{-\rho C_D A v^2}{2}\hat{\vec{v}}=\frac{-\rho C_D Av}{2}\vec{v}&lt;/script&gt;

&lt;h4 id=&quot;power-to-overcome-drag&quot;&gt;Power to Overcome Drag&lt;/h4&gt;
&lt;p&gt;We can find the power required to overcome the drag force by the dot product of drag and the velocity of the object. And since the drag force is always oriented the same way as velocity, we can say:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P_d=\vec{F_D}\cdot\vec{v}=\frac{\rho C_DAv^3}{2}&lt;/script&gt;

&lt;h4 id=&quot;terminal-velocity&quot;&gt;Terminal Velocity&lt;/h4&gt;
&lt;p&gt;When an object is in free-fall in a liquid, gravity is counteracted by drag until it reaches an equilibrium. At this equilibrium the net force on the object, $\vec{F_{net}}$, equals zero and the object’s velocity stays constant:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/physics/terminal_velocity.png?style=centerme&quot; alt=&quot;terminalvel&quot; /&gt;&lt;/p&gt;

&lt;p&gt;By solving for the velocity when the sum of the forces equals zero, we arrive at the terminal velocity $v_t$:&lt;/p&gt;

&lt;details&gt;&lt;summary&gt;Derivation&lt;/summary&gt;&lt;p&gt;$$\begin{align*}
F_D-F_g=0 \tag{drag and gravity cancel out}\\
F_D=F_g\\
\frac{\rho C_DAv^2}{2}=mg\\
v^2=\frac{2mg}{\rho C_DAv^2}
\end{align*}$$&lt;/p&gt;&lt;/details&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\boxed{v_t=\sqrt{\frac{2mg}{\rho C_dA}}}&lt;/script&gt;

&lt;p&gt;&lt;em&gt;This terminal velocity is more aptly described as a terminal speed, since it is just a magnitude.&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;drag-coefficient&quot;&gt;Drag Coefficient&lt;/h3&gt;
&lt;p&gt;The drag coefficient is a proportionality constant used in the drag equation and is measured empirically. Objects with more blunt faces have higher drag coefficients and objects with more streamlined designs have lower ones. Here is a diagram of different drag coefficients in a particular fluid:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/physics/drag_coefficients.png?style=centerme&quot; alt=&quot;dragCOF&quot; width=&quot;400px&quot; /&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Friction</title>
   <link href="http://localhost:4000/friction/"/>
   <updated>2018-03-26T00:00:00-04:00</updated>
   <id>http://localhost:4000/friction</id>
   <content type="html">&lt;h3 id=&quot;general-info&quot;&gt;General Info&lt;/h3&gt;
&lt;!-- &amp; Table of Contents --&gt;
&lt;p&gt;Dry Friction is a force that opposes the motion of two surfaces that slide against each other. This force is proportional to the normal force of the object.&lt;/p&gt;

&lt;!-- - [Amontons' Laws of Dry Friction](#amontons-laws-of-dry-friction)
- [From Static to Kinetic Friction](#from-static-to-kinetic-friction)
- [Static Friction](#static-friction)
- [Kinetic Friction](#kinetic-friction)
- [Coefficient of Friction](#coefficient-of-friction) --&gt;

&lt;h4 id=&quot;where-does-friction-come-from&quot;&gt;Where does Friction come from?&lt;/h4&gt;
&lt;p&gt;The frictional force is not fundamental and is instead a useful abstraction of the complex electromagnetic interactions between two surfaces that come into contact with each other. The sum total of the forces created by these interactions can, in a variety of cases, be thought of as a singular force that is proportional to the normal force.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/physics/friction_between_surfaces.png?style=centerme&quot; alt=&quot;complexfriction&quot; width=&quot;450px&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;intuition-and-newtons-first-law&quot;&gt;Intuition and Newton’s First Law&lt;/h4&gt;
&lt;p&gt;The frictional force is responsible for filling the gap between our everyday intuition of motion (that it eventually stops) and Newton’s first law (that an object in motion remains in motion, unless acted upon by an outside force). The ‘outside force’ that retards motion in the case many of everyday objects is indeed friction.
$\renewcommand{\vec}[1]{\mathbf{#1}}$
&lt;!--more--&gt;&lt;/p&gt;

&lt;!-- ![xkcd](https://imgs.xkcd.com/comics/experiment.png?style=centerme)
&lt;center&gt;&lt;i&gt;Relevant xckd&lt;/i&gt;&lt;/center&gt; --&gt;

&lt;h3 id=&quot;amontons-laws-of-dry-friction&quot;&gt;Amontons’ Laws of Dry Friction&lt;/h3&gt;
&lt;p&gt;The empirical laws that govern friction were (re)discovered by French physicist Guillaume Amontons:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Amontons’ First Law&lt;/strong&gt;: The force of friction is directly proportional to the applied load.
    &lt;ul&gt;
      &lt;li&gt;The more an object is pushed into a surface (normal force) the more it interacts with that surface (friction).&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Amontons’ Second Law&lt;/strong&gt;: The force of friction is independent of the apparent area of contact.
    &lt;ul&gt;
      &lt;li&gt;While more surface area means more interactions with the surface (friction), having more area means the applied load is more distributed, offsetting the increase in area.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Amontons’ Third Law (&lt;strong&gt;Coulomb’s Law of Friction&lt;/strong&gt;): Kinetic friction is independent of the sliding velocity.
    &lt;ul&gt;
      &lt;li&gt;Kinetic friction takes over after static friction has been overcome and object starts to move, but once its in motion friction is constant.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;from-static-to-kinetic-friction&quot;&gt;From Static to Kinetic Friction&lt;/h3&gt;
&lt;p&gt;There are two types of friction: static friction which acts on an object at rest, and kinetic friction which acts on an object in motion. When the force of static friction reaches a maximum, kinetic friction takes over:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/physics/friction_graph.png?style=centerme&quot; alt=&quot;graph&quot; width=&quot;500px&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;static-friction&quot;&gt;Static Friction&lt;/h3&gt;
&lt;p&gt;An object at rest on a surface will resist motion on the surface until a certain maximum force has been reached. The magnitude of that max force is given by:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;F_{s,max}=\mu_sN&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;$F_{s,max}$ is the magnitude of the maximum force of static friction.&lt;/li&gt;
  &lt;li&gt;$\mu_s$ is the coefficient of static friction.&lt;/li&gt;
  &lt;li&gt;$N$ is the magnitude of the normal force on the object (the component of the total force on the object that is normal to the plane it’s resting on).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;At any other point before that max force has been reached, the force of static friction is equal in magnitude and opposite in direction to the component of the total force on the object that is parallel to the plane it’s resting on, effectively canceling out any acceleration across the surface:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\vec{F_s}=-\vec{F_{\parallel}}&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;$\vec{F_s}$ is the force of static friction.&lt;/li&gt;
  &lt;li&gt;$\vec{F_\parallel}$ is the total force on the object parallel to the surface.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This parallel force can be described in terms of the normal force which is orthogonal to the surface:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\vec{F_{\parallel}}=\vec{F_{net}}-\vec{N}&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;$\vec{F_{net}}$ is the total force applied to the object.&lt;/li&gt;
  &lt;li&gt;$\vec{N}$ is the normal force on the object.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;work-done-by-static-friction&quot;&gt;Work done by Static Friction&lt;/h4&gt;
&lt;p&gt;Notice that because static friction cancels out all motion across a particular surface, the force does not do any work (at least from the reference frame of the two surfaces).&lt;/p&gt;

&lt;h3 id=&quot;kinetic-friction&quot;&gt;Kinetic Friction&lt;/h3&gt;
&lt;p&gt;An object in motion on a surface will experience kinetic friction that is proportional to the normal force on the object and independent of its velocity. The magnitude of this force is given by:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;F_{k}=\mu_kN&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;$F_{k}$ is the magnitude of the force of static friction.&lt;/li&gt;
  &lt;li&gt;$\mu_k$ is the coefficient of kinetic friction.&lt;/li&gt;
  &lt;li&gt;$N$ is the magnitude of the normal force on the object.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Kinetic friction is pointed in the opposite direction of the object’s velocity:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{\vec{F_k}}=-\hat{\vec{v}}&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;$\hat{\vec{F_k}}$ is the unit vector of the object’s velocity.&lt;/li&gt;
  &lt;li&gt;$\hat{\vec{v}}$ is the unit vector of the object’s velocity.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Putting these together we find that the force of kinetic friction is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\vec{F_{k}}=-\mu_kN\hat{\vec{v}}=\frac{-\mu_kN}{\|\vec{v}\|}\vec{v}&lt;/script&gt;

&lt;p&gt;&lt;em&gt;Note that this does not violate Coulomb’s Law of Friction because it refers only to the magnitude of kinetic friction.&lt;/em&gt;&lt;/p&gt;

&lt;h4 id=&quot;non-conservative-force-and-thermal-energy&quot;&gt;Non-Conservative Force and Thermal Energy&lt;/h4&gt;
&lt;p&gt;Friction is a non-conservative force, meaning that the work done by kinetic friction is dependent on the path taken. Friction also causes heat energy to be released into the system, meaning it does not conserve mechanical energy.&lt;/p&gt;

&lt;p&gt;To calculate the thermal energy created by kinetic friction, a line integral through the path taken must be used:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;E_{th}=\int_C\vec{F_k}(\vec{x})\cdot d\vec{x}&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;$\vec{F_k}(\vec{x})$ is a vector field of the force of kinetic friction.&lt;/li&gt;
  &lt;li&gt;$\vec{x}$ is the position of the object.&lt;/li&gt;
  &lt;li&gt;$C$ is the path the object took.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;coefficient-of-friction&quot;&gt;Coefficient of Friction&lt;/h3&gt;
&lt;p&gt;The proportionality constant between the frictional force and the normal force is called the coefficient of static/kinetic friction and is denoted $\mu_s$ and $\mu_k$ respectively. This constant differs depending on the 2 surfaces involved and, as a result of being a abstract approximation of complex interactions, cannot be derived from first principles and is instead measured empirically.&lt;/p&gt;

&lt;p&gt;Below are some common coefficients of static and kinetic friction:
&lt;img src=&quot;http://hadron.physics.fsu.edu/~crede/TEACHING/PHY2048C/Calendar/W6_D1/Friction%20Coefficients_files/friction-coeffs.gif&quot; alt=&quot;table&quot; width=&quot;600px&quot; /&gt;&lt;/p&gt;

&lt;!-- ![xkcd](https://imgs.xkcd.com/comics/mu.png?style=centerme)
&lt;center&gt;&lt;i&gt;Relevant xckd&lt;/i&gt;&lt;/center&gt; --&gt;

&lt;h4 id=&quot;angle-of-repose&quot;&gt;Angle of Repose&lt;/h4&gt;
&lt;p&gt;When an object is placed on a ramp, the object will overcome static friction and slide down the ramp at some angle $\theta$. As it turns out, the tangent of this angle is equivalent to the coefficient of static friction:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/8/85/Free_body.svg?style=centerme&quot; alt=&quot;angle of repose&quot; /&gt;&lt;/p&gt;

&lt;details&gt;&lt;summary&gt;Derivation&lt;/summary&gt;&lt;p&gt;

$$\begin{align*}
N=mg\cos\theta \tag{force normal to the ramp}\\
\mu_sN=mg\sin\theta \tag{$F_k$ at the moment of slipping}\\
\mu_smg\cos\theta=mg\sin\theta\\
\mu_s=\frac{\sin\theta}{\cos\theta}=\tan\theta
\end{align*}$$&lt;/p&gt;&lt;/details&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\boxed{\tan\theta=\mu_s}&lt;/script&gt;
</content>
 </entry>
 
 <entry>
   <title>Classical Physics</title>
   <link href="http://localhost:4000/classical-physics/"/>
   <updated>2018-03-24T00:00:00-04:00</updated>
   <id>http://localhost:4000/classical-physics</id>
   <content type="html">&lt;p&gt;Classical physics refers to the branches of physics that were conceived of prior to quantum mechanics and Einsteinian relativity. These branches of physics are more than sufficient in describing phenomena at non-relativistic speeds and non-quantum scales. Moreover, they serve as starting points for understanding and deriving modern physics.&lt;/p&gt;

&lt;h2 id=&quot;classical-mechanics&quot;&gt;Classical Mechanics&lt;/h2&gt;
&lt;p&gt;Classical mechanics is the study of the motion of massive bodies. It is usually split into two main branches:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;/kinematics&quot;&gt;Kinematics&lt;/a&gt; - The description of translational motion through space. Other types of motion include:
    &lt;ul&gt;
      &lt;li&gt;Rotational Kinematics&lt;/li&gt;
      &lt;li&gt;Simple Harmonic Motion (Oscillatory Motion)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Dynamics - The causes of motion. Main topics include:
    &lt;ul&gt;
      &lt;li&gt;Forces&lt;/li&gt;
      &lt;li&gt;Work, Power &amp;amp; Energy&lt;/li&gt;
      &lt;li&gt;Linear Momentum&lt;/li&gt;
      &lt;li&gt;Rotational Dynamics&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;!--more--&gt;

&lt;h4 id=&quot;reformulations&quot;&gt;Reformulations&lt;/h4&gt;
&lt;p&gt;While Newtonian mechanics, as described by the &lt;strong&gt;&lt;em&gt;principia&lt;/em&gt;&lt;/strong&gt;, was the first formulation of classical mechanics, there exists others. Namely Lagrangian Mechanics and Hamiltonian mechanics. These formulations are instrumental in developing modern theories like quantum mechanics.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://en.citizendium.org/images/thumb/f/f5/Classical_mechanics_timeline.PNG/800px-Classical_mechanics_timeline.PNG?style=centerme&quot; alt=&quot;formulations&quot; width=&quot;500px&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;classical-electromagnetism&quot;&gt;Classical Electromagnetism&lt;/h2&gt;
&lt;p&gt;Classical electromagnetism studies the properties and interactions of electric charges, magnets, and the electromagnetic force in general. There are 4 main topics here:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Electrostatics
    &lt;ul&gt;
      &lt;li&gt;Electric Charge&lt;/li&gt;
      &lt;li&gt;Electric Fields&lt;/li&gt;
      &lt;li&gt;Electric Potential&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Magnetostatics&lt;/li&gt;
  &lt;li&gt;Electrodynamics&lt;/li&gt;
  &lt;li&gt;Electrical Networks
    &lt;ul&gt;
      &lt;li&gt;Capacitance&lt;/li&gt;
      &lt;li&gt;Current&lt;/li&gt;
      &lt;li&gt;Resistance&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;thermodynamics&quot;&gt;Thermodynamics&lt;/h2&gt;
&lt;p&gt;Thermodynamics is the study of the workings and effects of heat, or &lt;strong&gt;thermal energy&lt;/strong&gt;, in a system. Some main topics include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The 4 Laws of Thermodynamics&lt;/li&gt;
  &lt;li&gt;Entropy&lt;/li&gt;
  &lt;li&gt;Temperature&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Playing Atari Pong with Reinforcement Learning</title>
   <link href="http://localhost:4000/reinforcement-learning-pong/"/>
   <updated>2018-03-18T00:00:00-04:00</updated>
   <id>http://localhost:4000/reinforcement-learning-pong</id>
   <content type="html">&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;
&lt;p&gt;In 2013 the relatively new AI startup DeepMind released their paper &lt;a href=&quot;https://arxiv.org/pdf/1312.5602.pdf&quot;&gt;&lt;em&gt;Playing Atari with Deep Reinforcement Learning&lt;/em&gt;&lt;/a&gt; detailing an artificial neural network that was able to play, not 1, but 7 Atari games with human and even super-human level proficiency. What made this paper so astounding was the fact that it was a single, general purpose neural network (a &lt;strong&gt;general artificial intelligence&lt;/strong&gt; if you will) that could be trained to play all these games rather than 7 separate ones.&lt;/p&gt;

&lt;p&gt;If this wasn’t enough, in 2015 they blew the machine learning community, and everyone else considering the news coverage, away with their paper &lt;a href=&quot;https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf&quot;&gt;&lt;em&gt;Human-level control through deep reinforcement learning&lt;/em&gt;&lt;/a&gt; in which they construct what they call a &lt;strong&gt;Deep Q Network&lt;/strong&gt; (DQN) to play &lt;em&gt;42&lt;/em&gt; different Atari games, all of varying complexity, with performance that exceeded a professional human player.&lt;a href=&quot;https://deepmind.com/research/dqn/&quot;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;

&lt;!--more--&gt;

&lt;!-- ![xkcd](https://imgs.xkcd.com/comics/pong.png?style=centerme)
&lt;center&gt;&lt;i&gt;Relevant xckd&lt;/i&gt;&lt;/center&gt; --&gt;

&lt;h2 id=&quot;q-learning&quot;&gt;Q-Learning&lt;/h2&gt;
&lt;p&gt;The researchers at Google’s DeepMind achieved this stunning success with a type of machine learning called &lt;strong&gt;reinforcement learning&lt;/strong&gt; and more specifically &lt;strong&gt;Q-learning&lt;/strong&gt;. In essence, the goal of Q-learning is to approximate some ideal function $Q(s,a)$ that outputs a reward (how good we are doing at the task), where $s$ is a possible state of the environment/game/etc. and $a$ is a possible action to take in that state. If we had such a function, or even a good approximation, we could simply plug in our current state and choose whatever action will maximize $Q$ which would then maximize how well we perform the task. To approximate this function, the researches used a convolutional neural network (CNN) and trained it using Q-learning, thus creating a Deep Q Network. You can read more about Q-learning and DQNs &lt;a href=&quot;https://ai.intel.com/demystifying-deep-reinforcement-learning/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;By implementing Q-learning in a convolutional neural network (CNN) they create a DQN capable of predicting what actions to take based on the current state of the game.&lt;/p&gt;

&lt;h2 id=&quot;policy-gradients&quot;&gt;Policy Gradients&lt;/h2&gt;
&lt;p&gt;That said, Q-learning isn’t the only way to achieve these results. Another popular type of reinforcement learning is what known as &lt;strong&gt;policy gradients&lt;/strong&gt;. This method is more direct and conceptually simpler than Q-learning. Essentially, you input the current state, action taken, and reward given at every step and optimize the network accordingly.&lt;/p&gt;

&lt;p&gt;And make no mistake, while simpler, policy networks can be just as good as DQNs. In fact, when tuned correctly, they perform even better than DQNs. Don’t believe me? Just ask the authors of the original papers themselves:
&lt;a href=&quot;https://arxiv.org/pdf/1602.01783.pdf&quot;&gt;Asynchronous Methods for Deep Reinforcement Learning&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;my-attempt&quot;&gt;My Attempt&lt;/h2&gt;
&lt;!-- Encouraged by the aforementioned research, I thought I would attempt to create an ANN capable of playing pong using reinforcement learning. Using OpenAI's [Gym](https://gym.openai.com) package to model [Pong](https://gym.openai.com/envs/Pong-v0/), and Google's [*TensorFlow*](https://www.tensorflow.org/) library to construct the network, I'll attempt to explain the code, its results, and its accuracy. --&gt;
&lt;p&gt;Encouraged by the aforementioned research, I thought I would attempt to create an ANN capable of playing pong using reinforcement learning. To do this I used OpenAI’s &lt;a href=&quot;https://gym.openai.com&quot;&gt;Gym&lt;/a&gt; package to model &lt;a href=&quot;https://gym.openai.com/envs/Pong-v0/&quot;&gt;Pong&lt;/a&gt;, and Google’s &lt;a href=&quot;https://www.tensorflow.org/&quot;&gt;&lt;em&gt;TensorFlow&lt;/em&gt;&lt;/a&gt; library to construct the network.&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;https://github.com/ozanerhansha/NeuralNetworks/tree/master/src/pongRL&quot;&gt;code&lt;/a&gt; for this network, dubbed PongNet, is in my &lt;a href=&quot;https://github.com/ozanerhansha/NeuralNetworks&quot;&gt;NeuralNetwork&lt;/a&gt; repository on GitHub.&lt;/p&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;
&lt;p&gt;Learning starts to appear after 1500 games (a game goes on until one player reaches 20 points) and it reaches a 50% win-rate at around 8000 games. More testing needs to be done to see the maximum accuracy of this particular network.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/projects/pongAI/pongdata.png?style=centerme&quot; alt=&quot;pongdata&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Below is the network (on a good day) playing against the same bot it trained with for 10,000 games.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/projects/pongAI/pongai.gif?style=centerme&quot; alt=&quot;bc&quot; /&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Kinematics</title>
   <link href="http://localhost:4000/kinematics/"/>
   <updated>2018-03-17T00:00:00-04:00</updated>
   <id>http://localhost:4000/kinematics</id>
   <content type="html">&lt;!-- Make vectors bold rather than arrow headed --&gt;
&lt;p&gt;$\renewcommand{\vec}[1]{\mathbf{#1}}$
Translational Kinematics describes the motion of objects through space over time. Using an object’s &lt;a href=&quot;/position&quot;&gt;position&lt;/a&gt;, velocity and acceleration vectors, it is possible to predict where it will be in the future and where it was in the past. Where an object gets that velocity/acceleration is what dynamics seeks to explain.&lt;/p&gt;

&lt;h3 id=&quot;variable-acceleration&quot;&gt;Variable Acceleration&lt;/h3&gt;
&lt;p&gt;To calculate the motion of an object with continuous acceleration, simply refer to the definitions of velocity and acceleration:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
\vec{v}=\frac{d\vec{x}}{dt}\\
\vec{a}=\frac{d\vec{v}}{dt}
\end{align}&lt;/script&gt;

&lt;p&gt;Integrating these equations we get:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
\vec{x}=\int\vec{v} \,dt+\vec{x}_0\\
\vec{v}=\int\vec{a} \,dt+\vec{v}_0
\end{align}&lt;/script&gt;

&lt;!--more--&gt;

&lt;ul&gt;
  &lt;li&gt;$\vec{x}$ is the object’s position as a function of time.&lt;/li&gt;
  &lt;li&gt;$\vec{x}_0$ is the initial position, at &lt;script type=&quot;math/tex&quot;&gt;t=0&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;$\vec{v}$ is the object’s velocity as a function of time.&lt;/li&gt;
  &lt;li&gt;$\vec{v}_0$ is the initial velocity, at &lt;script type=&quot;math/tex&quot;&gt;t=0&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;$\vec{a}$ is the object’s acceleration as a function of time.&lt;/li&gt;
  &lt;li&gt;$t$ is time&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;constant-acceleration&quot;&gt;Constant Acceleration&lt;/h3&gt;
&lt;p&gt;When acceleration is assumed to be constant, as is the case for many physical systems, the following kinematic equations can be derived:&lt;/p&gt;

&lt;!-- #### Position Independent Equation
$$\begin{align}
\vec{v}&amp;=\int\vec{a} \,dt+\vec{v}_0 \tag{integral def. of \(\vec{v}\)}\\
&amp;=\vec{a}\int dt+\vec{v}_0 \tag{\(\vec{a}\) is constant}\\
&amp;=\vec{a}t+\vec{v}_0
\end{align}$$ --&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Position Independent Equation&lt;/strong&gt;&lt;/summary&gt;
&lt;p&gt;$$\begin{align}
\vec{v}&amp;amp;=\int\vec{a} \,dt+\vec{v}_0 \tag{integral def. of \(\vec{v}\)}\\
&amp;amp;=\vec{a}\int dt+\vec{v}_0 \tag{\(\vec{a}\) is constant}\\
&amp;amp;=\vec{a}t+\vec{v}_0
\end{align}$$&lt;/p&gt;
&lt;p&gt;$$\boxed{\vec{v}=\vec{v}_0+\vec{a}t}$$&lt;/p&gt;
&lt;/details&gt;

&lt;!-- #### Velocity Independent Equation
$$\begin{align}
\vec{v}&amp;=\vec{a}t+\vec{v}_0 \tag{\(\vec{x}\) independent Eq.}\\
\vec{x}&amp;=\int\vec{v} \,dt+\vec{x}_0 \tag{integral def. of \(\vec{x}\)}\\
&amp;=\int(\vec{a}t+\vec{v}_0) \,dt+\vec{x}_0 \\
&amp;=\int\vec{a}t\,dt + \int\vec{v}_0 \,dt+\vec{x}_0 \\
&amp;=\vec{a}\int t \,dt + \vec{v}_0t+\vec{x}_0 \tag{\(\vec{a}\) is constant}\\
&amp;=\frac{\vec{a}t^2}{2}+\vec{v}_0t+\vec{x}_0
\end{align}$$

$$\boxed{\vec{x}=\frac{\vec{a}t^2}{2}+\vec{v}_0t+\vec{x}_0}$$ --&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Velocity Independent Equation&lt;/strong&gt;&lt;/summary&gt;
&lt;p&gt;$$\begin{align}
\vec{v}&amp;amp;=\vec{a}t+\vec{v}_0 \tag{\(\vec{x}\) independent Eq.}\\
\vec{x}&amp;amp;=\int\vec{v} \,dt+\vec{x}_0 \tag{integral def. of \(\vec{x}\)}\\
&amp;amp;=\int(\vec{a}t+\vec{v}_0) \,dt+\vec{x}_0 \\
&amp;amp;=\int\vec{a}t\,dt + \int\vec{v}_0 \,dt+\vec{x}_0 \\
&amp;amp;=\vec{a}\int t \,dt + \vec{v}_0t+\vec{x}_0 \tag{\(\vec{a}\) is constant}\\
&amp;amp;=\frac{\vec{a}t^2}{2}+\vec{v}_0t+\vec{x}_0
\end{align}$$&lt;/p&gt;
&lt;p&gt;$$\boxed{\vec{x}=\vec{x}_0+\vec{v}_0t+\frac{\vec{a}t^2}{2}}$$&lt;/p&gt;
&lt;/details&gt;

&lt;!-- #### Acceleration Independent Equation
$$\begin{align}
\vec{v}&amp;=\vec{a}t+\vec{v}_0 \tag{\(\vec{x}\) independent Eq.}\\
\vec{a}&amp;=\frac{\vec{v}-\vec{v_0}}{t}\\
\vec{x}&amp;=\frac{\vec{a}t^2}{2}+\vec{v}_0t+\vec{x}_0 \tag{\(\vec{v}\) independent Eq.}\\
&amp;=\frac{\frac{\vec{v}-\vec{v_0}}{t}t^2}{2}+\vec{v}_0t+\vec{x}_0\\
&amp;=\frac{\vec{v_0}+\vec{v}}{2}t+\vec{x}_0\\
\end{align}$$

$$\boxed{\vec{x}=\frac{\vec{v_0}+\vec{v}}{2}t+\vec{x}_0}$$ --&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Acceleration Independent Equation&lt;/strong&gt;&lt;/summary&gt;
&lt;p&gt;$$\begin{align}
\vec{v}&amp;amp;=\vec{a}t+\vec{v}_0 \tag{\(\vec{x}\) independent Eq.}\\
\vec{a}&amp;amp;=\frac{\vec{v}-\vec{v_0}}{t}\\
\vec{x}&amp;amp;=\frac{\vec{a}t^2}{2}+\vec{v}_0t+\vec{x}_0 \tag{\(\vec{v}\) independent Eq.}\\
&amp;amp;=\frac{\frac{\vec{v}-\vec{v_0}}{t}t^2}{2}+\vec{v}_0t+\vec{x}_0\\
&amp;amp;=\frac{\vec{v_0}+\vec{v}}{2}t+\vec{x}_0\\
\end{align}$$&lt;/p&gt;
&lt;p&gt;$$\boxed{\vec{x}=\vec{x}_0+\frac{\vec{v_0}+\vec{v}}{2}t}$$&lt;/p&gt;
&lt;/details&gt;

&lt;!-- #### Time Independent Equation
$$\begin{align}
&amp;\vec{v}=\vec{a}t+\vec{v}_0 \tag{\(\vec{x}\) independent Eq.}\\
&amp;{\vec{a}}t=\vec{v}-\vec{v}_0\\
&amp;\vec{x}=\frac{\vec{v_0}+\vec{v}}{2}t+\vec{x}_0 \tag{\(\vec{a}\) independent Eq.}\\
&amp;2(\vec{x}-\vec{x}_0)=(\vec{v}+\vec{v}_0)t\\
\end{align}$$

$$\begin{align}
2\vec{a}\cdot(\vec{x}-\vec{x}_0)&amp;=(\vec{v}+\vec{v}_0)\cdot\vec{a}t\\
&amp;=(\vec{v}+\vec{v}_0)\cdot(\vec{v}-\vec{v}_0) \tag{foil dot product}\\
&amp;=\vec{v} \cdot \vec{v} - \vec{v}_0 \cdot \vec{v}_0\\
&amp;=\left \| \vec{v} \right \|^2-\left \| \vec{v}_0 \right \|^2\\
\end{align}$$

$$\boxed{\left \| \vec{v} \right \|^2 = 2\vec{a}\cdot(\vec{x}-\vec{x}_0)+\left \| \vec{v}_0 \right \|^2}$$ --&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Time Independent Equation&lt;/strong&gt;&lt;/summary&gt;
&lt;p&gt;$$\begin{align}
&amp;amp;\vec{v}=\vec{a}t+\vec{v}_0 \tag{\(\vec{x}\) independent Eq.}\\
&amp;amp;{\vec{a}}t=\vec{v}-\vec{v}_0\\
&amp;amp;\vec{x}=\frac{\vec{v_0}+\vec{v}}{2}t+\vec{x}_0 \tag{\(\vec{a}\) independent Eq.}\\
&amp;amp;2(\vec{x}-\vec{x}_0)=t(\vec{v}+\vec{v}_0)\\
\end{align}$$

$$\begin{align}
2\vec{a}\cdot(\vec{x}-\vec{x}_0)&amp;amp;=\vec{a}t\cdot (\vec{v}+\vec{v}_0)\\
&amp;amp;=(\vec{v}-\vec{v}_0)\cdot(\vec{v}+\vec{v}_0) \\
&amp;amp;=\vec{v} \cdot \vec{v} - \vec{v}_0 \cdot \vec{v}_0 \tag{foil dot product}\\
&amp;amp;=\left \| \vec{v} \right \|^2-\left \| \vec{v}_0 \right \|^2\\
\end{align}$$&lt;/p&gt;

&lt;p&gt;$$\boxed{\left \| \vec{v} \right \|^2 = 2\vec{a}\cdot(\vec{x}-\vec{x}_0)+\left \| \vec{v}_0 \right \|^2}$$&lt;/p&gt;
&lt;/details&gt;

&lt;h3 id=&quot;book-keeping&quot;&gt;Book Keeping&lt;/h3&gt;
&lt;p&gt;There are a couple of things we can do to clean up this set of 4 equations before we display them all together.&lt;/p&gt;

&lt;h4 id=&quot;displacement-vs-position&quot;&gt;Displacement vs. Position&lt;/h4&gt;
&lt;p&gt;While these equations describe the movement of an object in terms of its initial and current position, $\vec{x}_0$ and $\vec{x}$ respectively, it is common to think about kinematics in terms of &lt;strong&gt;displacement&lt;/strong&gt;, or change in distance, $\Delta\vec{x}$ instead:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Delta\vec{x}=\vec{x}-\vec{x}_0&lt;/script&gt;

&lt;h4 id=&quot;average-velocity&quot;&gt;Average Velocity&lt;/h4&gt;
&lt;p&gt;If we look at the acceleration independent equation, we can see it contains the expression $\frac{\vec{v_0}+\vec{v}}{2}$. As it turns out, when acceleration is constant, this expression is actually equivalent to the object’s &lt;strong&gt;average velocity&lt;/strong&gt; $\overline{\vec{v}}$:&lt;/p&gt;

&lt;details&gt;
&lt;summary&gt;Derivation&lt;/summary&gt;
&lt;p&gt;$$\begin{align}
&amp;amp;\overline{\vec{v}}=\frac{\vec{x}-\vec{x}_0}{t} \tag{def. of \(\overline{\vec{v}}\)}\\
&amp;amp;\overline{\vec{v}}t=\vec{x}-\vec{x}_0\\
&amp;amp;\vec{x}-\vec{x}_0=\frac{\vec{a}t^2}{2}+\vec{v}_0t \tag{\(\vec{v}\) independent Eq.}\\
&amp;amp;\overline{\vec{v}}t=\frac{\vec{a}t^2}{2}+\vec{v}_0t\\
&amp;amp;\overline{\vec{v}}=\frac{\vec{a}t}{2}+\vec{v}_0\\
&amp;amp;\vec{v}=\vec{a}t+\vec{v}_0 \tag{\(\vec{x}\) independent Eq.}\\
&amp;amp;\vec{a}t=\vec{v}-\vec{v}_0\\
&amp;amp;\overline{\vec{v}}=\frac{\vec{v}-\vec{v}_0}{2}+\vec{v}_0=\frac{\vec{v_0}+\vec{v}}{2}
\end{align}$$&lt;/p&gt;
&lt;/details&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\boxed{\overline{\vec{v}}=\frac{\vec{v_0}+\vec{v}}{2}}&lt;/script&gt;

&lt;h4 id=&quot;dot-product&quot;&gt;Dot Product&lt;/h4&gt;
&lt;p&gt;When deriving the time independent kinematic equation, we used the dot product when manipulating the expressions $\vec{a}t\cdot(\vec{v}+\vec{v}_0)$ and $(\vec{v}-\vec{v}_0)\cdot(\vec{v}+\vec{v}_0)$. We were justified in doing so because the dot product is both commutative and distributive.&lt;/p&gt;

&lt;p&gt;The dot product of a vector with itself is that vector’s &lt;strong&gt;squared norm&lt;/strong&gt; and is often denoted as such:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\vec{v}\cdot\vec{v}=\left \| \vec{v} \right \|^2=v^2&lt;/script&gt;

&lt;h4 id=&quot;the-kinematic-equations&quot;&gt;The Kinematic Equations&lt;/h4&gt;
&lt;p&gt;Taking into account average velocity, the simpler dot product notation and using displacement instead of position, we can rewrite the kinematic equations for constant acceleration as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{gather}
&amp;\vec{v}=\vec{v}_0+\vec{a}t \tag{\(\vec{x}\) independent}\\
&amp;\Delta\vec{x}=\vec{v}_0t+\frac{\vec{a}t^2}{2} \tag{\(\vec{v}\) independent}\\
&amp;\Delta\vec{x}=\overline{\vec{v}}t=\frac{\vec{v_0}+\vec{v}}{2}t \tag{\(\vec{a}\) independent}\\
&amp;v^2 = v_0^2+2\vec{a}\cdot\Delta\vec{x} \tag{t independent}
\end{gather} %]]&gt;&lt;/script&gt;

&lt;!-- #### Note on Applicability of Kinematic Equations
It is important to note that the kinematic equations apply to all systems of 3 variables $x, \frac{dx}{dt}$, and $\frac{d^2x}{dt^2}$ as long as $$\frac{d^2x}{dt^2}$$ is constant. For example, $x$ could represent the  --&gt;

&lt;h3 id=&quot;graphing&quot;&gt;Graphing&lt;/h3&gt;
&lt;p&gt;When the position, velocity, and acceleration functions are graphed with respect to time, their graphs are related by the kinematic equations.&lt;/p&gt;

&lt;h4 id=&quot;general-info&quot;&gt;General Info&lt;/h4&gt;
&lt;p&gt;For any graph of $\vec{x}(t)$&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Slope of the secant line created by two points is the average velocity during that time interval.&lt;/li&gt;
  &lt;li&gt;Slope of line tangent to curve at time &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt; is the object’s instantaneous velocity at $t$.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For any graph of $\vec{v}(t)$&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Slope of the secant line created by two points is the average acceleration during that time interval.&lt;/li&gt;
  &lt;li&gt;Slope of line tangent to curve at time &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt; is the object’s instantaneous acceleration at &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;Area under a region of the curve from $[a,b]$ is the object’s displacement during $[a,b]$.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For any graph of $\vec{a}(t)$&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Area under a region of the curve from $[a,b]$ is the object’s change in velocity during $[a,b]$.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;graph-shapes&quot;&gt;Graph Shapes&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/assets/physics/kinematics/kinematics.png?style=centerme&quot; alt=&quot;graphs&quot; /&gt;&lt;/p&gt;

&lt;!-- #### An Object at Rest
![graphs](/assets/physics/kinematics/kinematics_at_rest.png?style=centerme)
- $\vec{x}(t) = c$, where c is a constant.
- $\vec{v}(t)=0$
- $\vec{a}(t)=0$

#### An Object with Constant Velocity
![graphs](/assets/physics/kinematics/kinematics_const_vel.png?style=centerme)
- $\vec{x}(t)$ is linear.
- $\vec{v}(t) = c$, where c is a constant.
- $\vec{a}(t)=0$

#### An Object with Constant Acceleraion
![graphs](/assets/physics/kinematics/kinematics_const_accel.png?style=centerme)
- $\vec{x}(t)$ is quadratic.
- $\vec{v}(t)$ is linear.
- $\vec{a}(t) = c$, where c is a constant. --&gt;
</content>
 </entry>
 
 <entry>
   <title>Position</title>
   <link href="http://localhost:4000/position/"/>
   <updated>2018-03-15T00:00:00-04:00</updated>
   <id>http://localhost:4000/position</id>
   <content type="html">&lt;p&gt;The position of a particle or object in &lt;strong&gt;space&lt;/strong&gt; is defined as a point on an n-dimensional &lt;a href=&quot;/cartesian-product/#coordinates&quot;&gt;&lt;strong&gt;Cartesian coordinate system&lt;/strong&gt;&lt;/a&gt;. This point can equivalently be thought of as a vector.&lt;/p&gt;

&lt;h3 id=&quot;classical-physics&quot;&gt;Classical Physics&lt;/h3&gt;
&lt;p&gt;In classical physics, a position or displacement $\vec{x}$ is a 3-dimensional vector with components $\left(x,y,z\right)$.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Position is used to refer to a specific point in a coordinate space while displacement refers to the distance traveled from an initial point (usually this point is the origin making position and displacement equivalent)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The magnitude of displacement, or net &lt;strong&gt;distance&lt;/strong&gt;, is calculated the same as any other vector:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\left|\vec{x}\right|=\sqrt{x^2+y^2+z^2}&lt;/script&gt;

&lt;!--more--&gt;

&lt;h4 id=&quot;displacement-function&quot;&gt;Displacement Function&lt;/h4&gt;
&lt;p&gt;An object’s position can be described as a function of time, $\vec{x}(t)$. This position function would thus map from time to 3D-space:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\vec{x}:\mathbb{R}\rightarrow \mathbb{R}^3&lt;/script&gt;

&lt;p&gt;Taking the derivative of an object’s displacement function with respect to $t$ provides the object’s velocity function, $\vec{v}(t)$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\mathrm{d}}{\mathrm{d}t}\vec{x}(t)=\vec{v}(t)&lt;/script&gt;

&lt;!-- Click [here]() for a list of the repeated time derivatives of displacement. --&gt;

&lt;h3 id=&quot;relativity&quot;&gt;Relativity&lt;/h3&gt;
&lt;p&gt;In relativity, both time and space are combined into a single space-time continuum. As such, we need a more general notion of ‘&lt;em&gt;position&lt;/em&gt;’ that includes time. This 4-dimensional position (3 spatial dimensions and 1 time) is called 4-position or, more commonly, an &lt;strong&gt;event&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;An event is defined as a 4-dimensional vector with components $\left(t,x,y,z\right)$. Relativity shows that events that occur at the same time in one reference frame, don’t necessarily happen at the same time in another. This is known as &lt;strong&gt;relativity of simultaneity&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;This is only true for space-like separated events where $|c\Delta t| \lt |\Delta x|$&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;In special relativity the distance between two positions is not invariant in different reference frames. This brings us to the &lt;strong&gt;spacetime interval&lt;/strong&gt;. The spacetime interval, $(\Delta s)^2$ between two events stays constant in all reference frames. In special relativity $(\Delta s)^2$ is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;(\Delta s)^2=(c\Delta t)^2-(\Delta x)^2-(\Delta y)^2-(\Delta z)^2&lt;/script&gt;

&lt;h3 id=&quot;quantum-mechanics&quot;&gt;Quantum Mechanics&lt;/h3&gt;
&lt;p&gt;In quantum mechanics, the state of a particle is given (and completely described) by its &lt;strong&gt;wavefunction&lt;/strong&gt; $\psi(\mathbf{x}, t)$. With $\mathbf{x}$ being a point in 3-space and $t$ being a point in time. $\psi$ returns a complex valued probability amplitude.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\psi:\mathbb{R^4}\rightarrow \mathbb{C}&lt;/script&gt;

&lt;p&gt;The probability of observing a particle at position $x$ at time $t$ is given by $\left \vert \psi \right \vert^2$. This implies that, in the one dimensional case, at a given time $t$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\int_{-\infty}^{\infty}\left | \psi(x) \right |^2 dx=1&lt;/script&gt;

&lt;p&gt;Because the particle has to be somewhere in space, the probabilities must sum to 1. We integrate from $-\infty$ to $\infty$ since the particle is a wave spread out over all of space. This means that the particle could possibly be anywhere in the universe, just with an unfathomably low probability. With some vector calculus, this can extended to the 3D case we see in the real world.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Physics</title>
   <link href="http://localhost:4000/physics/"/>
   <updated>2018-03-14T00:00:00-04:00</updated>
   <id>http://localhost:4000/physics</id>
   <content type="html">&lt;p&gt;Physics is the branch of science that studies the properties and mechanics of matter and energy in  spacetime and their interaction with the 4 fundamental forces.&lt;/p&gt;

&lt;p&gt;There are 3 main divisions of physics:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#classical-physics&quot;&gt;Classical Physics&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#special--general-relativity&quot;&gt;Special &amp;amp; General Relativity&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#quantum-mechanics-qm&quot;&gt;Quantum Mechanics&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/5/56/Modernphysicsfields.svg?style=centerme&quot; alt=&quot;physics&quot; /&gt;&lt;/p&gt;

&lt;!--more--&gt;

&lt;h3 id=&quot;classical-physics&quot;&gt;Classical Physics&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;/classical-physics&quot;&gt;Classical physics&lt;/a&gt; is a catch all for the theories of physics that predated the relativistic and quantum theories of physics we have today. They very adequately describe the interactions and mechanics of everyday objects, fluids, gasses, electricity, magnetism, and even celestial bodies.&lt;/p&gt;

&lt;p&gt;The 3 main branches of pre-Einsteinian physics are:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;/classical-physics#classical-mechanics&quot;&gt;Classical Mechanics&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/classical-physics#classical-electromagnetism&quot;&gt;Classical Electromagnetism&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/classical-physics#thermodynamics&quot;&gt;Thermodynamics&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;special--general-relativity&quot;&gt;Special &amp;amp; General Relativity&lt;/h3&gt;
&lt;h4 id=&quot;special-relativity-sr&quot;&gt;Special Relativity (SR)&lt;/h4&gt;
&lt;p&gt;When the speeds of the objects being studied approach the speed of light, classical physics no longer suffices to accurately describe physical phenomena. In this case it’s necessary to use Einstein’s theory of special relativity, which is wholly derived from two postulates:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The laws of physics are the same for all non-accelerating frames of reference.&lt;/li&gt;
  &lt;li&gt;The speed of light is constant in all reference frames.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The theory models the 3 dimensions of space and 1 dimension of time as a singular 4D spacetime. It explains various high speed phenomena as well the inconsistencies between classical mechanics and the classical theory of electromagnetism.&lt;/p&gt;

&lt;p&gt;SR has been widely successful and has even been incorporated with quantum mechanics to form quantum field theory. That said, special relativity doesn’t explain or deal with the force of gravity, keeping spacetime flat rather than curved.&lt;/p&gt;

&lt;h4 id=&quot;general-relativity-gr&quot;&gt;General Relativity (GR)&lt;/h4&gt;
&lt;p&gt;This shortcoming led to the theory of general relativity. It generalizes special relativity and Newton’s law of gravitation into a coherent theory where the curvature of spacetime itself becomes responsible for the force of gravity. Unfortunately, unlike special relativity, there has been no successful marriage of general relativity and quantum mechanics. A quantum theory of gravity remains a major goal of modern physics.&lt;/p&gt;

&lt;h3 id=&quot;quantum-mechanics-qm&quot;&gt;Quantum Mechanics (QM)&lt;/h3&gt;
&lt;p&gt;When considering matter and energy on very small scales, both classical and relativistic physics break down. This led to the creation of quantum mechanics, which mathematically grounds the mechanics and temporal evolution of subatomic particles. Quantum mechanics itself is not a theory of a particular type of particle like bosons or photons. Think of it as an analogue to classical mechanics which cover the motion and forces that act upon an object.&lt;/p&gt;

&lt;h4 id=&quot;quantum-field-theory&quot;&gt;Quantum Field Theory&lt;/h4&gt;
&lt;p&gt;By combining quantum mechanics and special relativity, we are left with a framework for creating quantum theories of subatomic particles. QFTs like quantum electrodynamics and chromodynamics both describe the interactions of a particular type of fundamental force (electromagnetism and the strong nuclear force respectively). These theories, via QM, model particles as perturbations (waves) on an underlying field that permeates all of space.&lt;/p&gt;

&lt;!-- &lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Quantum Field Theory&lt;/strong&gt;&lt;/summary&gt;
&lt;p&gt;By combining quantum mechanics and special relativity, we are left with a framework for creating quantum theories of subatomic particles. QFTs like quantum electrodynamics and chromodynamics both describe the interactions of a particular type of fundamental force (electromagnetism and the strong nuclear force respectively). These theories, via QM, model particles as perturbations (waves) on an underlying field that permeates all of space.&lt;/p&gt;
&lt;/details&gt; --&gt;

&lt;!-- #### Quantum Electrodynamics (QED)
Quantum electrodynamics is the QFT of the electromagnetic force. It is the quantum analogue to classical electrodynamics and completely describes the interactions between matter and the electromagnetic force (which is mediated by photons). --&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Quantum Electrodynamics (QED)&lt;/strong&gt;&lt;/summary&gt;
&lt;p&gt;Quantum electrodynamics is the QFT of the electromagnetic force. It is the quantum analogue to classical electrodynamics and completely describes the interactions between matter and the electromagnetic force (which is mediated by photons).&lt;/p&gt;
&lt;/details&gt;

&lt;!-- #### Electroweak Theory (EWT)
Electroweak theory is a QFT that provides a unified description of both the electromagnetic and weak nuclear force (The weak force being responsible for the radioactive decay of atoms).

EWT is thus a generalization of QED that adds on the weak force. Right after the big bang, the universe was hot enough that these two forces were one indistinguishable force. The theory describes the electroweak force before that time and the separate electromagnetic and weak forces after that time. --&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Electroweak Theory (EWT)&lt;/strong&gt;&lt;/summary&gt;
&lt;p&gt;Electroweak theory is a QFT that provides a unified description of both the electromagnetic and weak nuclear force (The weak force being responsible for the radioactive decay of atoms).&lt;/p&gt;

&lt;p&gt;EWT is thus a generalization of QED that adds on the weak force. Right after the big bang, the universe was hot enough that these two forces were one indistinguishable force. The theory describes the electroweak force before that time and the separate electromagnetic and weak forces after that time.&lt;/p&gt;
&lt;/details&gt;

&lt;!-- #### Quantum Chromodynamics (QCD)
Quantum chromodynamics is the QFT of the strong nuclear force. This theory describes the interactions between quarks and gluons. The strong nuclear force is the force that keeps the hadrons (which are made up of quarks and gluons) in atomic nuclei bonded together. --&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Quantum Chromodynamics (QCD)&lt;/strong&gt;&lt;/summary&gt;
&lt;p&gt;Quantum chromodynamics is the QFT of the strong nuclear force. This theory describes the interactions between quarks and gluons. The strong nuclear force is the force that keeps the hadrons (which are made up of quarks and gluons) in atomic nuclei bonded together.&lt;/p&gt;
&lt;/details&gt;

&lt;h4 id=&quot;standard-model-of-physics&quot;&gt;Standard Model of Physics&lt;/h4&gt;
&lt;p&gt;The Standard Model is the combination of QED, EQT, and QCD into one unified theory of electromagnetism, the weak nuclear force, and the strong nuclear force. It is humanity’s ultimate theory of the universe. That said, it still lacks a description of gravity. Such a theory of quantum gravity would allow for the creation of a “Theory of Everything” (TOE) that describes all four fundamental forces in tandem.
&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/0/00/Standard_Model_of_Elementary_Particles.svg?style=centerme&quot; alt=&quot;standardmodel&quot; /&gt;&lt;/p&gt;

&lt;!-- ![xkcd](https://imgs.xkcd.com/comics/turn-on.png?style=centerme)
&lt;center&gt;&lt;i&gt;Relevant xckd&lt;/i&gt;&lt;/center&gt; --&gt;

&lt;!-- &lt;details open&gt;
&lt;summary&gt;&lt;strong&gt;Standard Model of Physics&lt;/strong&gt;&lt;/summary&gt;
&lt;p&gt;The Standard Model is the combination of all the QFT into one unified theory of electromagnetism, the weak nuclear force, and the strong nuclear force. It is humanity's ultimate theory of the universe. That said, it still lacks a description of gravity. Such a theory of quantum gravity would allow for the creation of a &quot;Theory of Everything&quot; (TOE) that describes all four fundamental forces in tandem.&lt;/p&gt;
&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/0/00/Standard_Model_of_Elementary_Particles.svg?style=centerme&quot; alt=&quot;standard model pic&quot;&gt;&lt;/img&gt;
&lt;/details&gt; --&gt;
</content>
 </entry>
 
 <entry>
   <title>Minkowski Spacetime</title>
   <link href="http://localhost:4000/minkowski-spacetime/"/>
   <updated>2018-01-23T00:00:00-05:00</updated>
   <id>http://localhost:4000/minkowski-spacetime</id>
   <content type="html">&lt;h2 id=&quot;what-is-spacetime&quot;&gt;What is Spacetime?&lt;/h2&gt;
&lt;p&gt;Before we talk about Minkowski Spacetime, we need to address what it means to combine space and time.&lt;/p&gt;

&lt;p&gt;Imagine the Earth orbiting the sun. At every point in time it will be in a different point in its orbit:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.pitt.edu/~jdnorton/teaching/HPS_0410/chapters/spacetime/planet3d.gif?style=centerme&quot; alt=&quot;orbit2D&quot; /&gt;&lt;/p&gt;

&lt;p&gt;If we stack these slices of time in a third (temporal) dimension, it would look like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.pitt.edu/~jdnorton/teaching/HPS_0410/chapters/spacetime/planet4d.gif?style=centerme&quot; alt=&quot;orbit3D&quot; /&gt;&lt;/p&gt;

&lt;!-- *It looks similar to euler's formula, $e^{i\theta}=\cos \theta + i\sin\theta$, with space being the complex plane and time being the angle $\theta$. The imaginary part will come in handy later.* --&gt;

&lt;p&gt;This is an example of a 3D spacetime. It has 2 dimensions of space and 1 of time. Our universe has 3 space and 1 time dimension, making our spacetime (referred to as &lt;em&gt;Minkowski Spacetime&lt;/em&gt;) 4 dimensional.&lt;/p&gt;

&lt;!--more--&gt;

&lt;h4 id=&quot;why-do-this&quot;&gt;Why do this?&lt;/h4&gt;
&lt;p&gt;Spacetime serves as a useful mathematical and intuitive model of the universe that we can use to reason about physics, particularly Einstein’s special relativity. Indeed, it was Einstein’s former teacher &lt;a href=&quot;https://en.wikipedia.org/wiki/Hermann_Minkowski&quot;&gt;Hermann Minkowski&lt;/a&gt; that came up with spacetime as a tool to understand his pupil’s breakthrough theory.&lt;/p&gt;

&lt;h2 id=&quot;definition&quot;&gt;Definition&lt;/h2&gt;
&lt;p&gt;Formally speaking, Minkowski spacetime (which I’ll simply refer to as &lt;em&gt;spacetime&lt;/em&gt;) is an &lt;a href=&quot;\abstract-algebra&quot;&gt;algebraic structure&lt;/a&gt; that represents space purely in mathematical terms. In particular it is a 4D &lt;a href=&quot;https://en.wikipedia.org/wiki/Vector_space&quot;&gt;vector space&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Points in this space are represented by vectors and are referred to as &lt;strong&gt;events&lt;/strong&gt;. The naming stems from the fact that these points represents both moments in time as well as locations in space.&lt;/p&gt;

&lt;!-- Remove link to vector space wikipedia --&gt;
&lt;!-- *If you don't know what a vector space is, I've written a post about it [here](\2018\01\20\vector-spaces).* --&gt;

&lt;p&gt;An event, usually denoted $s$, is represented by a 4D vector where $x,y,z,t$ are real numbers:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;s=(x,y,z,ict)&lt;/script&gt;

&lt;p&gt;You may have noticed that the last coordinate, which corresponds to time, is multiplied by a factor of $ic$. $i$, as we know, is the imaginary unit and multiplying it by time makes the fourth coordinate an imaginary number. The fourth coordinate always being an imaginary number means that spacetime consists of 3 real spatial dimensions and one imaginary temporal dimension.&lt;/p&gt;

&lt;p&gt;$c$ represents the speed of light (299,792,458 m/s) and when multiplied by a time value (like seconds) it gives the distance a photon would travel in that time. This is necessary to convert from time units to length units. It wouldn’t really make sense to have a position vector with different units for dimensions, as we’ll see…&lt;/p&gt;

&lt;h4 id=&quot;an-example&quot;&gt;An Example&lt;/h4&gt;
&lt;p&gt;Say you walk 2m forward out of your house, walk 3 meters to the right, and jump (an impressive) 1 meter into the air. Let’s also say that at the moment you were 1 meter in the air, 6 seconds had passed.&lt;/p&gt;

&lt;p&gt;Setting the event of you leaving your house as the origin, the coordinates of the event of you jumping in the air are:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;x = 2m (forward)&lt;/li&gt;
  &lt;li&gt;y = 3m (to the right)&lt;/li&gt;
  &lt;li&gt;z = 1m (in the air)&lt;/li&gt;
  &lt;li&gt;t = 6s (had passed since you left the house)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Thus that event in spacetime is represented by the ordered quadruplet:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
  s &amp;= (2,3,1,6\text{c}i) \\
  &amp;= (2,3,1,1.80i\times 10^9) \\
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;Note that the time coordinate is much bigger in magnitude due to how large the speed of light is.&lt;/p&gt;

&lt;p&gt;In this framework you can, loosely, consider 1 second to be equivalent to 299,792,458 imaginary meters.&lt;/p&gt;

&lt;h2 id=&quot;proper-distance&quot;&gt;Proper Distance&lt;/h2&gt;
&lt;p&gt;The distance between two points in space is given as such:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Delta d=\sqrt{(\Delta x)^2+(\Delta y)^2+(\Delta z)^2}&lt;/script&gt;

&lt;p&gt;Where $\Delta x,\Delta y,\Delta z$ represent the distance between the two $x/y/z$ coordinates of the two points. (i.e $\Delta x = x_2 - x_1$)&lt;/p&gt;

&lt;p&gt;But special relativity tells us that space and time are relative and so it might be useful to have a notion of ‘distance’ for events in space&lt;em&gt;time&lt;/em&gt; and not just space.&lt;/p&gt;

&lt;p&gt;This analogue is called &lt;strong&gt;proper distance&lt;/strong&gt; (denoted $\Delta s$) and we obtain it just like we did the one above. Square each coordinate, add them together, and take that sum’s square root:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Delta s=\sqrt{(\Delta x)^2+(\Delta y)^2+(\Delta z)^2-\text{c}^2(\Delta t)^2}&lt;/script&gt;

&lt;p&gt;This measure of ‘distance’ is similar to the standard spatial one, barring the strange looking last term $-\text{c}^2(\Delta t)^2$. Upon further consideration, however, this term makes just as much sense as the others:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
(x)^2&amp;=x^2 \\
(y)^2&amp;=y^2 \\
(z)^2&amp;=z^2 \\
(i\text{c}t)^2&amp;=i^2\text{c}^2t^2=-\text{c}^2 t^2
\end{align*} %]]&gt;&lt;/script&gt;

&lt;h4 id=&quot;an-example-1&quot;&gt;An Example&lt;/h4&gt;
&lt;p&gt;Say you fly a spaceship 3000km into space from earth (the origin), let’s call that the y-axis, in the span of 1ms (this is a very fast spaceship).&lt;/p&gt;

&lt;p&gt;So since earth is the origin these are the change in coordinates:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;$\Delta x = 0m$&lt;/li&gt;
  &lt;li&gt;$\Delta y = 4000km = 4000000m$&lt;/li&gt;
  &lt;li&gt;$\Delta z = 0m$&lt;/li&gt;
  &lt;li&gt;$\Delta t = 1ms = .01s$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Thus the proper distance between the origin and that event in spacetime is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
  \Delta s &amp;= \sqrt{0^2+(3.0\times 10^6)^2+0^2+((.01)\text{c}i)^2} \text{ meters} \\
  &amp;= \sqrt{(4.0\times 10^6)^2+(2.997i\times 10^6)^2} \text { meters} \\
  &amp;= \sqrt{(4.0\times 10^6)^2-(2.997\times 10^6)^2} \text { meters} \\
  &amp;= 2.65\times 10^6 \text { meters} \\
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;Remember this distance isn’t through space but spacetime, so it doesn’t have quite the same meaning as you may first think.&lt;/p&gt;

&lt;h4 id=&quot;proper-time&quot;&gt;Proper Time&lt;/h4&gt;
&lt;p&gt;If you take a good look at the formula for proper distance you’ll notice a problem: What if radicand is negative?&lt;/p&gt;

&lt;p&gt;Take the example of jumping in the air form earlier. If we use the proper distance formula with it we get:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
  \Delta s &amp;= \sqrt{2^2+3^2+1^2+(6\text{c}i)^2} \text{ meters} \\
  &amp;= \sqrt{4+9+1+(1.799i\times 10^9)^2} \text { meters} \\
  &amp;= \sqrt{4+9+1-(1.799\times 10^9)^2} \text { meters} \\
  &amp;= \sqrt{-3.236\times 10^{18}} \text { meters} \\
  &amp;= 1.799i\times 10^9 \text { meters} \\
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;While we did accept the idea of imaginary time earlier, we cannot accept an imaginary proper distance. I mean the whole point was to get rid of the imaginary part right? So where did we go wrong? Well as it turns out, spacetime is a bit more complicated than I first led on (how surprising).&lt;/p&gt;

&lt;p&gt;Two events in spacetime have the property of being either spacelike seperated or timelike seperated. The difference between them is essentially whether the two events could be causally connected (they are close enough to be affected by a particle traveling at the speed of light). See &lt;a href=&quot;https://en.wikipedia.org/wiki/Light_cone&quot;&gt;light cones&lt;/a&gt; for more.&lt;/p&gt;

&lt;p&gt;The proper distance is the formula used for &lt;em&gt;spacelike&lt;/em&gt; separated events. Another quantity, known as &lt;a href=&quot;https://en.wikipedia.org/wiki/Proper_time&quot;&gt;proper time&lt;/a&gt;, is used to measure the length between two &lt;em&gt;timelike&lt;/em&gt; events in spacetime.&lt;/p&gt;

&lt;h2 id=&quot;asides&quot;&gt;Asides&lt;/h2&gt;
&lt;p&gt;What I’ve described in this post is a very basic notion of spacetime and how one may codify events in it. There are many more aspects of spacetime and events in it like 4-velocities (the spacetime equivalent to velocity), 4-accelerations, proper time, Lorentz transformations, etc.&lt;/p&gt;

&lt;p&gt;Also note that our normal notion of distance and time still apply to spacetime. All that’s different is the fact that space and time are not invariant. That is to say, the regular old spatial &lt;em&gt;distance&lt;/em&gt; between two events can change depending on your reference frame, but the &lt;em&gt;proper distance&lt;/em&gt; between them is constant in all reference frames.&lt;/p&gt;

&lt;h4 id=&quot;regarding-imaginary-time&quot;&gt;Regarding Imaginary Time&lt;/h4&gt;
&lt;p&gt;You may have felt a little uneasy when I introduced time as being an “imaginary 4th dimension” of spacetime or a second being equivalent to 299,792,458 imaginary meters. If so, you aren’t alone. Physicists aren’t so fond of concepts like imaginary time and so they’ve devised ways of getting around that, namely with something called a &lt;a href=&quot;https://en.wikipedia.org/wiki/Metric_signature&quot;&gt;metric signature&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This has its benefits but, for our purposes, Minkowski spacetime is easier to understand if we consider time to be an imaginary dimension. It melds well with the definition of proper length.&lt;/p&gt;

&lt;p&gt;Moreover, I think considering time as an imaginary dimension provides a useful analogy for its ephemeral nature. We can’t &lt;em&gt;move&lt;/em&gt; through time like we can through space. This is analogous to us not being able to directly measure imaginary quantities like we can with real ones.&lt;/p&gt;

&lt;!-- #### Regarding Vector Spaces
Also while Minkowski Spacetime is generally considered a vector space, if the set of values that the coordinates come from aren't frost he same Field of numbers (the space coordinates come from the real numbers and the time coordinate comes from the imaginary numbers) then it technically can't be considered a vector space. This is another blow to the imaginary time version of spacetime. --&gt;
</content>
 </entry>
 
 <entry>
   <title>Compression, Images, and FileToPNG</title>
   <link href="http://localhost:4000/filetopng/"/>
   <updated>2018-01-16T00:00:00-05:00</updated>
   <id>http://localhost:4000/filetopng</id>
   <content type="html">&lt;h2 id=&quot;what-is-filetopng&quot;&gt;What is FileToPNG?&lt;/h2&gt;
&lt;p&gt;FileToPNG is a tool I wrote in Java that converts the raw binary representation of any file into a corresponding PNG image representation. The resulting PNG (being a lossless file type) can be sent through text, email, etc. before finally being reconstructed back into its original form.&lt;/p&gt;

&lt;p&gt;The project’s GitHub repository can be found &lt;a href=&quot;https://github.com/ozanerhansha/FileToPNG&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;While FileToPNG’s main purpose was to be a tool that allowed for the sending of files over image only communication channels, I’ve found that, based on what type of files are put into it, the resulting images show some interesting patterns. I will elaborate on a couple of these findings in this post.&lt;/p&gt;

&lt;!--more--&gt;

&lt;h2 id=&quot;the-main-algorithm&quot;&gt;The Main Algorithm&lt;/h2&gt;
&lt;p&gt;While I have programmed a primitive UI (the code for which can be found &lt;a href=&quot;https://github.com/ozanerhansha/FileToPNG/blob/master/src/Main.java&quot;&gt;here&lt;/a&gt;) to make using the tool easier, it’s not the main focus of FileToPNG and is really just boilerplate Java.&lt;/p&gt;

&lt;p&gt;The real meat of the program is the conversion functions for PNG to File and vice versa. The code for this can be found &lt;a href=&quot;https://github.com/ozanerhansha/FileToPNG/blob/master/src/GFile.java&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The algorithm goes something like this:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Convert the file into a string of bytes (8 bits = 1 byte).&lt;/li&gt;
  &lt;li&gt;Read through this list and make a pixel out of every 3 bytes. This can be done because a pixel is comprised of 3 colors (red, green, and blue) each with a value from 0-255. These 256 values can be represented in 8 bits (because 2&lt;sup&gt;8&lt;/sup&gt;=256).&lt;/li&gt;
  &lt;li&gt;Once a list of pixels (which should be about a 1/3 of the size of the byte list) has been made, create a square image (in .png format) out of these pixels.&lt;/li&gt;
  &lt;li&gt;Account for files whose bytes aren’t a multiple of 3. (I used special ‘2 byte’ and ‘1 byte’ pixels that have different alpha values to demarcate them)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Reconstructing the images into files is as simple as reversing this process.&lt;/p&gt;

&lt;h4 id=&quot;side-note-on-function-mapping&quot;&gt;Side Note on Function Mapping&lt;/h4&gt;
&lt;p&gt;You can think of the program as a function that maps the set of all &lt;a href=&quot;https://en.wikipedia.org/wiki/Kleene_star&quot;&gt;finite bit strings&lt;/a&gt; to the set of all square images. Because this image is a square of pixels with 4 layers (red, green, blue, and alpha) we can represent it as the set of all rank 3 tensors with size $n\times n\times 4$ and integer elements from $0$ to $255$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{FileToPNG}:\{0,1\}^+\to \left(\mathbb{N}_{255}\right)^{n\times n\times 4}&lt;/script&gt;

&lt;h4 id=&quot;side-note-on-exclusion-of-alpha-channel&quot;&gt;Side Note on Exclusion of Alpha Channel&lt;/h4&gt;
&lt;p&gt;You may notice that while .png files have 4 color channels (red, green, blue and alpha), I only use 3 to store the data of the file and leave the alpha channel to indicate whether a given pixel only holds 1 or 2 bytes of information rather than 3.&lt;/p&gt;

&lt;p&gt;This is obviously inefficient and I’m sure there are better ways of representing the file as a png while simultaneously taking advantage of all 4 channels, effectively saving 25% more space for each pixel. One way to do this might have to do with the dimensions of the image. There is no reason for the image to be a square.&lt;/p&gt;

&lt;p&gt;I haven’t done anything about this because, in its current state, the algorithm is better suited to exploring the representations of the data it encodes rather than efficiently compressing them in image form. Having the alpha channel available would make the resulting images (and information they represent) harder to visualize.&lt;/p&gt;

&lt;h2 id=&quot;properties-of-png-representations&quot;&gt;Properties of PNG Representations&lt;/h2&gt;
&lt;p&gt;FileToPNG doesn’t do any sort of encrypting or compression of the file’s data (as of yet). As such, the PNG representation it spits out can give us an unfiltered look at the file’s structure.&lt;/p&gt;

&lt;h3 id=&quot;text-files&quot;&gt;Text Files&lt;/h3&gt;
&lt;p&gt;Take text files for example. Their content, of course, varies widely depending on the length and topic of discussion. But, even if they don’t use the same language, they all still use the same set of Unicode characters which all have the same configuration of bits. Meaning they share some similarities:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/projects/filetopng/text_diagram.png?style=centerme&quot; alt=&quot;Text in FileToPNG&quot; width=&quot;500px&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;music-files&quot;&gt;Music Files&lt;/h3&gt;
&lt;p&gt;This one was kind of surprising. When I took an .mp3 file of a song and put it through the program, the resulting image was seemingly random:&lt;/p&gt;

&lt;!-- ![mp3](/assets/projects/filetopng/song_mp3.png?style=centerme){:width=&quot;300px&quot;}

### 1MB file of random noise for comparison:
![Random Noise](/assets/projects/filetopng/random_data.png?style=centerme){:width=&quot;300px&quot;} --&gt;

&lt;p&gt;&lt;img src=&quot;/assets/projects/filetopng/random_music.png?style=centerme&quot; alt=&quot;mp3&quot; width=&quot;700px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I was disappointed at first, but then I realized that randomness is fundamentally information packed. That is to say, if the PNG representation had some sort of overarching structure, then that would imply that there is still some redundancy in the file that could be compressed, thereby making the file smaller and more random looking.&lt;/p&gt;

&lt;p&gt;This lines up with the fact that mp3 files are &lt;strong&gt;lossy&lt;/strong&gt; (i.e they sacrifice perfect quality for a smaller size).&lt;/p&gt;

&lt;!-- ![xkcd](https://imgs.xkcd.com/comics/digital_data.png?style=centerme){:width=&quot;600px&quot;}
&lt;center&gt;&lt;i&gt;Relevant xckd&lt;/i&gt;&lt;/center&gt; --&gt;

&lt;p&gt;The natural step forward at that point was to get a .wav version of the song, which is &lt;strong&gt;lossless&lt;/strong&gt;, and see if it had any structure. And, as expected, it did:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/projects/filetopng/song_wav.png?style=centerme&quot; alt=&quot;wav&quot; width=&quot;600px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Note the repeating bands of different ‘frequencies.’ The image was also much bigger in area due to being lossless (the size difference isn’t reflected here because the image would extend out of the webpage).&lt;/p&gt;

&lt;h4 id=&quot;qualitative-measure-of-information-density&quot;&gt;Qualitative Measure of Information Density&lt;/h4&gt;
&lt;p&gt;It is in this way that we can examine how “densely packed” with information a given file is. If we see patterns in the colors or structure of the image, we know there can be more compression done. However, if the image is indistinguishable from random noise, then it’s probably rich in information.&lt;/p&gt;

&lt;h3 id=&quot;nintendo-64-roms&quot;&gt;Nintendo 64 ROMs&lt;/h3&gt;
&lt;p&gt;As a final test, I thought I would try converting a couple of Nintendo 64 games (in file form) that were on my desktop to PNGs and, unsurprisingly, they share many similarities:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/projects/filetopng/n64_diagram.png?style=centerme&quot; alt=&quot;N64 Roms in FileToPNG&quot; width=&quot;500px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Interested, I did some research into N64 cartridges and found out that all ‘ROM dumps’ (files that were created by copying a physical N64 cartridge onto a computer) have what’s called a &lt;em&gt;header&lt;/em&gt;. This header includes custodial information like the game’s version, internal name, and other bits of information that might be interesting to a game historian. These headers are the black boxes at the top of &lt;em&gt;Super Mario 64&lt;/em&gt; and &lt;em&gt;Mario Kart 64&lt;/em&gt; respectively.&lt;/p&gt;

&lt;p&gt;You may also notice that the &lt;em&gt;Super Mario 64&lt;/em&gt; rom is smaller than the &lt;em&gt;Mario Kart 64&lt;/em&gt; one, implying that it has a smaller file size. Apparently, N64 games came on a variety of hardware with different storage capacities and capabilities. This may seem normal nowadays but in an era where all games had to work on the same piece of limited hardware, this variability is pretty amazing.&lt;/p&gt;

&lt;p&gt;Another similarity can be spotted after the last black bar, at then bottom of the images. Past this point, both the ROMs consist of ‘garbage data’ meant to fill up the game cartridge they were stored on. You can tell because this section looks more like the random sample then the rest of the picture.&lt;/p&gt;

&lt;!-- ![xkcd](https://imgs.xkcd.com/comics/file_extensions.png?style=centerme)
&lt;center&gt;&lt;i&gt;Relevant xckd&lt;/i&gt;&lt;/center&gt; --&gt;

&lt;!-- ![xkcd](https://imgs.xkcd.com/comics/porn.png?style=centerme)
&lt;center&gt;&lt;i&gt;Relevant xckd&lt;/i&gt;&lt;/center&gt; --&gt;
</content>
 </entry>
 
 <entry>
   <title>Breast Cancer Classification</title>
   <link href="http://localhost:4000/breast-cancer-classification/"/>
   <updated>2018-01-15T00:00:00-05:00</updated>
   <id>http://localhost:4000/breast-cancer-classification</id>
   <content type="html">&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;
&lt;p&gt;I made this model to test how easily I could use external datasets to create and train a neural network with Google’s &lt;a href=&quot;https://www.tensorflow.org/&quot;&gt;&lt;em&gt;TensorFlow&lt;/em&gt;&lt;/a&gt; library. In this post I’ll attempt to explain the code, its results, and its accuracy. The &lt;a href=&quot;https://github.com/ozanerhansha/NeuralNetworks/blob/master/src/test/bcDiagnosis.py&quot;&gt;entire program&lt;/a&gt; is in my &lt;a href=&quot;https://github.com/ozanerhansha/NeuralNetworks&quot;&gt;NeuralNetwork&lt;/a&gt; repository on GitHub as well as at the end of this post.&lt;/p&gt;

&lt;h2 id=&quot;the-training-data&quot;&gt;The Training Data&lt;/h2&gt;
&lt;p&gt;All the training data comes from the &lt;a href=&quot;http://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+%28diagnostic%29&quot;&gt;Wisconsin Breast Cancer Data Set&lt;/a&gt;, hosted by the University of California’s &lt;a href=&quot;http://archive.ics.uci.edu/ml/index.php&quot;&gt;Machine Learning Repository&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The training data consists of 569 subjects each with 3 values (mean, worst value, and standard error) for each of 10 categories:&lt;/p&gt;

&lt;!--more--&gt;

&lt;ul&gt;
  &lt;li&gt;Radius (mean of distances from center to points on the perimeter)&lt;/li&gt;
  &lt;li&gt;Texture (standard deviation of gray-scale values)&lt;/li&gt;
  &lt;li&gt;Perimeter&lt;/li&gt;
  &lt;li&gt;Area&lt;/li&gt;
  &lt;li&gt;Smoothness (local variation in radius lengths)&lt;/li&gt;
  &lt;li&gt;Compactness $(\frac{perimeter^2}{area} - 1)$&lt;/li&gt;
  &lt;li&gt;Concavity (severity of concave portions of the contour)&lt;/li&gt;
  &lt;li&gt;Concave points (number of concave portions of the contour)&lt;/li&gt;
  &lt;li&gt;Symmetry&lt;/li&gt;
  &lt;li&gt;Fractal dimension (measure of edge complexity)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;The meaning of these values is irrelevant to our purpose. All we need to know is that we can use these $10\times3=30$ values to predict whether the given breast cancer sample is malignant or benign.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;All the training examples are stored on a CSV (Comma Separated Value) file called &lt;code class=&quot;highlighter-rouge&quot;&gt;wdbc.data&lt;/code&gt;, so our first job is to import the file into our program:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;os&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;#Used for finding file path&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;#Get dataset file path&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dirname&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__file__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;file_name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'wdbc.data'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;file_object&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;file_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'r'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;cleaning-the-data&quot;&gt;Cleaning the Data&lt;/h2&gt;
&lt;p&gt;Now that it’s imported, we can start ‘cleaning’ the data (that is turning this large string of numbers into a list of training examples labeled  &lt;strong&gt;benign&lt;/strong&gt; or &lt;strong&gt;malignant&lt;/strong&gt;).&lt;/p&gt;

&lt;p&gt;First off we have to split up the dataset, which is currently just one massive string, into a bunch of strings. One for each subject (patient):&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;#Split dataset into separate points (as strings)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;string_points&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;file_object&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;string_points&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If you take a look at the actual &lt;a href=&quot;https://github.com/ozanerhansha/NeuralNetworks/blob/master/src/test/wdbc.data&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;wdbc.data&lt;/code&gt;&lt;/a&gt; file, you’ll see that some of the subjects are clustered in groups of benign and malignant. If we trained the network with the subjects in this order, it would bias the network’s guesses. To avoid this, we’ll shuffle the subjects:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shuffle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;string_points&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;#Randomize (avoid bias)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Next we create a list of &lt;strong&gt;feature vectors&lt;/strong&gt;, each with 30 entries. A feature vector is simply a list of all the data we have about a particular problem, or in this case a particular subject. Its purpose is to be transformed into the desired answer by the neural network. It’s the &lt;strong&gt;input&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;We will also create a list of $y$ labels (&lt;code class=&quot;highlighter-rouge&quot;&gt;[0,1]&lt;/code&gt; for benign and &lt;code class=&quot;highlighter-rouge&quot;&gt;[1,0]&lt;/code&gt; for malignant) that correspond with the list of feature vectors. This is the desired &lt;strong&gt;output&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;We’ll be using the &lt;a href=&quot;http://www.numpy.org&quot;&gt;numpy&lt;/a&gt; library here, so let’s import it as well:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;num_input_features&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;30&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;#30 data points per subject&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;#Initialize dataset class arrays&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;point_array&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;empty&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_input_features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y_labels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;empty&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;#Trim data points&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#Format as np.arrays&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#Add to class arrays (Benign or Malignant)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;point&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;string_points&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;point&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;point&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;','&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'M'&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;point&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;#if malignant&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;y_labels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;#if benign (can only be labeled 'B' or 'M')&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;y_labels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;point&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;point&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;#trim for only the 10 important features&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;temp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;point&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;#convert to numpy array&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;temp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;temp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;astype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;#cast as float array&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;point_array&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;point_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;temp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;All that’s left is to split up the subjects (the list of feature vectors) into training and testing sets. Testing using the same data the network was trained on encourages the network to memorize the data rather than generalize the dataset and make meaningful predictions.&lt;/p&gt;

&lt;p&gt;Of the 569 subjects, the first 400 will be used to train the network while the remaining 169 will be used to test the network’s accuracy:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;#Split training and testing data&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;experience_matrix&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;point_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;400&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;experience_matrix_y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;400&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;test_matrix&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;point_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;400&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;test_matrix_y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;400&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;constructing-the-network&quot;&gt;Constructing the Network&lt;/h2&gt;
&lt;p&gt;A single layer perceptron network seems to do well enough in fitting the test data, so we’ll build one here. The standard model for such a network is given by:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{y}=\text{softmax}(Wx+b)&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;$x$ is the input (a 30 dimensional vector of all the cancer cell’s statistical values).&lt;/li&gt;
  &lt;li&gt;$\hat{y}$ is the network’s approximation of $y$, the right answer. (a $2$D vector with the first/second value corresponding to its confidence that the cell is benign/malignant).&lt;/li&gt;
  &lt;li&gt;$W$ is a matrix of weights (the matrix is $30\times2$ so that it transforms $30$ dimensional vectors into $2$ dimensional ones).&lt;/li&gt;
  &lt;li&gt;$b$ is a vector of ‘bias’ values (a $2$D vector that allows the network more freedom when training, similar to the y-intercept in a linear equation).&lt;/li&gt;
  &lt;li&gt;The $\text{softmax}$ function is the network’s &lt;a href=&quot;https://en.wikipedia.org/wiki/Activation_function&quot;&gt;activation function&lt;/a&gt;, which introduces a nonlinearity to the network. This is integral for any neural network to learn from the data it’s provided. The function also equalizes the network’s confidence predictions so that they add up to 100%. The mathematical description of softmax is:&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/projects/breast-cancer/softmax.png&quot; alt=&quot;softmax graph&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here’s what the model looks like in python with &lt;a href=&quot;https://www.tensorflow.org/&quot;&gt;TensorFlow&lt;/a&gt;:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;tensorflow&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;#nx10 Matrix (Input)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;placeholder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_input_features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#10x1 Matrix (Weights)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Variable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_input_features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#1x2 vector (Bias)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Variable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#A scalar (x*W + b)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;y_noSoftmax&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matmul&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;#Apply softmax (scales y_noSoftmax to be between 0 &amp;amp; 1)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y_hat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;softmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_noSoftmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'y_Hat'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;em&gt;Note that we’ve computed $Wx+b$ first before computing its $\text{softmax}$. Separating these steps will allow us to compute the error function more easily with the TensorFlow library later on.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;training-the-network&quot;&gt;Training the Network&lt;/h2&gt;
&lt;p&gt;Now that we’ve assembled the network, we need to train it using the training data, or &lt;code class=&quot;highlighter-rouge&quot;&gt;experience_matrix&lt;/code&gt;, and the associated training labels we made earlier.&lt;/p&gt;

&lt;p&gt;TensorFlow works by creating what is called a computational graph from which it can calculate and find derivatives of every value in the network, thereby allowing it to optimize (i.e teach) the neural network via backpropagation &lt;a href=&quot;http://colah.github.io/posts/2015-08-Backprop/&quot;&gt;(Christopher Olah’s blog has a great post on this)&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;And so, the network won’t start until we create a new graph (session) on which all the training calculations can run:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;#Session&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;InteractiveSession&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;#Create Session&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;global_variables_initializer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;#Init Variables&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Next we create the actual training model. First we create a placeholder for the labels of the training data, $y$.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;#nx2 Matrix (One-Hot Vector, Label Data)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;placeholder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'y_Labeled'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Then we create a loss function. A loss function is basically a measure of how off the network is in its guesses. This means the smaller the output of this function, the more accurate our network becomes. So, naturally, if we minimize this function we will have effectively &lt;em&gt;trained&lt;/em&gt; the network. This is deep learning in a nutshell. Here we use &lt;strong&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Cross_entropy&quot;&gt;Cross Entropy&lt;/a&gt;&lt;/strong&gt;, as it works nicely with the softmax layer we have at the end.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;#Loss Function (cross entropy between y and y_hat)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;cross_entropy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reduce_mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;softmax_cross_entropy_with_logits&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;logits&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_noSoftmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Loss_Function'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Finally we create the training step. This is where we choose what optimization method to use. In this case we’ll use the tried and true &lt;strong&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Gradient_descent&quot;&gt;Gradient Descent&lt;/a&gt;&lt;/strong&gt;. I reccommend you look into it yourself, but the gist is that we take partial derivatives of the cost function with respect to every weight variable in the network then slightly adjust them in the direction that would minimize the cost.&lt;/p&gt;

&lt;p&gt;This algorithm approaches the local minimum rapidly at first but then slows down once it has gotten close. Here’s a visualization:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/projects/breast-cancer/gradient_descent_3D_alpha.gif?style=centerme&quot; alt=&quot;SGD&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And here it is in tensorflow:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;#Performs Gradient Descent on loss function&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;train_step&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;GradientDescentOptimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;minimize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;cross_entropy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Train_Step'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now that we have a training step (a single iteration of gradient descent) we can just run that operation, say, 1000 times for each of the 400 training examples.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;#Run train step repeatedly&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
  &lt;span class=&quot;c&quot;&gt;#Run training step on the entire batch&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;train_step&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feed_dict&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;experience_matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;experience_matrix_y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Also note that in the code above, this is where we finally plug our training data (&lt;code class=&quot;highlighter-rouge&quot;&gt;experience_matrix&lt;/code&gt;) and labels (&lt;code class=&quot;highlighter-rouge&quot;&gt;experience_matrix_y&lt;/code&gt;) into the network. That means that everything we’ve done (in terms of constructing/training the model) can be generalized to many other machine learning problems.&lt;/p&gt;

&lt;h2 id=&quot;assessing-the-network&quot;&gt;Assessing the Network&lt;/h2&gt;
&lt;p&gt;To assess how accurate our network is, we simply have to plug in our testing data (&lt;code class=&quot;highlighter-rouge&quot;&gt;test_matrix&lt;/code&gt;) and labels (&lt;code class=&quot;highlighter-rouge&quot;&gt;test_matrix_y&lt;/code&gt;) into the network and pick the catagory, benign or malignant, that it is most confident in.&lt;/p&gt;

&lt;p&gt;First we’ll make a list of Booleans which represent whether the network got the right answer or not:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;#Returns whether model made correct prediction (List of booleans)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;correct_prediction&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;equal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;argmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_hat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;argmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'isCorrect'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Then we average all the results together to arrive at an percent accuracy:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;#Average of correct prediction (%Accuracy)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;accuracy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reduce_mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cast&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;correct_prediction&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Accuracy'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Finally, we just plug in our testing data and labels and print the accuracy (with a bit of pretty formatting) to the console.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;#Print Accuracy&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Accuracy:&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'{0:.2&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;}'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;accuracy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;eval&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feed_dict&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_matrix_y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;})))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;When we finally run the code our output should look something like this:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Accuracy: 90.53%&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Accuracy: 91.12%&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Accuracy: 88.17%&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Because we randomized our sample at the start, the accuracy of the network varies slightly each run.&lt;/p&gt;

&lt;p&gt;That said the slight increase/decrease in accuracy are just products of how the weights work out with the examples. As such any perceived improvement is probably coincidental and don’t reflect how accurate the network will be in the face of new samples. To get an accurate view of the network’s ability, running it multiple times and taking the average of its accuracy is probably your best bet.&lt;/p&gt;

&lt;p&gt;Doing this for our network yields about a &lt;strong&gt;90% accuracy&lt;/strong&gt;. Not bad. For reference, a monkey (that is, a random process) would classify the cells correctly 50% of the time (there are only two categories). That’s a 40% increase over randomly guessing!&lt;/p&gt;

&lt;h2 id=&quot;the-full-code&quot;&gt;The Full Code&lt;/h2&gt;
&lt;p&gt;This is all the code put together and is how it appears on my &lt;a href=&quot;https://github.com/ozanerhansha/NeuralNetworks&quot;&gt;NeuralNetwork&lt;/a&gt; repository. All the imports have been moved to the top of the program:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;s&quot;&gt;'''
Created on Jun 6, 2017
@author: Ozaner Hansha
'''&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;os&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;tensorflow&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;random&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;#Get dataset file path&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dirname&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__file__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;file_name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'wdbc.data'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;file_object&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;file_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'r'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;#Split dataset into separate points (as strings)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;string_points&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;file_object&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;string_points&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shuffle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;string_points&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;#Randomize (avoid bias)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;num_input_features&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;30&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;#Initialize dataset class arrays&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;point_array&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;empty&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_input_features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y_labels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;empty&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;#Trim data points&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#Format as np.arrays&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#Add to class arrays (Benign or Malignant)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;point&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;string_points&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;point&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;point&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;','&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'M'&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;point&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;#if malignant&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;y_labels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;#if benign (can only be labeled 'B' or 'M')&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;y_labels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;point&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;point&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;#trim for only the 10 important features&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;temp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;point&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;#convert to numpy array&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;temp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;temp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;astype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;#cast as float array&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;point_array&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;point_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;temp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;#Split training and testing data&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;experience_matrix&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;point_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;400&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;experience_matrix_y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;400&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;test_matrix&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;point_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;400&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;test_matrix_y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;400&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# End of data import/cleanup. Begin construction of neural network&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name_scope&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Hidden_Layer'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
  &lt;span class=&quot;c&quot;&gt;#nx10 Matrix (Input)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;placeholder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_input_features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
  &lt;span class=&quot;c&quot;&gt;#10x1 Matrix (Weights)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Variable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_input_features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;
  &lt;span class=&quot;c&quot;&gt;#1x2 vector (Bias)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Variable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;
  &lt;span class=&quot;c&quot;&gt;#A scalar (x*W + b)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;y_noSoftmax&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matmul&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;#Apply softmax (scales y_noSoftmax to be between 0 &amp;amp; 1)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y_hat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;softmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_noSoftmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'y_Hat'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;#Session&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;InteractiveSession&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;#Create Session&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;global_variables_initializer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;#Init Variables&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;#Training Model&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name_scope&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Training'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
  &lt;span class=&quot;c&quot;&gt;#nx2 Matrix (One-Hot Vector, Label Data)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;placeholder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'y_Labeled'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;c&quot;&gt;#Loss Function (cross entropy between y and y_hat)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;cross_entropy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reduce_mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;softmax_cross_entropy_with_logits&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;logits&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_noSoftmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Loss_Function'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;c&quot;&gt;#Performs Gradient Descent on loss function&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;train_step&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;GradientDescentOptimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;minimize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;cross_entropy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Train_Step'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;#Run train step repeatedly&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
  &lt;span class=&quot;c&quot;&gt;#Run training step on that batch&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;train_step&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feed_dict&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;experience_matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;experience_matrix_y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;#Evaluation&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name_scope&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Validation'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
  &lt;span class=&quot;c&quot;&gt;#Returns whether model made correct prediction (List of booleans)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;correct_prediction&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;equal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;argmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_hat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;argmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'isCorrect'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;c&quot;&gt;#Average of correct prediction (%Accuracy)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;accuracy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reduce_mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cast&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;correct_prediction&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Accuracy'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;#Print Accuracy&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Accuracy:&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'{0:.2&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;}'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;accuracy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;eval&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feed_dict&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_matrix_y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;})))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
</content>
 </entry>
 
 <entry>
   <title>Binary Prefixes</title>
   <link href="http://localhost:4000/binary-prefixes/"/>
   <updated>2018-01-05T00:00:00-05:00</updated>
   <id>http://localhost:4000/binary-prefixes</id>
   <content type="html">&lt;h2 id=&quot;decimal-prefixes&quot;&gt;Decimal Prefixes&lt;/h2&gt;
&lt;p&gt;When dealing with very large or very small amounts, it is common to append, or rather &lt;em&gt;pre&lt;/em&gt;pend, one of the SI (metric system) prefixes to whatever unit is being used. For example, the average human weighs $62000\text{g}$ but because that number is cumbersome to use, we usually append &lt;em&gt;kilo-&lt;/em&gt; (the SI prefix meaning a thousand) to the gram unit and say $62\text{kg}$ instead.&lt;/p&gt;

&lt;p&gt;This system works just fine for all sorts of units of measure like mass, length, energy, etc. But when it’s applied to units of information, specifically binary ones like bits and bytes, a problem arises…&lt;/p&gt;

&lt;!--more--&gt;

&lt;h2 id=&quot;binary-prefixes&quot;&gt;Binary Prefixes&lt;/h2&gt;
&lt;p&gt;You may have bought a hard drive or even a new phone and found that there is a discrepancy between the amount of storage shown on the package and the maximum amount displayed when you look in the settings.&lt;/p&gt;

&lt;p&gt;So what causes this disparity?&lt;/p&gt;

&lt;p&gt;The difference between a GB (gigabyte) and a GiB (gibibyte).&lt;/p&gt;

&lt;p&gt;Historically when a computer scientist wrote kB, for example, they didn’t mean $1000$ bytes. They meant $1024$ bytes. This is because computers operate in binary which is based on powers of two. In the kilobyte’s case, $2^{10} = 1024$ which is almost $1000$. This made it a close enough approximation of memory in a computer which was based on powers of $2$.&lt;/p&gt;

&lt;p&gt;This changed around 1998 when the International Electrotechnical Commission (IEC) and other regulatory organizations created a new set of prefixes to be used as binary prefixes. The US National Institute of Standards and Technology (NIST) followed suit and required that the SI prefixes only be used in the decimal sense. Below is a table of the metric prefixes vs the binary ones.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Decimal Prefix (SI)&lt;/th&gt;
      &lt;th&gt;Value&lt;/th&gt;
      &lt;th&gt;Value (1000)&lt;/th&gt;
      &lt;th&gt;Binary Prefix (IEC)&lt;/th&gt;
      &lt;th&gt;Value&lt;/th&gt;
      &lt;th&gt;Value (1024)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;kilo (k)&lt;/td&gt;
      &lt;td&gt;10&lt;sup&gt;3&lt;/sup&gt;&lt;/td&gt;
      &lt;td&gt;1000&lt;/td&gt;
      &lt;td&gt;kibi (ki)&lt;/td&gt;
      &lt;td&gt;2&lt;sup&gt;10&lt;/sup&gt;&lt;/td&gt;
      &lt;td&gt;1024&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;mega (M)&lt;/td&gt;
      &lt;td&gt;10&lt;sup&gt;6&lt;/sup&gt;&lt;/td&gt;
      &lt;td&gt;1000&lt;sup&gt;2&lt;/sup&gt;&lt;/td&gt;
      &lt;td&gt;mebi (Mi)&lt;/td&gt;
      &lt;td&gt;2&lt;sup&gt;20&lt;/sup&gt;&lt;/td&gt;
      &lt;td&gt;1024&lt;sup&gt;2&lt;/sup&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;giga (G)&lt;/td&gt;
      &lt;td&gt;10&lt;sup&gt;9&lt;/sup&gt;&lt;/td&gt;
      &lt;td&gt;1000&lt;sup&gt;3&lt;/sup&gt;&lt;/td&gt;
      &lt;td&gt;gibi (Gi)&lt;/td&gt;
      &lt;td&gt;2&lt;sup&gt;30&lt;/sup&gt;&lt;/td&gt;
      &lt;td&gt;1024&lt;sup&gt;3&lt;/sup&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;tera (T)&lt;/td&gt;
      &lt;td&gt;10&lt;sup&gt;12&lt;/sup&gt;&lt;/td&gt;
      &lt;td&gt;1000&lt;sup&gt;4&lt;/sup&gt;&lt;/td&gt;
      &lt;td&gt;tebi (Ti)&lt;/td&gt;
      &lt;td&gt;2&lt;sup&gt;40&lt;/sup&gt;&lt;/td&gt;
      &lt;td&gt;1024&lt;sup&gt;4&lt;/sup&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;peta (P)&lt;/td&gt;
      &lt;td&gt;10&lt;sup&gt;15&lt;/sup&gt;&lt;/td&gt;
      &lt;td&gt;1000&lt;sup&gt;5&lt;/sup&gt;&lt;/td&gt;
      &lt;td&gt;pebi (Pi)&lt;/td&gt;
      &lt;td&gt;2&lt;sup&gt;50&lt;/sup&gt;&lt;/td&gt;
      &lt;td&gt;1024&lt;sup&gt;5&lt;/sup&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;exa (E)&lt;/td&gt;
      &lt;td&gt;10&lt;sup&gt;18&lt;/sup&gt;&lt;/td&gt;
      &lt;td&gt;1000&lt;sup&gt;6&lt;/sup&gt;&lt;/td&gt;
      &lt;td&gt;exbi (Ei)&lt;/td&gt;
      &lt;td&gt;2&lt;sup&gt;60&lt;/sup&gt;&lt;/td&gt;
      &lt;td&gt;1024&lt;sup&gt;6&lt;/sup&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;zetta (Z)&lt;/td&gt;
      &lt;td&gt;10&lt;sup&gt;21&lt;/sup&gt;&lt;/td&gt;
      &lt;td&gt;1000&lt;sup&gt;7&lt;/sup&gt;&lt;/td&gt;
      &lt;td&gt;zebi (Zi)&lt;/td&gt;
      &lt;td&gt;2&lt;sup&gt;70&lt;/sup&gt;&lt;/td&gt;
      &lt;td&gt;1024&lt;sup&gt;7&lt;/sup&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;yotta (Y)&lt;/td&gt;
      &lt;td&gt;10&lt;sup&gt;24&lt;/sup&gt;&lt;/td&gt;
      &lt;td&gt;1000&lt;sup&gt;8&lt;/sup&gt;&lt;/td&gt;
      &lt;td&gt;yobi (Yi)&lt;/td&gt;
      &lt;td&gt;2&lt;sup&gt;80&lt;/sup&gt;&lt;/td&gt;
      &lt;td&gt;1024&lt;sup&gt;8&lt;/sup&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;So problem solved, right? Well no. Most people have never heard of a kibibyte (kiB), mebibyte (MiB), or gibibyte (Gib) and probably never will. Hardware manufacturers know this and, rather than deal with the consumer’s perception of information storage, opt to just use the closest decimal prefix.&lt;/p&gt;

&lt;p&gt;That said, there are a growing number of software and hardware applications that make use of binary prefixes.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://imgs.xkcd.com/comics/kilobyte.png?style=centerme&quot; alt=&quot;xkcd&quot; /&gt;&lt;/p&gt;
&lt;center&gt;&lt;i&gt;Relevant xckd&lt;/i&gt;&lt;/center&gt;
</content>
 </entry>
 

</feed>